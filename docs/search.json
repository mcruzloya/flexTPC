[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A flexible model for thermal performance curves",
    "section": "",
    "text": "FlexTPC is a mathematical model for thermal performance curves that aims to be both flexible and interpretable. We show an interactive visualization of flexTPC and its parameters below:\nviewof T_min = Inputs.range(\n  [-5, 20], \n  {value: 10, step: 0.2, label: \"Tmin:\"}\n)\nviewof T_max = Inputs.range(\n  [30, 50], \n  {value: 35, step: 0.2, label: \"Tmax:\"}\n)\n\nviewof r_max = Inputs.range(\n  [0, 1.2], \n  {value: 1, step: 0.1, label: \"rmax:\"}\n)\n\nviewof alpha = Inputs.range(\n  [0, 1], \n  {value: 0.8, step: 0.02, label: \"alpha:\"}\n)\n\nviewof beta = Inputs.range(\n  [0, 1], \n  {value: 0.3, step: 0.02, label: \"beta:\"}\n)\nfunction incrementalArray(start, end, step) {\n    var arr = [];\n    // convert count to an integer to avoid rounding errors\n    var count = +((end - start) / step).toFixed();\n    for (var j = 0; j &lt;= count; j++) {\n        var i = start + j * step;\n        arr.push(i);\n    }\n    return arr;\n}\n\nfunction flexTPC(x, T_min, T_max, r_max, alpha, beta) {\n  if (x &lt; T_min) {\n  return 0.0\n  }\n  if (x &gt; T_max) {\n  return 0.0\n  }\n  return r_max * Math.exp((alpha * (1.0 - alpha) / beta**2) * (alpha *    Math.log( (x - T_min) / alpha)  + \n      (1.0 - alpha) * Math.log( (T_max - x) / (1.0 - alpha)) -\n      Math.log(T_max - T_min)))\n}\n\nxvals = incrementalArray(-5, 50, 0.1)\nyvals = xvals.map((x) =&gt; flexTPC(x, T_min, T_max, r_max, alpha, beta))\nzip = (a, b) =&gt; a.map((k, i) =&gt; [k, b[i]]);\ndata = zip(xvals, yvals)\n\nPlot.plot({\n  width: 600,\n  height: 400,\n  y: { domain: [-0.01, 1.25] },\n  marks: [\n    Plot.line(\n      data,\n      {\n        strokeWidth: 3,\n        stroke: \"steelblue\",\n        fontSize: 14\n      }\n    ),\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    Plot.axisX({label: \"Temperature [°C]\", fontSize: 14, marginBottom: 40}),\n    Plot.axisY({label: \"trait performance\", fontSize: 14, x: 0 })\n  ]\n})\nFlexTPC is parametrized in terms of biologically interpretable quantities:"
  },
  {
    "objectID": "index.html#r-code",
    "href": "index.html#r-code",
    "title": "A flexible model for thermal performance curves",
    "section": "R code",
    "text": "R code\nFor convenience, we provide R functions that implement the flexTPC model below:\n\n# FlexTPC model for thermal performance curves.\n# (parametrized with alpha/beta)\nflexTPC &lt;- function(temp, Tmin, Tmax, rmax, alpha, beta) {\n  s &lt;- alpha * (1 - alpha) / beta^2\n  result &lt;- rep(0, length(temp))\n  Tidx = (temp &gt; Tmin) & (temp &lt; Tmax)\n  result[Tidx] &lt;- rmax * exp(s * (alpha * log( (temp[Tidx] - Tmin) / alpha) \n                  + (1 - alpha) * log( (Tmax - temp[Tidx]) / (1 - alpha)) \n                  - log(Tmax - Tmin)) ) \n  return(result)\n}\n  \n# FlexTPC model for thermal performance curves.\n# (parametrized with Topt/B)\nflexTPC2 &lt;- function(temp, Tmin, Tmax, rmax, Topt, B) {\n  alpha &lt;- (Topt - Tmin) / (Tmax - Tmin)\n  beta &lt;- B / (Tmax - Tmin)\n  return(flexTPC(temp, Tmin, Tmax, rmax, alpha, beta))\n}"
  },
  {
    "objectID": "index.html#reference",
    "href": "index.html#reference",
    "title": "A flexible model for thermal performance curves",
    "section": "Reference",
    "text": "Reference\nIf you use flexTPC, please cite\nCruz-Loya M, Mordecai EA, Savage VM. 2024. A flexible model for thermal performance curves. (biorXiv preprint) (data and code)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "rflexTPC/Visualization.html",
    "href": "rflexTPC/Visualization.html",
    "title": "Restricted models from flexTPC equation",
    "section": "",
    "text": "import numpy as np\n\nfrom bokeh.plotting import figure, show\nfrom bokeh.io import output_notebook, curdoc\nfrom bokeh.models import ColumnDataSource, Slider\nfrom bokeh.layouts import column, row, layout\n\nfrom matplotlib import pyplot as plt\n\n\ndef flexTPC(T, Tmin=10, Tmax=35, rmax=1.0, α=0.8, β̃=0.2):\n    result = np.zeros(len(T))\n    Tidx = (T &gt; Tmin) & (T &lt; Tmax)\n    s = α * (1 - α) / β̃**2\n    result[Tidx] = rmax * np.exp(s * (α * np.log((T[Tidx] - Tmin) / α) +\n                                      (1 - α) * np.log((Tmax - T[Tidx]) / (1 - α)) - \n                                      np.log(Tmax - Tmin)))\n    return result\n\ndef rflexTPC(T, Tmin=10, Tmax=35, rmax=1.0, α=0.8):\n    β̃ = 1 / np.sqrt(8) - (1 / np.sqrt(8) - 0.2) * np.abs(α - 0.5) / 0.3\n    result = np.zeros(len(T))\n    Tidx = (T &gt; Tmin) & (T &lt; Tmax)\n    s = α * (1 - α) / β̃**2\n    result[Tidx] = rmax * np.exp(s * (α * np.log((T[Tidx] - Tmin) / α) +\n                                      (1.0 - α) * np.log((Tmax - T[Tidx]) / (1.0 - α)) - \n                                      np.log(Tmax - Tmin)))\n    return result\n\ndef rflexTPC_nd(τ, rmax=1.0, α=0.8):\n    β̃ = 1 / np.sqrt(8) - (1 / np.sqrt(8) - 0.2) * np.abs(α - 0.5) / 0.3\n    result = np.zeros(len(T))\n    s = α * (1 - α) / β̃**2\n    result = rmax * np.exp(s * (α * np.log(τ / α) +\n                                      (1.0 - α) * np.log((1.0 - τ) / (1.0 - α))))\n    return result\n\n\nT = np.arange(5, 40, 0.001)\nfor α in [0.1, 0.3, 0.5, 0.8, 0.9]:\n    plt.plot(T, rflexTPC(T, rmax=1, α=α), '-', label=f'α={α}')\nplt.xlabel('temperature')\nplt.ylabel('performance')\nplt.legend(loc='best')\nplt.show()\n\n\n\n\n\n\n\n\n\noutput_notebook()\n\n    \n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\nT = np.arange(0, 40, 0.01)\n\nsource = ColumnDataSource(data=dict(x=T, y=flexTPC(T, rmax=2.0, α=0.5, β̃=1/np.sqrt(8))))\n\nplot = figure(height=400, width=400, title=\"flexTPC\",\n              tools=\"crosshair,pan,reset,save,wheel_zoom\",\n              x_range=[-5, 40], y_range=[0, 2.1], x_axis_label=r'$$T$$', y_axis_label=r'$$r(T)$$')\n# add a line renderer with legend and line thickness to the plot\nplot.line('x', 'y', source=source, line_width=3, line_alpha=1.0)\n\n# Set up widgets\nTmin_sl = Slider(title=r\"$$T_{\\min}$$\", value=10.0, start=-5.0, end=20.0, step=0.1)\nTmax_sl = Slider(title=r\"$$T_{\\max}$$\", value=35.0, start=25.0, end=50.0, step=0.1)\nrmax_sl = Slider(title=r\"$$r_{\\max}$$\", value=1.0, start=0.0, end=2.0, step=0.01)\nα_sl = Slider(title=r\"$$\\alpha$$\", value=0.5, start=0, end=1.0, step=0.01)\nβ̃_sl = Slider(title=r\"$$\\tilde{\\beta}$$\", value=0.35, start=0.0, end=1.0, step=0.01)\n\ndef update_data(attrname, old, new):\n    # Get the current slider values\n    Tmin = Tmin_sl.value\n    Tmax = Tmax_sl.value\n    rmax = rmax_sl.value\n    α = α_sl.value\n    β̃ = β̃_sl.value\n\n    # Generate the new curve\n    x = T\n    y = flexTPC(T, Tmin=Tmin, Tmax=Tmax, rmax=rmax, α=α, β̃=β̃)\n\n    source.data = dict(x=x, y=y)\n\nfor w in [Tmin_sl, Tmax_sl, rmax_sl, α_sl, β̃_sl]:\n    w.on_change('value', update_data)\n\n# Set up layouts and add to document\ninputs = column(Tmin_sl, Tmax_sl, rmax_sl, α_sl, β̃_sl)\n\nl = layout(\n    [\n        [plot, [Tmin_sl, Tmax_sl, rmax_sl, α_sl, β̃_sl]],\n    ],\n)\n\ncurdoc().add_root(row(inputs, plot, width=800))\ncurdoc().title = \"Sliders\"\n\nshow(l)\n\nWARNING:bokeh.embed.util:\nYou are generating standalone HTML/JS output, but trying to use real Python\ncallbacks (i.e. with on_change or on_event). This combination cannot work.\n\nOnly JavaScript callbacks may be used with standalone output. For more\ninformation on JavaScript callbacks with Bokeh, see:\n\n    https://docs.bokeh.org/en/latest/docs/user_guide/interaction/js_callbacks.html\n\nAlternatively, to use real Python callbacks, a Bokeh server application may\nbe used. For more information on building and running Bokeh applications, see:\n\n    https://docs.bokeh.org/en/latest/docs/user_guide/server.html\n\n\n\n\n  \n\n\n\n\n\n\ninputs = column(text, offset, amplitude, phase, freq)"
  },
  {
    "objectID": "model_comparison/Abcoli.html",
    "href": "model_comparison/Abcoli.html",
    "title": "Model comparison on antibiotic dataset",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport random\n\nrandom.seed(42)\n\nfrom matplotlib import pyplot as plt\n\nfrom scipy.optimize import minimize\nfrom sklearn.model_selection import LeaveOneOut\ndef briere1(T, Tmin=10.0, Tmax=50.0, c=1.0):\n    result = np.zeros(len(T))\n    cond = (Tmin &lt; T) & (T &lt; Tmax)\n    result[cond] = c * T[cond] * np.exp(np.log(T[cond] - Tmin) + 0.5 * np.log(Tmax - T[cond]))\n    return result\n\ndef briere2(T, Tmin=10.0, Tmax=50.0, c=1.0, b=2.0):\n    result = np.zeros(len(T))\n    cond = (Tmin &lt; T) & (T &lt; Tmax)\n    result[cond] = c * T[cond] * np.exp(np.log(T[cond] - Tmin) +  np.log(Tmax - T[cond]) / b)\n    return result\n\n# Fully biologically interpretable parametrization of flexTPC model\ndef flexTPC(T, Tmin=10.0, Tmax=50.0, rmax=1.0, α=0.8, β=0.2):\n    result = np.zeros(len(T))\n    cond = (Tmin &lt; T) & (T &lt; Tmax)\n    s = α * (1.0 - α) / β**2\n    result[cond] = rmax * np.exp(s * (α * (np.log(T[cond] - Tmin) - np.log(α)) +\n                                      (1 - α) * (np.log(Tmax - T[cond]) - np.log(1 - α) )\n                                       - np.log(Tmax - Tmin)))\n    return result\n\n# Exponential product curve.\ndef expprodcurve(T, Tmin=10.0, Tmax=50.0, c=1.0, kI=0.1, kU=0.5):\n    result = np.zeros(len(T))\n    cond = (Tmin &lt; T) & (T &lt; Tmax)\n    result[cond] = c * (1.0 - np.exp(-kI*(T[cond] - Tmin))) * (1.0 - np.exp(-kU*(Tmax - T[cond])))\n    return result\n\n# Kumaraswarmy distribution TPC.\ndef kumaraswarmy(T, Tmin=10.0, Tmax=50.0, c=1.0, a=5.0, b=5.0):\n    result = np.zeros(len(T))\n    cond = (Tmin &lt; T) & (T &lt; Tmax)\n    τ = (T[cond] - Tmin) / (Tmax - Tmin)\n    result[cond] = c * τ**(a - 1) * (1.0 - τ**a)**(b - 1)\n    return result\ndef nloglik2(θ, Tdata, rdata, model=flexTPC):\n    params, σ2 = θ[:-1], θ[-1]\n    Tmin, Tmax = θ[:2]\n    #outside = (Tdata &lt; Tmin) | (Tdata &gt; Tmax)\n    n = len(rdata)\n    return 0.5 * (n * np.log(2*np.pi*σ2) + np.sum((rdata - model(Tdata, *params))**2 / σ2))\n    \ndef AIC(nll, p):\n    return 2 * nll + 2 * p \n\ndef BIC(nll, n, p):\n    return 2 * nll + p * np.log(n)"
  },
  {
    "objectID": "model_comparison/Abcoli.html#e.-coli-antibiotics-dataset",
    "href": "model_comparison/Abcoli.html#e.-coli-antibiotics-dataset",
    "title": "Model comparison on antibiotic dataset",
    "section": "E. coli antibiotics dataset",
    "text": "E. coli antibiotics dataset\n\nabcoli = pd.read_csv(\"ab_data.csv\")\nsingle = abcoli[(abcoli[\"drug2name\"] == \"WT\")]\ndrugs = ['AMP',\n 'CLI',\n 'CPR',\n 'ERY',\n 'FOX',\n 'GEN',\n 'LVX',\n 'NTR',\n 'STR',\n 'TET',\n 'TMP',\n 'TOB',\n 'WT']\n\n\ndrugs\n\n['AMP',\n 'CLI',\n 'CPR',\n 'ERY',\n 'FOX',\n 'GEN',\n 'LVX',\n 'NTR',\n 'STR',\n 'TET',\n 'TMP',\n 'TOB',\n 'WT']\n\n\n\nsingle\n\n\n\n\n\n\n\n\ndrug1name\ndrug2name\ndrug1num\ndrug2num\nT\nt\nsample\nOD\n\n\n\n\n28\nNTR\nWT\n8.0\n13.0\n37.0\n24.0\n1.0\n0.6244\n\n\n29\nNTR\nWT\n8.0\n13.0\n37.0\n24.0\n2.0\n0.6865\n\n\n30\nNTR\nWT\n8.0\n13.0\n37.0\n24.0\n3.0\n0.7640\n\n\n31\nNTR\nWT\n8.0\n13.0\n37.0\n24.0\n4.0\n0.7984\n\n\n32\nNTR\nWT\n8.0\n13.0\n22.0\n24.0\n1.0\n0.0139\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1927\nTET\nWT\n10.0\n13.0\n44.0\n24.0\n4.0\n0.3349\n\n\n1928\nTET\nWT\n10.0\n13.0\n30.0\n24.0\n1.0\n0.3772\n\n\n1929\nTET\nWT\n10.0\n13.0\n30.0\n24.0\n2.0\n0.3865\n\n\n1930\nTET\nWT\n10.0\n13.0\n30.0\n24.0\n3.0\n0.3883\n\n\n1931\nTET\nWT\n10.0\n13.0\n30.0\n24.0\n4.0\n0.4085\n\n\n\n\n364 rows × 8 columns\n\n\n\n\nabdata = single.groupby([\"drug1name\", \"T\"], as_index=False).agg({'OD':'mean'})\n#Set low values to zero\n\n#abdata[\"OD\"][abdata[\"OD\"] &lt; 0.02] = 0.0\n\n\nabdata[abdata[\"OD\"] &lt; 0.0]\n\n\n\n\n\n\n\n\ndrug1name\nT\nOD\n\n\n\n\n\n\n\n\n\n\nabdata[abdata[\"drug1name\"] == \"AMP\"]\n\n\n\n\n\n\n\n\ndrug1name\nT\nOD\n\n\n\n\n0\nAMP\n22.0\n0.255500\n\n\n1\nAMP\n25.0\n0.399700\n\n\n2\nAMP\n30.0\n0.717800\n\n\n3\nAMP\n37.0\n0.713775\n\n\n4\nAMP\n41.0\n0.625300\n\n\n5\nAMP\n44.0\n0.001950\n\n\n6\nAMP\n46.0\n0.002450\n\n\n\n\n\n\n\n\ndef nloglikab(θ, Tdata, rdata, model=flexTPC):\n    params, σ2 = θ[:-1], θ[-1]\n    Tmin, Tmax = θ[:2]\n    return 0.5 * (len(rdata) * np.log(2*np.pi*σ2) + np.sum((rdata \n                                                    - model(Tdata, *params))**2 / σ2))\n\n\n#initial_mb[\"GEN\"] = [20.0, 46.5, 0.5, 0.7, 2.0, 0.005]\n#initial_mb[\"STR\"] = [20.0, 46.5, 0.5, 0.7, 2.0, 0.005]\n#initial_mb[\"TOB\"] = [20.0, 46.5, 0.5, 0.7, 2.0, 0.005]\n#initial_mb[\"ERY\"] = [20.0, 46.5, 0.5, 0.7, 4.0, 0.005]\n#initial_mb[\"AMP\"] = [15.0, 46.5, 0.7, 0.7, 2.0, 0.005]\n\ninitial_b1 = {drug:[20.0, 46.5, 3e-4, 0.005] for drug in drugs}\ninitial_b2 = {drug:[20.0, 46.5, 3e-4, 2.0, 0.005] for drug in drugs}\ninitial_flex = {drug:[20.0, 46.5, 0.7, 0.7, 0.2, 0.005] for drug in drugs}\ninitial_epc = {drug:[20.0, 46.5, 1.0, 0.05, 0.4, 0.005] for drug in drugs}\ninitial_kum = {drug:[20.0, 46.5, 2.0, 3, 2, 0.005] for drug in drugs}\n\n\nloo = LeaveOneOut()\nloo.get_n_splits(abdata[\"T\"][abdata[\"drug1name\"] == \"WT\"])\n\n7\n\n\n\ninitial_flex\n\n{'AMP': [20.0, 46.5, 0.7, 0.7, 0.2, 0.005],\n 'CLI': [20.0, 46.5, 0.7, 0.7, 0.2, 0.005],\n 'CPR': [20.0, 46.5, 0.7, 0.7, 0.2, 0.005],\n 'ERY': [20.0, 46.5, 0.7, 0.7, 0.2, 0.005],\n 'FOX': [20.0, 46.5, 0.7, 0.7, 0.2, 0.005],\n 'GEN': [20.0, 46.5, 0.7, 0.7, 0.2, 0.005],\n 'LVX': [20.0, 46.5, 0.7, 0.7, 0.2, 0.005],\n 'NTR': [20.0, 46.5, 0.7, 0.7, 0.2, 0.005],\n 'STR': [20.0, 46.5, 0.7, 0.7, 0.2, 0.005],\n 'TET': [20.0, 46.5, 0.7, 0.7, 0.2, 0.005],\n 'TMP': [20.0, 46.5, 0.7, 0.7, 0.2, 0.005],\n 'TOB': [20.0, 46.5, 0.7, 0.7, 0.2, 0.005],\n 'WT': [20.0, 46.5, 0.7, 0.7, 0.2, 0.005]}\n\n\n\nT = np.arange(10, 50, 0.001)\nplt.figure(figsize=(15, 6))\n\n\nfor i, drug in enumerate(drugs):\n    subset = abdata[abdata[\"drug1name\"] == drug]\n    \n    plt.subplot(2, 7, i+1)\n    # Plot datapoints.\n    Tdata = subset[\"T\"]\n    rdata = subset[\"OD\"]\n\n    plt.plot(Tdata, rdata, '^')\n    \n    # Plot initial parameters curves by Briere.\n    par = initial_b1[drug]    \n    plt.plot(T, briere1(T, *par[:-1]), color=\"firebrick\")\n    \n    par = initial_b2[drug]\n    plt.plot(T, briere2(T, *par[:-1]), color=\"orange\")\n    \n    par = initial_flex[drug]\n    plt.plot(T, flexTPC(T, *par[:-1]), color=\"darkgreen\")\n\n    par = initial_epc[drug]\n    plt.plot(T, expprodcurve(T, *par[:-1]), color=\"purple\")\n\n    par = initial_kum[drug]\n    plt.plot(T, kumaraswarmy(T, *par[:-1]), color=\"blue\")\n    \n    plt.ylim(-0.025, 0.9)\n    plt.xlabel(\"Temperature [C]\")\n    plt.ylabel(\"OD\", fontsize=10)\n    plt.title(drug)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n# LOOCV\n\nparams = {\"flexTPC\":{}, \"b1\":{}, \"b2\":{}, \"epc\":{}, \"kum\":{}}\nmodels = [\"b1\", \"b2\", \"flexTPC\", \"epc\", \"kum\"]\nloocv_nll = {model:{drug:[] for drug in drugs } for model in models}\n\nmethod = \"Nelder-Mead\"\nfor i, drug in enumerate(drugs):\n    subset = abdata[abdata[\"drug1name\"] == drug]\n    Tdata = np.array(subset[\"T\"])\n    rdata = np.array(subset[\"OD\"])\n    \n    print(drug)\n    print(\"numsplits:\", loo.get_n_splits(Tdata))\n    \n    for train, test in loo.split(Tdata):\n        T_train, r_train = Tdata[train], rdata[train]\n        T_test, r_test = Tdata[test], rdata[test]\n    \n\n        params[\"b1\"][drug] = minimize(nloglikab, initial_b1[drug],\n                           bounds=[(0.0, 20.0), \n                                    (30.0, 60.0), \n                                    (0.0, 10.0),  \n                                    (0, 0.01)],\n                      args=(Tdata, rdata, briere1), options={\"maxiter\":100000}, method=method)\n        θ = params[\"b1\"][drug][\"x\"]\n        loocv_nll[\"b1\"][drug].append(nloglikab(θ, T_test, r_test, model=briere1))\n\n        params[\"b2\"][drug] = minimize(nloglikab, initial_b2[drug],\n                            bounds=[(0.0, 20.0), \n                                    (30.0, 60.0), \n                                    (0.0, 10.0),\n                                    (1.0, 20.0),\n                                    (0, 0.01)],\n                      args=(Tdata, rdata, briere2), options={\"maxiter\":100000}, method=method)\n        θ = params[\"b2\"][drug][\"x\"]\n        loocv_nll[\"b2\"][drug].append(nloglikab(θ, T_test, r_test, model=briere2))\n\n        \n        params[\"flexTPC\"][drug] = minimize(nloglikab, initial_flex[drug], #Check nloglikab vs nloglok\n                            bounds=[(0.0, 20.0), \n                                    (30.0, 60.0), \n                                    (0.0, 1.0), \n                                    (0.1, 0.95), \n                                    (0.0, 2.0), \n                                    (0, 0.01)],\n                      args=(Tdata, rdata), options={\"maxiter\":100000}, method=method)\n        θ = params[\"flexTPC\"][drug][\"x\"]\n        loocv_nll[\"flexTPC\"][drug].append(nloglikab(θ, T_test, r_test, model=flexTPC))\n\n        params[\"epc\"][drug] = minimize(nloglikab, initial_epc[drug],\n                                bounds=[(0.0, 20.0), \n                                        (30.0, 60.0), \n                                        (0.0, np.inf), \n                                        (0.0, np.inf), \n                                        (0.0, np.inf), \n                                        (0, 1.0)],\n                          args=(Tdata, rdata, expprodcurve), options={\"maxiter\":100000}, method=method)\n        θ = params[\"epc\"][drug][\"x\"]\n        loocv_nll[\"epc\"][drug].append(nloglikab(θ, T_test, r_test, model=expprodcurve))\n\n        params[\"kum\"][drug] = minimize(nloglikab, initial_kum[drug],\n                                        bounds=[(0.0, 20.0), \n                                                (30.0, 60.0),\n                                                (0.0, np.inf), \n                                                (1.0, np.inf), \n                                                (1.0, np.inf), \n                                                (0, 1.0)],\n                                  args=(Tdata, rdata, kumaraswarmy), options={\"maxiter\":100000}, method=method)\n        θ = params[\"kum\"][drug][\"x\"]\n        loocv_nll[\"kum\"][drug].append(nloglikab(θ, T_test, r_test, model=kumaraswarmy))\n\nAMP\nnumsplits: 7\nCLI\nnumsplits: 7\nCPR\nnumsplits: 7\nERY\nnumsplits: 7\nFOX\nnumsplits: 7\n\n\n/var/folders/g8/wjqhnz1n01d5l4pdv8cnykw40000gp/T/ipykernel_90204/1310441239.py:4: RuntimeWarning: divide by zero encountered in log\n  return 0.5 * (len(rdata) * np.log(2*np.pi*σ2) + np.sum((rdata\n/var/folders/g8/wjqhnz1n01d5l4pdv8cnykw40000gp/T/ipykernel_90204/1310441239.py:4: RuntimeWarning: divide by zero encountered in divide\n  return 0.5 * (len(rdata) * np.log(2*np.pi*σ2) + np.sum((rdata\n/var/folders/g8/wjqhnz1n01d5l4pdv8cnykw40000gp/T/ipykernel_90204/1310441239.py:4: RuntimeWarning: invalid value encountered in scalar add\n  return 0.5 * (len(rdata) * np.log(2*np.pi*σ2) + np.sum((rdata\n\n\nGEN\nnumsplits: 7\nLVX\nnumsplits: 7\nNTR\nnumsplits: 7\nSTR\nnumsplits: 7\nTET\nnumsplits: 7\nTMP\nnumsplits: 7\nTOB\nnumsplits: 7\nWT\nnumsplits: 7\n\n\n\nparams = {\"flexTPC\":{}, \"b1\":{}, \"b2\":{}, \"epc\":{}, \"kum\":{}}\nmodels = [\"b1\", \"b2\", \"flexTPC\", \"epc\", \"kum\"]\nmethod = \"Nelder-Mead\"\nfor i, drug in enumerate(drugs):\n    subset = abdata[abdata[\"drug1name\"] == drug]\n    Tdata = np.array(subset[\"T\"])\n    rdata = np.array(subset[\"OD\"])    \n\n    params[\"b1\"][drug] = minimize(nloglikab, initial_b1[drug],\n                           bounds=[(0.0, 20.0), \n                                    (30.0, 60.0), \n                                    (0.0, 10.0),  \n                                    (0, 0.01)],\n                      args=(Tdata, rdata, briere1), options={\"maxiter\":100000}, method=method)\n    \n    params[\"b2\"][drug] = minimize(nloglikab, initial_b2[drug],\n                            bounds=[(0.0, 20.0), \n                                    (30.0, 60.0), \n                                    (0.0, 10.0),\n                                    (1.0, 20.0),\n                                    (0, 0.01)],\n                      args=(Tdata, rdata, briere2), options={\"maxiter\":100000}, method=method)\n    params[\"flexTPC\"][drug] = minimize(nloglikab, initial_flex[drug], #nloglikab vs nloglok\n                            bounds=[(0.0, 20.0), \n                                    (30.0, 60.0), \n                                    (0.0, 1.0), \n                                    (0.1, 0.95), \n                                    (0.0, 2.0), \n                                    (0, 0.01)],\n                      args=(Tdata, rdata), options={\"maxiter\":100000}, method=method)\n    params[\"epc\"][drug] = minimize(nloglikab, initial_epc[drug],\n                                bounds=[(0.0, 20.0), \n                                        (30.0, 60.0), \n                                        (0.0, np.inf), \n                                        (0.0, np.inf), \n                                        (0.0, np.inf), \n                                        (0, 1.0)],\n                          args=(Tdata, rdata, expprodcurve), options={\"maxiter\":100000}, method=method)\n    params[\"kum\"][drug] = minimize(nloglikab, initial_kum[drug],\n                                        bounds=[(0.0, 20.0), \n                                                (30.0, 60.0),\n                                                (0.0, np.inf), \n                                                (1.0, np.inf), \n                                                (1.0, np.inf), \n                                                (0, 1.0)],\n                                  args=(Tdata, rdata, kumaraswarmy), options={\"maxiter\":100000}, method=method)\n\n/var/folders/g8/wjqhnz1n01d5l4pdv8cnykw40000gp/T/ipykernel_90204/1310441239.py:4: RuntimeWarning: divide by zero encountered in log\n  return 0.5 * (len(rdata) * np.log(2*np.pi*σ2) + np.sum((rdata\n/var/folders/g8/wjqhnz1n01d5l4pdv8cnykw40000gp/T/ipykernel_90204/1310441239.py:4: RuntimeWarning: divide by zero encountered in divide\n  return 0.5 * (len(rdata) * np.log(2*np.pi*σ2) + np.sum((rdata\n/var/folders/g8/wjqhnz1n01d5l4pdv8cnykw40000gp/T/ipykernel_90204/1310441239.py:4: RuntimeWarning: invalid value encountered in scalar add\n  return 0.5 * (len(rdata) * np.log(2*np.pi*σ2) + np.sum((rdata\n\n\n\nT = np.arange(10, 50, 0.001)\n\nfig, axarr = plt.subplots(3, 5, figsize=(6.81102, 4), dpi=300, sharex=True, sharey=True)\n\nplt.rcParams.update({'font.size': 8})\n\nfor i, drug in enumerate(drugs):\n    ax = axarr[i // 5, i % 5]\n    subset = abdata[abdata[\"drug1name\"] == drug]\n\n    ax.set_xticks([10, 20, 30, 40, 50])\n    ax.set_yticks([0, 0.5, 1])\n\n\n    #plt.subplot(3, 5, i+1)\n    # Plot datapoints.\n    Tdata = subset[\"T\"]\n    rdata = subset[\"OD\"]\n\n    ax.plot(Tdata, rdata, '^', markersize=3)\n    \n    # Plot fitted curves by Briere.\n    par = params[\"b1\"][drug][\"x\"]\n    ax.plot(T, briere1(T, *par[:-1]), color=\"firebrick\", linewidth=1, alpha=0.8)\n    \n    par = params[\"b2\"][drug][\"x\"]\n    ax.plot(T, briere2(T, *par[:-1]), color=\"orange\", linewidth=1, alpha=0.8)\n    \n             \n    par = params[\"flexTPC\"][drug][\"x\"]\n    ax.plot(T, flexTPC(T, *par[:-1]), color=\"darkgreen\", linewidth=1, alpha=0.8)\n    \n\n    #par = params[\"epc\"][drug][\"x\"]\n    #ax.plot(T, expprodcurve(T, *par[:-1]), color=\"purple\", linewidth=1, alpha=0.8)\n\n    #par = params[\"kum\"][drug][\"x\"]\n    #ax.plot(T, kumaraswarmy(T, *par[:-1]), color=\"blue\", linewidth=1, alpha=0.8)\n    \n    ax.set_ylim(-0.025, 1.0)\n\n    if drug != 'WT':\n        ax.set_title(drug, fontsize=10)\n    else:\n        ax.set_title('no drug', fontsize=10)\n    if drug in (\"TMP\", \"TOB\", \"WT\"):\n        ax.set_xlabel(\"Temperature [°C]\", fontsize=8)\n    if drug in (\"AMP\", \"GEN\", \"TMP\"):\n        ax.set_ylabel(\"OD\", fontsize=10)\n        \naxarr[-1, -1].axis(\"off\")\naxarr[-1, -2].axis(\"off\")\n\n\n#ERY\n#rect = plt.Rectangle(\n#        # (lower-left corner), width, height\n#        (0.63, 0.69), 0.18, 0.29, fill=False, color=\"k\", lw=1.5, \n#        zorder=1000, transform=fig.transFigure, figure=fig\n#    )\n\n#GEN\n#rect2 = plt.Rectangle(\n#        # (lower-left corner), width, height\n#        (0.04, 0.4), 0.22, 0.29, fill=False, color=\"k\", lw=1.5, \n#        zorder=1000, transform=fig.transFigure, figure=fig\n#    )\n\n#fig.patches.extend([rect, rect2])\nplt.tight_layout()\n\nplt.savefig(\"abcoli.svg\")\nplt.savefig(\"abcoli.pdf\")\nplt.savefig(\"abcoli.png\")\n\n\n\n\n\n\n\n\n\nT = np.arange(10, 50, 0.001)\n\nfig, axarr = plt.subplots(3, 5, figsize=(6.81102, 4), dpi=300, sharex=True, sharey=True)\n\nplt.rcParams.update({'font.size': 8})\n\nfor i, drug in enumerate(drugs):\n    ax = axarr[i // 5, i % 5]\n    subset = abdata[abdata[\"drug1name\"] == drug]\n\n    ax.set_xticks([10, 20, 30, 40, 50])\n    ax.set_yticks([0, 0.5, 1])\n\n\n    #plt.subplot(3, 5, i+1)\n    # Plot datapoints.\n    Tdata = subset[\"T\"]\n    rdata = subset[\"OD\"]\n\n    ax.plot(Tdata, rdata, '^', markersize=3)\n\n             \n    \n    \n\n    par = params[\"epc\"][drug][\"x\"]\n    ax.plot(T, expprodcurve(T, *par[:-1]), color=\"purple\", linewidth=1, alpha=0.8)\n\n    par = params[\"kum\"][drug][\"x\"]\n    ax.plot(T, kumaraswarmy(T, *par[:-1]), color=\"blue\", linewidth=1, alpha=0.8)\n\n    par = params[\"flexTPC\"][drug][\"x\"]\n    ax.plot(T, flexTPC(T, *par[:-1]), color=\"darkgreen\", linewidth=1, alpha=0.8)\n    \n    ax.set_ylim(-0.025, 1.0)\n\n    if drug != 'WT':\n        ax.set_title(drug, fontsize=10)\n    else:\n        ax.set_title('no drug', fontsize=10)\n    if drug in (\"TMP\", \"TOB\", \"WT\"):\n        ax.set_xlabel(\"Temperature [°C]\", fontsize=8)\n    if drug in (\"AMP\", \"GEN\", \"TMP\"):\n        ax.set_ylabel(\"OD\", fontsize=10)\n        \naxarr[-1, -1].axis(\"off\")\naxarr[-1, -2].axis(\"off\")\n\n\n#ERY\n#rect = plt.Rectangle(\n#        # (lower-left corner), width, height\n#        (0.63, 0.69), 0.18, 0.29, fill=False, color=\"k\", lw=1.5, \n#        zorder=1000, transform=fig.transFigure, figure=fig\n#    )\n\n#GEN\n#rect2 = plt.Rectangle(\n#        # (lower-left corner), width, height\n#        (0.04, 0.4), 0.22, 0.29, fill=False, color=\"k\", lw=1.5, \n#        zorder=1000, transform=fig.transFigure, figure=fig\n#    )\n\n#fig.patches.extend([rect, rect2])\nplt.tight_layout()\n\nplt.savefig(\"abcoli_pc_kum.svg\")\nplt.savefig(\"abcoli_pc_kum.pdf\")\nplt.savefig(\"abcoli_pc_kum.png\")\n\n\n\n\n\n\n\n\n\nn_params = {'b1':4, 'b2':5, 'flexTPC':6, 'epc':6, 'kum':6} # Includes standard deviation.\nparam_matrix = {model:np.zeros((13, n_params[model])) for model in models}\n\nfor model in models:\n    for i, drug in enumerate(drugs):\n        param_matrix[model][i, ] = params[model][drug][\"x\"]\n\ncolnames = {'b1':['Tmin', 'Tmax', 'c', 'sigma2'],\n            'b2':['Tmin', 'Tmax', 'c', 'm', 'sigma2'],\n            'flexTPC':['Tmin', 'Tmax', 'rmax', 'alpha', 'beta', 'sigma2'],\n           'epc':['Tmin', 'Tmax', 'c', 'kI', 'kU', 'sigma2'],\n           'kum':['Tmin', 'Tmax', 'c', 'a', 'b', 'sigma2']}\n\nparam_df = {model:pd.DataFrame(param_matrix[model], columns=colnames[model]) for model in models}\nfor model in models:\n    param_df[model]['drug'] = drugs\n    param_df[model] = param_df[model][['drug'] + colnames[model]]\n    param_df[model].to_csv(f'abcoli_params_{model}.csv', index=False)\n\n\nparam_df['epc']\n\n\n\n\n\n\n\n\ndrug\nTmin\nTmax\nc\nkI\nkU\nsigma2\n\n\n\n\n0\nAMP\n20.0\n44.007843\n0.824156\n1.614674e-01\n0.493304\n0.001229\n\n\n1\nCLI\n20.0\n44.054805\n0.866008\n8.242660e-02\n0.713136\n0.001949\n\n\n2\nCPR\n20.0\n44.000001\n71240.939870\n5.623456e-07\n180349.336808\n0.007429\n\n\n3\nERY\n20.0\n44.000002\n138202.531337\n1.494974e-07\n95836.460775\n0.016318\n\n\n4\nFOX\n20.0\n44.322608\n0.850540\n5.915439e-02\n0.712125\n0.000682\n\n\n5\nGEN\n20.0\n43.586791\n0.336935\n4.717883e-01\n13.322271\n0.001133\n\n\n6\nLVX\n20.0\n44.356000\n1.096169\n7.229880e-02\n0.930030\n0.001773\n\n\n7\nNTR\n20.0\n44.005670\n1665.495450\n2.757200e-05\n0.455426\n0.001853\n\n\n8\nSTR\n20.0\n44.010421\n0.397158\n3.059134e-01\n0.961540\n0.001156\n\n\n9\nTET\n20.0\n44.694587\n1.443680\n3.744536e-02\n0.612516\n0.001204\n\n\n10\nTMP\n20.0\n43.175693\n0.890553\n7.821355e-02\n1.202987\n0.000335\n\n\n11\nTOB\n20.0\n44.008580\n0.464558\n2.443800e-01\n0.697478\n0.000537\n\n\n12\nWT\n20.0\n44.106355\n0.815505\n1.522118e-01\n7.663247\n0.000530\n\n\n\n\n\n\n\n\nnll = np.zeros((len(drugs), len(models)))\naic = np.zeros((len(drugs), len(models)))\nbic = np.zeros((len(drugs), len(models)))\nloo_nll = np.zeros((len(drugs), len(models)))\n\np = [4, 5, 6, 6, 6] # Number of parameters.\nn = 7 # Number of datapoints.\n\nfor i, d in enumerate(drugs):\n    for j, m in enumerate(models): \n        nll[i, j] = params[m][d][\"fun\"]\n        aic[i, j] = AIC(nll[i, j], p[j])\n        bic[i, j] = BIC(nll[i, j], n, p[j])\n        loo_nll[i, j] = np.mean(loocv_nll[m][d])\n\n\ndrugs\n\n['AMP',\n 'CLI',\n 'CPR',\n 'ERY',\n 'FOX',\n 'GEN',\n 'LVX',\n 'NTR',\n 'STR',\n 'TET',\n 'TMP',\n 'TOB',\n 'WT']\n\n\n\nfor drug in drugs:\n    print(drug, np.round(params[\"flexTPC\"][drug][\"x\"], 3) )\n\nAMP [2.00e+01 4.40e+01 7.57e-01 6.42e-01 4.38e-01 1.00e-03]\nCLI [2.00e+01 4.40e+01 6.61e-01 7.65e-01 3.87e-01 2.00e-03]\nCPR [2.0000e+01 4.4195e+01 8.4600e-01 7.9500e-01 2.4300e-01 2.0000e-03]\nERY [2.00e+01 6.00e+01 5.15e-01 4.66e-01 9.70e-02 5.00e-03]\nFOX [2.00e+01 4.40e+01 5.59e-01 8.80e-01 3.52e-01 1.00e-03]\nGEN [2.00e+01 4.40e+01 3.59e-01 5.75e-01 6.18e-01 2.00e-03]\nLVX [2.0000e+01 4.4026e+01 8.2400e-01 7.8800e-01 3.6300e-01 2.0000e-03]\nNTR [2.00e+01 4.40e+01 7.69e-01 7.69e-01 3.14e-01 1.00e-03]\nSTR [2.00e+01 4.40e+01 4.25e-01 6.04e-01 5.01e-01 2.00e-03]\nTET [2.0000e+01 4.4001e+01 7.1300e-01 8.7200e-01 3.3100e-01 1.0000e-03]\nTMP [20.    44.     0.681  0.782  0.377  0.   ]\nTOB [2.00e+01 4.40e+01 4.77e-01 6.24e-01 4.73e-01 1.00e-03]\nWT [20.    44.003  0.794  0.853  0.437  0.   ]\n\n\n\ndfAIC = pd.DataFrame(np.round(aic, 2), columns=models, index=drugs)\ndfBIC = pd.DataFrame(np.round(bic, 2), columns=models, index=drugs)\ndfLOOCV = pd.DataFrame(np.round(loo_nll, 2), columns=models, index=drugs)\n\n\ndfAIC\n\n\n\n\n\n\n\n\nb1\nb2\nflexTPC\nepc\nkum\n\n\n\n\nAMP\n-5.56\n-8.64\n-17.12\n-15.04\n-16.95\n\n\nCLI\n-11.98\n-10.51\n-11.38\n-11.82\n-11.20\n\n\nCPR\n-2.92\n-7.16\n-11.75\n-2.45\n-11.03\n\n\nERY\n3.28\n0.25\n-4.95\n3.06\n-3.54\n\n\nFOX\n-18.14\n-16.31\n-21.11\n-19.17\n-21.33\n\n\nGEN\n-3.16\n-3.96\n-11.56\n-15.61\n-5.99\n\n\nLVX\n-12.93\n-10.96\n-12.43\n-12.48\n-11.90\n\n\nNTR\n-16.03\n-16.47\n-17.42\n-12.17\n-16.31\n\n\nSTR\n-5.39\n-7.42\n-12.87\n-15.47\n-12.54\n\n\nTET\n-15.07\n-13.83\n-14.88\n-15.19\n-10.33\n\n\nTMP\n-18.63\n-16.74\n-24.29\n-24.14\n-23.32\n\n\nTOB\n-7.15\n-9.40\n-17.35\n-20.84\n-16.76\n\n\nWT\n-5.49\n-6.43\n-25.33\n-20.93\n-25.23\n\n\n\n\n\n\n\n\ndfBIC\n\n\n\n\n\n\n\n\nb1\nb2\nflexTPC\nepc\nkum\n\n\n\n\nAMP\n-5.78\n-8.91\n-17.45\n-15.37\n-17.27\n\n\nCLI\n-12.19\n-10.78\n-11.70\n-12.14\n-11.52\n\n\nCPR\n-3.14\n-7.43\n-12.07\n-2.77\n-11.35\n\n\nERY\n3.07\n-0.02\n-5.27\n2.73\n-3.87\n\n\nFOX\n-18.35\n-16.58\n-21.43\n-19.49\n-21.66\n\n\nGEN\n-3.38\n-4.23\n-11.88\n-15.94\n-6.31\n\n\nLVX\n-13.15\n-11.23\n-12.75\n-12.81\n-12.22\n\n\nNTR\n-16.25\n-16.74\n-17.74\n-12.49\n-16.64\n\n\nSTR\n-5.61\n-7.69\n-13.19\n-15.80\n-12.87\n\n\nTET\n-15.28\n-14.10\n-15.20\n-15.51\n-10.65\n\n\nTMP\n-18.85\n-17.01\n-24.61\n-24.47\n-23.64\n\n\nTOB\n-7.37\n-9.67\n-17.68\n-21.16\n-17.09\n\n\nWT\n-5.70\n-6.70\n-25.65\n-21.26\n-25.56\n\n\n\n\n\n\n\n\ndfLOOCV\n\n\n\n\n\n\n\n\nb1\nb2\nflexTPC\nepc\nkum\n\n\n\n\nAMP\n-0.97\n-1.33\n-2.08\n-1.93\n-2.07\n\n\nCLI\n-1.43\n-1.47\n-1.67\n-1.70\n-1.66\n\n\nCPR\n-0.78\n-1.23\n-1.70\n-1.03\n-1.64\n\n\nERY\n-0.34\n-0.70\n-1.21\n-0.64\n-1.11\n\n\nFOX\n-1.87\n-1.88\n-2.36\n-2.23\n-2.38\n\n\nGEN\n-0.80\n-1.00\n-1.68\n-1.97\n-1.28\n\n\nLVX\n-1.50\n-1.50\n-1.74\n-1.75\n-1.71\n\n\nNTR\n-1.72\n-1.89\n-2.10\n-1.73\n-2.02\n\n\nSTR\n-0.96\n-1.24\n-1.78\n-1.96\n-1.75\n\n\nTET\n-1.65\n-1.70\n-1.92\n-1.94\n-1.59\n\n\nTMP\n-1.90\n-1.91\n-2.59\n-2.58\n-2.52\n\n\nTOB\n-1.08\n-1.39\n-2.10\n-2.35\n-2.05\n\n\nWT\n-0.96\n-1.17\n-2.67\n-2.35\n-2.66\n\n\n\n\n\n\n\n\ndef get_delta(df):\n    '''\n    Turns results into difference relative to best model.\n    '''\n    best = np.min(df, axis=1) \n    return(df.subtract(np.min(df, axis=1), axis=0))\n\n\nget_delta(dfAIC)\n\n\n\n\n\n\n\n\nb1\nb2\nflexTPC\nepc\nkum\n\n\n\n\nAMP\n11.56\n8.48\n0.00\n2.08\n0.17\n\n\nCLI\n0.00\n1.47\n0.60\n0.16\n0.78\n\n\nCPR\n8.83\n4.59\n0.00\n9.30\n0.72\n\n\nERY\n8.23\n5.20\n0.00\n8.01\n1.41\n\n\nFOX\n3.19\n5.02\n0.22\n2.16\n0.00\n\n\nGEN\n12.45\n11.65\n4.05\n0.00\n9.62\n\n\nLVX\n0.00\n1.97\n0.50\n0.45\n1.03\n\n\nNTR\n1.39\n0.95\n0.00\n5.25\n1.11\n\n\nSTR\n10.08\n8.05\n2.60\n0.00\n2.93\n\n\nTET\n0.12\n1.36\n0.31\n0.00\n4.86\n\n\nTMP\n5.66\n7.55\n0.00\n0.15\n0.97\n\n\nTOB\n13.69\n11.44\n3.49\n0.00\n4.08\n\n\nWT\n19.84\n18.90\n0.00\n4.40\n0.10\n\n\n\n\n\n\n\n\nget_delta(dfBIC)\n\n\n\n\n\n\n\n\nb1\nb2\nflexTPC\nepc\nkum\n\n\n\n\nAMP\n11.67\n8.54\n0.00\n2.08\n0.18\n\n\nCLI\n0.00\n1.41\n0.49\n0.05\n0.67\n\n\nCPR\n8.93\n4.64\n0.00\n9.30\n0.72\n\n\nERY\n8.34\n5.25\n0.00\n8.00\n1.40\n\n\nFOX\n3.31\n5.08\n0.23\n2.17\n0.00\n\n\nGEN\n12.56\n11.71\n4.06\n0.00\n9.63\n\n\nLVX\n0.00\n1.92\n0.40\n0.34\n0.93\n\n\nNTR\n1.49\n1.00\n0.00\n5.25\n1.10\n\n\nSTR\n10.19\n8.11\n2.61\n0.00\n2.93\n\n\nTET\n0.23\n1.41\n0.31\n0.00\n4.86\n\n\nTMP\n5.76\n7.60\n0.00\n0.14\n0.97\n\n\nTOB\n13.79\n11.49\n3.48\n0.00\n4.07\n\n\nWT\n19.95\n18.95\n0.00\n4.39\n0.09\n\n\n\n\n\n\n\n\nget_delta(dfLOOCV)\n\n\n\n\n\n\n\n\nb1\nb2\nflexTPC\nepc\nkum\n\n\n\n\nAMP\n1.11\n0.75\n0.00\n0.15\n0.01\n\n\nCLI\n0.27\n0.23\n0.03\n0.00\n0.04\n\n\nCPR\n0.92\n0.47\n0.00\n0.67\n0.06\n\n\nERY\n0.87\n0.51\n0.00\n0.57\n0.10\n\n\nFOX\n0.51\n0.50\n0.02\n0.15\n0.00\n\n\nGEN\n1.17\n0.97\n0.29\n0.00\n0.69\n\n\nLVX\n0.25\n0.25\n0.01\n0.00\n0.04\n\n\nNTR\n0.38\n0.21\n0.00\n0.37\n0.08\n\n\nSTR\n1.00\n0.72\n0.18\n0.00\n0.21\n\n\nTET\n0.29\n0.24\n0.02\n0.00\n0.35\n\n\nTMP\n0.69\n0.68\n0.00\n0.01\n0.07\n\n\nTOB\n1.27\n0.96\n0.25\n0.00\n0.30\n\n\nWT\n1.71\n1.50\n0.00\n0.32\n0.01\n\n\n\n\n\n\n\n\nget_delta(dfLOOCV[['b1', 'b2', 'flexTPC']])\n\n\n\n\n\n\n\n\nb1\nb2\nflexTPC\n\n\n\n\nAMP\n7.78\n5.24\n0.0\n\n\nCLI\n1.70\n1.43\n0.0\n\n\nCPR\n6.41\n3.29\n0.0\n\n\nERY\n6.11\n3.60\n0.0\n\n\nFOX\n3.48\n3.39\n0.0\n\n\nGEN\n6.20\n4.80\n0.0\n\n\nLVX\n1.74\n1.73\n0.0\n\n\nNTR\n2.69\n1.48\n0.0\n\n\nSTR\n5.74\n3.73\n0.0\n\n\nTET\n1.91\n1.52\n0.0\n\n\nTMP\n4.82\n4.77\n0.0\n\n\nTOB\n7.11\n4.98\n0.0\n\n\nWT\n11.92\n10.45\n0.0\n\n\n\n\n\n\n\n\nnp.mean(get_delta(dfLOOCV), axis=0)\n\nb1         5.630769\nb2         4.307692\nflexTPC    0.430000\nepc        1.205385\nkum        1.044615\ndtype: float64\n\n\n\npd.DataFrame({'LOOCV':np.mean(get_delta(dfLOOCV), axis=0)})\n\n\n\n\n\n\n\n\nLOOCV\n\n\n\n\nb1\n5.630769\n\n\nb2\n4.307692\n\n\nflexTPC\n0.430000\n\n\nepc\n1.205385\n\n\nkum\n1.044615"
  },
  {
    "objectID": "model_comparison/Botrana.html",
    "href": "model_comparison/Botrana.html",
    "title": "Comparison of Briere and flexTPC models in insect development data",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport random\nrandom.seed(42) # Set seed for reproducibility\n\nfrom matplotlib import pyplot as plt\n\nfrom scipy.optimize import minimize\nfrom sklearn.model_selection import LeaveOneOut\ndef briere1(T, Tmin=10.0, Tmax=50.0, c=1.0):\n    result = np.zeros(len(T))\n    cond = (Tmin &lt; T) & (T &lt; Tmax)\n    result[cond] = c * T[cond] * np.exp(np.log(T[cond] - Tmin) + 0.5 * np.log(Tmax - T[cond]))\n    return result\n\ndef briere2(T, Tmin=10.0, Tmax=50.0, c=1.0, b=2.0):\n    result = np.zeros(len(T))\n    cond = (Tmin &lt; T) & (T &lt; Tmax)\n    result[cond] = c * T[cond] * np.exp(np.log(T[cond] - Tmin) +  np.log(Tmax - T[cond]) / b)\n    return result\n\n# Fully biologically interpretable parametrization of flexTPC model.\ndef flexTPC(T, Tmin=10.0, Tmax=50.0, rmax=1.0, α=0.8, β=0.2):\n    result = np.zeros(len(T))\n    cond = (Tmin &lt; T) & (T &lt; Tmax)\n    s = α * (1.0 - α) / β**2\n    result[cond] = rmax * np.exp(s * (α * (np.log(T[cond] - Tmin) - np.log(α)) +\n                                      (1 - α) * (np.log(Tmax - T[cond]) - np.log(1 - α) )\n                                       - np.log(Tmax - Tmin)))\n    return result\n\n# Exponential product curve.\ndef expprodcurve(T, Tmin=10.0, Tmax=50.0, c=1.0, kI=0.1, kU=0.5):\n    result = np.zeros(len(T))\n    cond = (Tmin &lt; T) & (T &lt; Tmax)\n    result[cond] = c * (1.0 - np.exp(-kI*(T[cond] - Tmin))) * (1.0 - np.exp(-kU*(Tmax - T[cond])))\n    return result\n\n# Kumaraswarmy distribution TPC.\ndef kumaraswarmy(T, Tmin=10.0, Tmax=50.0, c=1.0, a=5.0, b=5.0):\n    result = np.zeros(len(T))\n    cond = (Tmin &lt; T) & (T &lt; Tmax)\n    τ = (T[cond] - Tmin) / (Tmax - Tmin)\n    result[cond] = c * τ**(a - 1) * (1.0 - τ**a)**(b - 1)\n    return result\nTvals = np.arange(5, 55, 0.1)\nplt.plot(Tvals, expprodcurve(Tvals, kI=0.1, kU=0.3))\ndef nloglik(θ, Tdata, rdata, model=flexTPC):\n    params, σ2 = θ[:-1], θ[-1]\n    Tmin, Tmax = θ[:2]\n    outside = (Tdata &lt; Tmin) | (Tdata &gt; Tmax)\n    inside = ~outside\n    n = len(rdata[inside])\n    if np.any(rdata[outside] &gt; 0.0):\n        return np.inf\n    else:\n        return 0.5 * (n * np.log(2*np.pi*σ2) + np.sum((rdata[inside] - model(Tdata[inside], *params))**2 / σ2))\n    \ndef AIC(nll, p):\n    return 2 * nll + 2 * p \n\ndef BIC(nll, n, p):\n    return 2 * nll + p * np.log(n)"
  },
  {
    "objectID": "model_comparison/Botrana.html#l.-botrana-development-dataset",
    "href": "model_comparison/Botrana.html#l.-botrana-development-dataset",
    "title": "Comparison of Briere and flexTPC models in insect development data",
    "section": "L. botrana development dataset",
    "text": "L. botrana development dataset\nFirst, let’s compare the model performance in Briere’s original dataset.\n\nbotrana = pd.read_csv(\"briere_data_L_botrana.csv\")\nstages = [\"eggs\", \"i1\", \"i2\", \"i3\", \"i4\", \"i5\", \"pupae\"]\n\n\nbotrana\n\n\n\n\n\n\n\n\nT\nn_eggs\neggs\nn_i1\ni1\nn_i2\ni2\nn_i3\ni3\nn_i4\ni4\nn_i5\ni5\nn_pupae\npupae\n\n\n\n\n0\n8\n0\ninf\n0\ninf\n0\ninf\n0\ninf\n0\ninf\n0\ninf\n0\ninf\n\n\n1\n10\n0\ninf\n28\n25.9\n14\n23.2\n0\ninf\n0\ninf\n0\ninf\n0\ninf\n\n\n2\n12\n77\n26.8\n48\n23.4\n23\n16.1\n15\n12.7\n12\n15.7\n11\n25.5\n0\ninf\n\n\n3\n14\n104\n19.4\n45\n15.1\n36\n10.6\n32\n9.8\n31\n10.6\n31\n17.9\n6\n48.0\n\n\n4\n16\n369\n15.6\n47\n13.2\n41\n8.9\n36\n8.3\n35\n10.0\n33\n13.5\n25\n35.5\n\n\n5\n18\n600\n11.0\n44\n11.7\n41\n7.1\n40\n5.6\n38\n7.0\n36\n13.3\n41\n23.2\n\n\n6\n20\n674\n8.2\n44\n5.9\n44\n4.8\n40\n4.7\n37\n5.1\n35\n7.6\n41\n15.1\n\n\n7\n22\n837\n7.0\n50\n4.2\n48\n3.8\n46\n4.0\n42\n4.9\n41\n7.1\n45\n12.9\n\n\n8\n24\n662\n5.3\n49\n3.9\n45\n3.1\n44\n3.8\n44\n3.7\n40\n6.5\n50\n10.2\n\n\n9\n26\n696\n5.1\n50\n3.7\n45\n3.0\n45\n3.2\n44\n3.6\n40\n6.2\n50\n8.6\n\n\n10\n28\n559\n5.0\n50\n3.6\n43\n2.9\n40\n3.0\n40\n3.6\n38\n6.0\n46\n7.9\n\n\n11\n30\n210\n4.9\n46\n3.3\n46\n2.9\n45\n2.9\n44\n2.9\n40\n5.8\n41\n7.8\n\n\n12\n32\n0\ninf\n47\n4.5\n43\n3.4\n42\n3.4\n38\n3.9\n26\n6.9\n45\n7.7\n\n\n13\n34\n0\ninf\n30\n6.7\n27\n6.4\n0\ninf\n0\ninf\n0\ninf\n19\ninf\n\n\n\n\n\n\n\n\n# Find initial estimates of minimum and maximum temperatures from data.\nTlims = {}\nfor i, stage in enumerate(stages):\n    T = botrana[\"T\"][botrana[stage] &lt; np.inf]\n    Tlims[stage] = (min(T) - 0.5, max(T) + 0.5)\n    print(stage, Tlims[stage])\n\n# Set initial estimates of maximum trait value.\ninit_rmax = {'eggs': 0.2,\n             'i1':0.2,\n             'i2':0.2,\n             'i3':0.2,\n             'i4':0.2,\n             'i5':0.2,\n            'pupae':0.2}\n\n# Set initial parameter values for all models.\ninitial_flex = {stage:[7.0, 36.0, init_rmax[stage], 0.8, 0.1, 0.015] for stage in stages}\ninitial_b1 = {stage:[7.0, 36.0, 7e-5, 0.015] for stage in stages}\ninitial_b2 = {stage:[7.0, 36.0, 7e-5, 2.0, 0.015] for stage in stages}\ninitial_epc = {stage:[7.0, 36.0, init_rmax[stage], 0.1, 0.3, 0.015] for stage in stages}\ninitial_kum = {stage:[7.0, 36.0, 0.5, 3, 2, 0.015] for stage in stages}\n\neggs (11.5, 30.5)\ni1 (9.5, 34.5)\ni2 (9.5, 34.5)\ni3 (11.5, 32.5)\ni4 (11.5, 32.5)\ni5 (11.5, 32.5)\npupae (13.5, 32.5)\n\n\n\nTvals = np.arange(5, 55, 0.1)\nplt.plot(Tvals, kumaraswarmy(Tvals, Tmin=7.0, Tmax=36.0, c=1.0, a=3, b=2))\n\n\n\n\n\n\n\n\n\nloo = LeaveOneOut()\nloo.get_n_splits(botrana[\"T\"])\n\n14\n\n\n\n# Calculate leave-one-out cross validated log likelihood.\nparams = {\"flexTPC\":{}, \"b1\":{}, \"b2\":{}, \"epc\":{}, \"kum\":{}}\nmodels = [\"b1\", \"b2\", \"flexTPC\", \"epc\", \"kum\"]\nmethod = \"Nelder-Mead\"\nloocv_nll = {model:{stage:[] for stage in stages } for model in models}\n\nfor i, y in enumerate(stages):\n    Tdata = np.array(botrana[\"T\"])\n    rdata = 1.0 / np.array(botrana[y])\n    \n    for train, test in loo.split(Tdata):\n        T_train, r_train = Tdata[train], rdata[train]\n        T_test, r_test = Tdata[test], rdata[test]\n        \n        params[\"b1\"][y] = minimize(nloglik, initial_b1[y],\n                            bounds=[(0.0, 20.0), \n                                    (20.0, 60.0), \n                                    (0.0, 1.0),  \n                                    (0, 1.0)],\n                      args=(T_train, r_train, briere1), options={\"maxiter\":100000}, method=method)\n        θ = params[\"b1\"][y][\"x\"]\n        loocv_nll[\"b1\"][y].append(nloglik(θ, T_test, r_test, model=briere1))\n        \n        params[\"b2\"][y] = minimize(nloglik, initial_b2[y],\n                            bounds=[(0.0, 20.0), \n                                    (20.0, 60.0), \n                                    (0.0, 1.0), \n                                    (1.0, 20.0),\n                                    (0, 1.0)],\n                      args=(Tdata, rdata, briere2), options={\"maxiter\":100000}, method=method)\n        θ = params[\"b2\"][y][\"x\"]\n        loocv_nll[\"b2\"][y].append(nloglik(θ, T_test, r_test, model=briere2))\n        \n        params[\"flexTPC\"][y] = minimize(nloglik, initial_flex[y],\n                                bounds=[(0.0, 20.0), \n                                        (20.0, 60.0), \n                                        (0.0, 1.0), \n                                        (0.0, 1.0), \n                                        (0.0, np.inf), \n                                        (0, 1.0)],\n                          args=(Tdata, rdata, flexTPC), options={\"maxiter\":100000}, method=method)\n        θ = params[\"flexTPC\"][y][\"x\"]\n        loocv_nll[\"flexTPC\"][y].append(nloglik(θ, T_test, r_test, model=flexTPC))\n\n        params[\"epc\"][y] = minimize(nloglik, initial_epc[y],\n                                bounds=[(0.0, 20.0), \n                                        (20.0, 60.0), \n                                        (0.0, 1.0), \n                                        (0.0, np.inf), \n                                        (0.0, np.inf), \n                                        (0, 1.0)],\n                          args=(Tdata, rdata, expprodcurve), options={\"maxiter\":100000}, method=method)\n        θ = params[\"epc\"][y][\"x\"]\n        loocv_nll[\"epc\"][y].append(nloglik(θ, T_test, r_test, model=expprodcurve))\n\n        params[\"kum\"][y] = minimize(nloglik, initial_kum[y],\n                                bounds=[(0.0, 20.0), \n                                        (20.0, 60.0), \n                                        (0.0, np.inf), \n                                        (1.0, np.inf), \n                                        (1.0, np.inf), \n                                        (0, 1.0)],\n                          args=(Tdata, rdata, kumaraswarmy), options={\"maxiter\":100000}, method=method)\n        θ = params[\"kum\"][y][\"x\"]\n        loocv_nll[\"kum\"][y].append(nloglik(θ, T_test, r_test, model=kumaraswarmy))\n\n/var/folders/g8/wjqhnz1n01d5l4pdv8cnykw40000gp/T/ipykernel_90008/2868019315.py:10: RuntimeWarning: divide by zero encountered in log\n  return 0.5 * (n * np.log(2*np.pi*σ2) + np.sum((rdata[inside] - model(Tdata[inside], *params))**2 / σ2))\n/var/folders/g8/wjqhnz1n01d5l4pdv8cnykw40000gp/T/ipykernel_90008/2868019315.py:10: RuntimeWarning: divide by zero encountered in divide\n  return 0.5 * (n * np.log(2*np.pi*σ2) + np.sum((rdata[inside] - model(Tdata[inside], *params))**2 / σ2))\n/var/folders/g8/wjqhnz1n01d5l4pdv8cnykw40000gp/T/ipykernel_90008/2868019315.py:10: RuntimeWarning: invalid value encountered in scalar add\n  return 0.5 * (n * np.log(2*np.pi*σ2) + np.sum((rdata[inside] - model(Tdata[inside], *params))**2 / σ2))\n\n\n\n# Fit to entire dataset.\n\nparams = {\"flexTPC\":{}, \"b1\":{}, \"b2\":{}, \"epc\":{}, \"kum\":{}}\nmodels = [\"b1\", \"b2\", \"flexTPC\", \"epc\", \"kum\"]\nmethod = \"Nelder-Mead\"\n\nfor i, y in enumerate(stages):\n    Tdata = np.array(botrana[\"T\"])\n    rdata = 1.0 / np.array(botrana[y])\n    \n    \n    params[\"b1\"][y] = minimize(nloglik, initial_b1[y],\n                        bounds=[(0.0, 20.0), \n                                (20.0, 60.0), \n                                (0.0, 1.0),  \n                                (0, 1.0)],\n                  args=(Tdata, rdata, briere1), options={\"maxiter\":100000}, method=method)\n    params[\"b2\"][y] = minimize(nloglik, initial_b2[y],\n                        bounds=[(0.0, 20.0), \n                                (20.0, 60.0), \n                                (0.0, 1.0), \n                                (1.0, 20.0),\n                                (0, 1.0)],\n                  args=(Tdata, rdata, briere2), options={\"maxiter\":100000}, method=method)\n    params[\"flexTPC\"][y] = minimize(nloglik, initial_flex[y],\n                        bounds=[(0.0, 20.0), \n                                (20.0, 60.0), \n                                (0.0, 1.0), \n                                (0.0, 1.0), \n                                (0.0, np.inf), \n                                (0, 1.0)],\n                  args=(Tdata, rdata, flexTPC), options={\"maxiter\":100000}, method=method)\n    params[\"epc\"][y] = minimize(nloglik, initial_epc[y],\n                                bounds=[(0.0, 20.0), \n                                        (20.0, 60.0), \n                                        (0.0, 1.0), \n                                        (0.0, np.inf), \n                                        (0.0, np.inf), \n                                        (0, 1.0)],\n                          args=(Tdata, rdata, expprodcurve), options={\"maxiter\":100000}, method=method)\n    params[\"kum\"][y] = minimize(nloglik, initial_kum[y],\n                                bounds=[(0.0, 20.0), \n                                        (20.0, 60.0), \n                                        (0.0, np.inf), \n                                        (1.0, np.inf), \n                                        (1.0, np.inf), \n                                        (0, 1.0)],\n                          args=(Tdata, rdata, kumaraswarmy), options={\"maxiter\":100000}, method=method)\n\n/var/folders/g8/wjqhnz1n01d5l4pdv8cnykw40000gp/T/ipykernel_90008/2868019315.py:10: RuntimeWarning: divide by zero encountered in log\n  return 0.5 * (n * np.log(2*np.pi*σ2) + np.sum((rdata[inside] - model(Tdata[inside], *params))**2 / σ2))\n/var/folders/g8/wjqhnz1n01d5l4pdv8cnykw40000gp/T/ipykernel_90008/2868019315.py:10: RuntimeWarning: divide by zero encountered in divide\n  return 0.5 * (n * np.log(2*np.pi*σ2) + np.sum((rdata[inside] - model(Tdata[inside], *params))**2 / σ2))\n/var/folders/g8/wjqhnz1n01d5l4pdv8cnykw40000gp/T/ipykernel_90008/2868019315.py:10: RuntimeWarning: invalid value encountered in scalar add\n  return 0.5 * (n * np.log(2*np.pi*σ2) + np.sum((rdata[inside] - model(Tdata[inside], *params))**2 / σ2))\n\n\n\nT = np.arange(5, 36.5, 0.001)\n#plt.figure(figsize=(5, 6.5), dpi=300)\n#plt.figure(figsize=(4.33, 6.5), dpi=300)\nalpha=0.8\n\nfig, axarr = plt.subplots(2, 4, figsize=(6.5, 3), dpi=300, sharex=True, sharey=True)\n\nfor i, y in enumerate(stages):\n    ax = axarr[i // 4, i % 4]\n    Tdata = np.array(botrana[\"T\"])\n    rdata = 1.0 / np.array(botrana[y])\n\n    ax.plot(Tdata, rdata, '^')\n    \n    # Plot fitted curves by models.\n    par = params[\"b1\"][y][\"x\"]\n    ax.plot(T, briere1(T, *par[:-1]), color=\"firebrick\", alpha=alpha)\n             \n    par = params[\"b2\"][y][\"x\"]\n    ax.plot(T, briere2(T, *par[:-1]), color=\"orange\", alpha=alpha)\n    \n    par = params[\"flexTPC\"][y][\"x\"]\n    ax.plot(T, flexTPC(T, *par[:-1]), color=\"darkgreen\", alpha=alpha)\n\n    #par = params[\"epc\"][y][\"x\"]\n    #ax.plot(T, expprodcurve(T, *par[:-1]), color=\"purple\", alpha=alpha)\n\n    #par = params[\"kum\"][y][\"x\"]\n    #ax.plot(T, kumaraswarmy(T, *par[:-1]), color=\"blue\", alpha=alpha)\n    \n    ax.set_ylim(-0.025, 0.4)\n    if y in (\"eggs\", \"i4\"):\n        ax.set_ylabel(\"Dev. rate [1 / day]\", fontsize=9)\n    if y in (\"i4\", \"i5\", \"pupae\"):\n        ax.set_xlabel(\"Temperature [°C]\", fontsize=9)\n    ax.set_title(y, fontsize=10)\naxarr[-1, -1].axis(\"off\")\nplt.tight_layout()\nplt.savefig(\"botrana.svg\")\nplt.savefig(\"botrana.tiff\")\nplt.savefig(\"botrana.png\")\nplt.savefig(\"botrana.pdf\")\n\n\n\n\n\n\n\n\n\nT = np.arange(5, 36.5, 0.001)\n#plt.figure(figsize=(5, 6.5), dpi=300)\n#plt.figure(figsize=(4.33, 6.5), dpi=300)\nalpha=0.8\n\nfig, axarr = plt.subplots(2, 4, figsize=(6.5, 3), dpi=300, sharex=True, sharey=True)\n\nfor i, y in enumerate(stages):\n    ax = axarr[i // 4, i % 4]\n    Tdata = np.array(botrana[\"T\"])\n    rdata = 1.0 / np.array(botrana[y])\n\n    ax.plot(Tdata, rdata, '^')\n    \n    # Plot fitted curves by models.\n    #par = params[\"b1\"][y][\"x\"]\n    #ax.plot(T, briere1(T, *par[:-1]), color=\"firebrick\", alpha=alpha)\n             \n    #par = params[\"b2\"][y][\"x\"]\n    #ax.plot(T, briere2(T, *par[:-1]), color=\"orange\", alpha=alpha)\n    \n    \n\n    par = params[\"epc\"][y][\"x\"]\n    ax.plot(T, expprodcurve(T, *par[:-1]), color=\"purple\", alpha=alpha)\n\n    par = params[\"kum\"][y][\"x\"]\n    ax.plot(T, kumaraswarmy(T, *par[:-1]), color=\"blue\", alpha=alpha)\n\n    par = params[\"flexTPC\"][y][\"x\"]\n    ax.plot(T, flexTPC(T, *par[:-1]), color=\"darkgreen\", alpha=alpha)\n    \n    ax.set_ylim(-0.025, 0.4)\n    if y in (\"eggs\", \"i4\"):\n        ax.set_ylabel(\"Dev. rate [1 / day]\", fontsize=9)\n    if y in (\"i4\", \"i5\", \"pupae\"):\n        ax.set_xlabel(\"Temperature [°C]\", fontsize=9)\n    ax.set_title(y, fontsize=10)\naxarr[-1, -1].axis(\"off\")\nplt.tight_layout()\nplt.savefig(\"botrana_pc_kum.svg\")\nplt.savefig(\"botrana_pc_kum.pdf\")\n\n\n\n\n\n\n\n\n\nn_params = {'b1':4, 'b2':5, 'flexTPC':6, 'epc':6, 'kum':6} # Includes standard deviation.\nparam_matrix = {model:np.zeros((7, n_params[model])) for model in models}\n\nfor model in models:\n    for i, stage in enumerate(stages):\n        param_matrix[model][i, ] = params[model][stage][\"x\"]\n\ncolnames = {'b1':['Tmin', 'Tmax', 'c', 'sigma2'],\n            'b2':['Tmin', 'Tmax', 'c', 'm', 'sigma2'],\n            'flexTPC':['Tmin', 'Tmax', 'rmax', 'alpha', 'beta', 'sigma2'],\n            'epc':['Tmin', 'Tmax', 'c', 'kI', 'kU', 'sigma2'],\n            'kum':['Tmin', 'Tmax', 'c', 'a', 'b', 'sigma2']}\n\nparam_df = {model:pd.DataFrame(param_matrix[model], columns=colnames[model]) for model in models}\nfor model in models:\n    param_df[model]['stage'] = stages\n    param_df[model] = param_df[model][['stage'] + colnames[model]]\n    param_df[model].to_csv(f'botrana_params_{model}.csv', index=False)\n\n\nnp.round(param_df['flexTPC'], 3)\n\n\n\n\n\n\n\n\nstage\nTmin\nTmax\nrmax\nalpha\nbeta\nsigma2\n\n\n\n\n0\neggs\n4.026\n34.000\n0.206\n0.735\n0.185\n0.001\n\n\n1\ni1\n0.000\n36.692\n0.294\n0.765\n0.181\n0.000\n\n\n2\ni2\n0.003\n35.100\n0.356\n0.809\n0.194\n0.000\n\n\n3\ni3\n4.870\n34.000\n0.345\n0.872\n0.235\n0.000\n\n\n4\ni4\n1.618\n34.000\n0.300\n0.842\n0.212\n0.000\n\n\n5\ni5\n0.000\n34.000\n0.177\n0.833\n0.205\n0.000\n\n\n6\npupae\n3.476\n34.000\n0.135\n0.867\n0.186\n0.000\n\n\n\n\n\n\n\n\nnp.round(param_df['epc'], 3)\n\n\n\n\n\n\n\n\nstage\nTmin\nTmax\nc\nkI\nkU\nsigma2\n\n\n\n\n0\neggs\n8.000\n34.000\n0.191\n0.093\n0.463\n0.002\n\n\n1\ni1\n8.000\n37.293\n0.356\n0.061\n0.301\n0.002\n\n\n2\ni2\n8.000\n34.878\n1.000\n0.021\n0.521\n0.001\n\n\n3\ni3\n8.000\n34.000\n0.987\n0.020\n0.776\n0.000\n\n\n4\ni4\n8.000\n34.375\n0.353\n0.066\n0.451\n0.002\n\n\n5\ni5\n8.000\n34.129\n0.337\n0.038\n0.433\n0.000\n\n\n6\npupae\n5.926\n34.055\n0.341\n0.018\n0.810\n0.000\n\n\n\n\n\n\n\n\nnp.round(param_matrix['flexTPC'], 3)\n\narray([[4.0260e+00, 3.4000e+01, 2.0600e-01, 7.3500e-01, 1.8500e-01,\n        1.0000e-03],\n       [0.0000e+00, 3.6692e+01, 2.9400e-01, 7.6500e-01, 1.8100e-01,\n        0.0000e+00],\n       [3.0000e-03, 3.5100e+01, 3.5600e-01, 8.0900e-01, 1.9400e-01,\n        0.0000e+00],\n       [4.8700e+00, 3.4000e+01, 3.4500e-01, 8.7200e-01, 2.3500e-01,\n        0.0000e+00],\n       [1.6180e+00, 3.4000e+01, 3.0000e-01, 8.4200e-01, 2.1200e-01,\n        0.0000e+00],\n       [0.0000e+00, 3.4000e+01, 1.7700e-01, 8.3300e-01, 2.0500e-01,\n        0.0000e+00],\n       [3.4760e+00, 3.4000e+01, 1.3500e-01, 8.6700e-01, 1.8600e-01,\n        0.0000e+00]])\n\n\n\nstage\n\n'pupae'\n\n\n\nnll = np.zeros((len(stages), len(models)))\nloo_nll = np.zeros((len(stages), len(models)))\naic = np.zeros((len(stages), len(models)))\nbic = np.zeros((len(stages), len(models)))\n\n\np = [4, 5, 6, 6, 6] # Number of parameters (including variance parameter).\nn = 14 # Number of datapoints.\n\nfor i, s in enumerate(stages):\n    for j, m in enumerate(models): \n        nll[i, j] = params[m][s][\"fun\"]\n        loo_nll[i, j] = np.mean(loocv_nll[m][s])\n        aic[i, j] = AIC(nll[i, j], p[j])\n        bic[i, j] = BIC(nll[i, j], n, p[j])\n\n\ndfAIC = pd.DataFrame(np.round(aic, 2), columns=models, index=stages)\ndfBIC = pd.DataFrame(np.round(bic, 2), columns=models, index=stages)\ndfLOOCV = pd.DataFrame(np.round(loo_nll, 2), columns=models, index=stages)\n\n\ndfAIC\n\n\n\n\n\n\n\n\nb1\nb2\nflexTPC\nepc\nkum\n\n\n\n\neggs\n-55.89\n-40.25\n-49.61\n-36.70\n-49.90\n\n\ni1\n-59.19\n-54.92\n-62.00\n-36.96\n-59.09\n\n\ni2\n-71.53\n-70.63\n-74.22\n-53.56\n-74.17\n\n\ni3\n-41.38\n-33.46\n-60.92\n-67.59\n-23.05\n\n\ni4\n-59.92\n-63.21\n-58.29\n-34.20\n-56.36\n\n\ni5\n-57.53\n-75.16\n-75.49\n-66.79\n-59.69\n\n\npupae\n-68.13\n-72.66\n-98.48\n-58.19\n-66.32\n\n\n\n\n\n\n\n\ndfBIC\n\n\n\n\n\n\n\n\nb1\nb2\nflexTPC\nepc\nkum\n\n\n\n\neggs\n-53.33\n-37.06\n-45.78\n-32.86\n-46.07\n\n\ni1\n-56.64\n-51.73\n-58.17\n-33.12\n-55.26\n\n\ni2\n-68.98\n-67.44\n-70.39\n-49.73\n-70.34\n\n\ni3\n-38.82\n-30.26\n-57.09\n-63.75\n-19.21\n\n\ni4\n-57.36\n-60.02\n-54.45\n-30.37\n-52.53\n\n\ni5\n-54.97\n-71.97\n-71.66\n-62.96\n-55.86\n\n\npupae\n-65.58\n-69.46\n-94.65\n-54.36\n-62.49\n\n\n\n\n\n\n\n\ndfLOOCV\n\n\n\n\n\n\n\n\nb1\nb2\nflexTPC\nepc\nkum\n\n\n\n\neggs\n1.997\n-1.795\n-2.200\n-1.739\n-2.211\n\n\ni1\n-1.464\n-2.319\n-2.643\n-1.748\n-2.539\n\n\ni2\n-2.238\n-2.880\n-3.079\n-2.342\n-3.078\n\n\ni3\n7.161\n-1.552\n-2.604\n-2.842\n-1.252\n\n\ni4\n2.135\n-2.615\n-2.510\n-1.650\n-2.442\n\n\ni5\n-0.475\n-3.041\n-3.125\n-2.814\n-2.561\n\n\npupae\n10.803\n-2.952\n-3.946\n-2.507\n-2.797\n\n\n\n\n\n\n\n\ndef get_delta(df):\n    '''\n    Turns results into difference relative to best model.\n    '''\n    best = np.min(df, axis=1) \n    return(df.subtract(np.min(df, axis=1), axis=0))\n\n\ndfAIC\n\n\n\n\n\n\n\n\nb1\nb2\nflexTPC\nepc\nkum\n\n\n\n\neggs\n-55.89\n-40.25\n-49.61\n-36.70\n-49.90\n\n\ni1\n-59.19\n-54.92\n-62.00\n-36.96\n-59.09\n\n\ni2\n-71.53\n-70.63\n-74.22\n-53.56\n-74.17\n\n\ni3\n-41.38\n-33.46\n-60.92\n-67.59\n-23.05\n\n\ni4\n-59.92\n-63.21\n-58.29\n-34.20\n-56.36\n\n\ni5\n-57.53\n-75.16\n-75.49\n-66.79\n-59.69\n\n\npupae\n-68.13\n-72.66\n-98.48\n-58.19\n-66.32\n\n\n\n\n\n\n\n\nget_delta(dfAIC)\n\n\n\n\n\n\n\n\nb1\nb2\nflexTPC\nepc\nkum\n\n\n\n\neggs\n0.00\n15.64\n6.28\n19.19\n5.99\n\n\ni1\n2.81\n7.08\n0.00\n25.04\n2.91\n\n\ni2\n2.69\n3.59\n0.00\n20.66\n0.05\n\n\ni3\n26.21\n34.13\n6.67\n0.00\n44.54\n\n\ni4\n3.29\n0.00\n4.92\n29.01\n6.85\n\n\ni5\n17.96\n0.33\n0.00\n8.70\n15.80\n\n\npupae\n30.35\n25.82\n0.00\n40.29\n32.16\n\n\n\n\n\n\n\n\nget_delta(dfBIC)\n\n\n\n\n\n\n\n\nb1\nb2\nflexTPC\nepc\nkum\n\n\n\n\neggs\n0.00\n16.27\n7.55\n20.47\n7.26\n\n\ni1\n1.53\n6.44\n0.00\n25.05\n2.91\n\n\ni2\n1.41\n2.95\n0.00\n20.66\n0.05\n\n\ni3\n24.93\n33.49\n6.66\n0.00\n44.54\n\n\ni4\n2.66\n0.00\n5.57\n29.65\n7.49\n\n\ni5\n17.00\n0.00\n0.31\n9.01\n16.11\n\n\npupae\n29.07\n25.19\n0.00\n40.29\n32.16\n\n\n\n\n\n\n\n\nget_delta(dfLOOCV)\n\n\n\n\n\n\n\n\nb1\nb2\nflexTPC\nepc\nkum\n\n\n\n\neggs\n4.21\n0.42\n0.01\n0.47\n0.00\n\n\ni1\n1.18\n0.32\n0.00\n0.89\n0.10\n\n\ni2\n0.84\n0.20\n0.00\n0.74\n0.00\n\n\ni3\n10.00\n1.29\n0.24\n0.00\n1.59\n\n\ni4\n4.75\n0.00\n0.10\n0.96\n0.17\n\n\ni5\n2.64\n0.08\n0.00\n0.31\n0.56\n\n\npupae\n14.75\n1.00\n0.00\n1.44\n1.15\n\n\n\n\n\n\n\n\nget_delta(dfLOOCV[['b1', 'b2', 'flexTPC']])\n\n\n\n\n\n\n\n\nb1\nb2\nflexTPC\n\n\n\n\neggs\n4.20\n0.41\n0.0\n\n\ni1\n1.18\n0.32\n0.0\n\n\ni2\n0.84\n0.20\n0.0\n\n\ni3\n9.76\n1.05\n0.0\n\n\ni4\n4.75\n0.00\n0.1\n\n\ni5\n2.64\n0.08\n0.0\n\n\npupae\n14.75\n1.00\n0.0\n\n\n\n\n\n\n\n\npd.DataFrame({'LOOCV':np.mean(get_delta(dfLOOCV), axis=0)})\n\n\n\n\n\n\n\n\nLOOCV\n\n\n\n\nb1\n5.481429\n\n\nb2\n0.472857\n\n\nflexTPC\n0.050000\n\n\nepc\n0.687143\n\n\nkum\n0.510000\n\n\n\n\n\n\n\n\nprint(dfLOOCV.to_latex())\n\n\\begin{tabular}{lrrrrr}\n\\toprule\n & b1 & b2 & flexTPC & epc & kum \\\\\n\\midrule\neggs & 27.960000 & -25.130000 & -30.810000 & -24.350000 & -30.950000 \\\\\ni1 & -20.500000 & -32.460000 & -37.000000 & -24.480000 & -35.540000 \\\\\ni2 & -31.330000 & -40.320000 & -43.110000 & -32.780000 & -43.090000 \\\\\ni3 & 100.250000 & -21.730000 & -36.460000 & -39.790000 & -17.520000 \\\\\ni4 & 29.900000 & -36.610000 & -35.140000 & -23.100000 & -34.180000 \\\\\ni5 & -6.650000 & -42.580000 & -43.750000 & -39.400000 & -35.850000 \\\\\npupae & 151.250000 & -41.330000 & -55.240000 & -35.100000 & -39.160000 \\\\\n\\bottomrule\n\\end{tabular}\n\n\n\n\nprint(dfAIC.to_latex())\n\n\\begin{tabular}{lrrrrr}\n\\toprule\n & b1 & b2 & flexTPC & epc & kum \\\\\n\\midrule\neggs & -55.890000 & -40.250000 & -49.610000 & -36.700000 & -49.900000 \\\\\ni1 & -59.190000 & -54.920000 & -62.000000 & -36.960000 & -59.090000 \\\\\ni2 & -71.530000 & -70.630000 & -74.220000 & -53.560000 & -74.170000 \\\\\ni3 & -41.380000 & -33.460000 & -60.920000 & -67.590000 & -23.050000 \\\\\ni4 & -59.920000 & -63.210000 & -58.290000 & -34.200000 & -56.360000 \\\\\ni5 & -57.530000 & -75.160000 & -75.490000 & -66.790000 & -59.690000 \\\\\npupae & -68.130000 & -72.660000 & -98.480000 & -58.190000 & -66.320000 \\\\\n\\bottomrule\n\\end{tabular}\n\n\n\n\nprint(dfBIC.to_latex())\n\n\\begin{tabular}{lrrrrr}\n\\toprule\n & b1 & b2 & flexTPC & epc & kum \\\\\n\\midrule\neggs & -53.330000 & -37.060000 & -45.780000 & -32.860000 & -46.070000 \\\\\ni1 & -56.640000 & -51.730000 & -58.170000 & -33.120000 & -55.260000 \\\\\ni2 & -68.980000 & -67.440000 & -70.390000 & -49.730000 & -70.340000 \\\\\ni3 & -38.820000 & -30.260000 & -57.090000 & -63.750000 & -19.210000 \\\\\ni4 & -57.360000 & -60.020000 & -54.450000 & -30.370000 & -52.530000 \\\\\ni5 & -54.970000 & -71.970000 & -71.660000 & -62.960000 & -55.860000 \\\\\npupae & -65.580000 & -69.460000 & -94.650000 & -54.360000 & -62.490000 \\\\\n\\bottomrule\n\\end{tabular}"
  },
  {
    "objectID": "fit_examples.html",
    "href": "fit_examples.html",
    "title": "Tutorial",
    "section": "",
    "text": "In this tutorial, we provide example code to show how to fit flexTPC with various methodologies (least squares estimation, maximum likelihood estimation and Bayesian approaches)."
  },
  {
    "objectID": "fit_examples.html#least-squares-estimation",
    "href": "fit_examples.html#least-squares-estimation",
    "title": "FlexTPC tutorial",
    "section": "Least squares estimation",
    "text": "Least squares estimation\nA simple approach to estimate TPCs is to perform nonlinear least squares estimation. This finds the curve that minimizes the squared difference between the TPC and the data points."
  },
  {
    "objectID": "fit_examples.html#maximum-likelihood-estimation",
    "href": "fit_examples.html#maximum-likelihood-estimation",
    "title": "Tutorial",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nWhen fitting a thermal performance curve model to data it is helpful to think about the model being comprised of two parts:\n\nA functional form \\(r(T)\\) for the thermal performance curve (e.g. flexTPC, the Briere model, etc.) that describes the temperature dependence of a biological trait.\nA model of the errors, which describes a plausible mechanism of generating the observed datapoints from the model around the mean given by \\(r(T)\\).\n\nLeast squares estimation is equivalent to assuming the model residuals are normally distributed with a constant variance."
  },
  {
    "objectID": "model_comparison/Gounot.html",
    "href": "model_comparison/Gounot.html",
    "title": "Gounot dataset of bacteria from glaciers",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport random\n\nrandom.seed(42)\n\nfrom matplotlib import pyplot as plt\n\nfrom scipy.optimize import minimize\nfrom sklearn.model_selection import LeaveOneOut\ndef briere1(T, Tmin=10.0, Tmax=50.0, c=1.0):\n    result = np.zeros(len(T))\n    cond = (Tmin &lt; T) & (T &lt; Tmax)\n    result[cond] = c * T[cond] * np.exp(np.log(T[cond] - Tmin) + 0.5 * np.log(Tmax - T[cond]))\n    return result\n\ndef briere2(T, Tmin=10.0, Tmax=50.0, c=1.0, b=2.0):\n    result = np.zeros(len(T))\n    cond = (Tmin &lt; T) & (T &lt; Tmax)\n    result[cond] = c * T[cond] * np.exp(np.log(T[cond] - Tmin) +  np.log(Tmax - T[cond]) / b)\n    return result\n\n# Fully biologically interpretable parametrization of flexTPC model\ndef flexTPC(T, Tmin=10.0, Tmax=50.0, rmax=1.0, α=0.8, β=0.2):\n    result = np.zeros(len(T))\n    cond = (Tmin &lt; T) & (T &lt; Tmax)\n    s = α * (1.0 - α) / β**2\n    result[cond] = rmax * np.exp(s * (α * (np.log(T[cond] - Tmin) - np.log(α)) +\n                                      (1 - α) * (np.log(Tmax - T[cond]) - np.log(1 - α) )\n                                       - np.log(Tmax - Tmin)))\n    return result\n\n# Exponential product curve.\ndef expprodcurve(T, Tmin=10.0, Tmax=50.0, c=1.0, kI=0.1, kU=0.5):\n    result = np.zeros(len(T))\n    cond = (Tmin &lt; T) & (T &lt; Tmax)\n    result[cond] = c * (1.0 - np.exp(-kI*(T[cond] - Tmin))) * (1.0 - np.exp(-kU*(Tmax - T[cond])))\n    return result\n\n# Kumaraswarmy distribution TPC.\ndef kumaraswarmy(T, Tmin=10.0, Tmax=50.0, c=1.0, a=5.0, b=5.0):\n    result = np.zeros(len(T))\n    cond = (Tmin &lt; T) & (T &lt; Tmax)\n    τ = (T[cond] - Tmin) / (Tmax - Tmin)\n    result[cond] = c * τ**(a - 1) * (1.0 - τ**a)**(b - 1)\n    return result\nglacierbac = pd.read_csv(\"glacierbac.csv\")\nglacierbac\n\n\n\n\n\n\n\n\nUnnamed: 0\nspecies\nstrain\nX\nY\npsychro\nT\nr\n\n\n\n\n0\n0\nArthrobacter glacialis\n137\n3.457467\n3.761369\nTrue\n16.0\n0.043007\n\n\n1\n1\nArthrobacter glacialis\n137\n3.482460\n3.985740\nTrue\n14.0\n0.053825\n\n\n2\n2\nArthrobacter glacialis\n137\n3.501018\n3.973215\nTrue\n12.0\n0.053155\n\n\n3\n3\nArthrobacter glacialis\n137\n3.527881\n3.858413\nTrue\n10.0\n0.047390\n\n\n4\n4\nArthrobacter glacialis\n137\n3.565603\n3.596571\nTrue\n7.0\n0.036473\n\n\n5\n5\nArthrobacter glacialis\n137\n3.603963\n3.328340\nTrue\n4.0\n0.027892\n\n\n6\n6\nArthrobacter glacialis\n137\n3.655096\n2.874697\nTrue\n0.0\n0.017720\n\n\n7\n7\nArthrobacter glacialis\n158\n3.408169\n3.639039\nTrue\n20.0\n0.038055\n\n\n8\n8\nArthrobacter glacialis\n158\n3.431886\n3.888990\nTrue\n18.0\n0.048862\n\n\n9\n9\nArthrobacter glacialis\n158\n3.457496\n3.959770\nTrue\n16.0\n0.052445\n\n\n10\n10\nArthrobacter glacialis\n158\n3.470943\n4.004769\nTrue\n15.0\n0.054859\n\n\n11\n11\nArthrobacter glacialis\n158\n3.492705\n4.017891\nTrue\n13.0\n0.055584\n\n\n12\n12\nArthrobacter glacialis\n158\n3.528532\n3.928822\nTrue\n10.0\n0.050847\n\n\n13\n13\nArthrobacter glacialis\n158\n3.567542\n3.724600\nTrue\n7.0\n0.041455\n\n\n14\n14\nArthrobacter glacialis\n158\n3.607823\n3.462797\nTrue\n4.0\n0.031906\n\n\n15\n15\nArthrobacter glacialis\n158\n3.637232\n3.252033\nTrue\n2.0\n0.025843\n\n\n16\n16\nArthrobacter sp\n55\n3.254137\n5.044757\nFalse\n34.0\n0.155207\n\n\n17\n17\nArthrobacter sp\n55\n3.276011\n5.813081\nFalse\n32.0\n0.334649\n\n\n18\n18\nArthrobacter sp\n55\n3.301625\n5.909461\nFalse\n30.0\n0.368507\n\n\n19\n19\nArthrobacter sp\n55\n3.322757\n5.986574\nFalse\n28.0\n0.398048\n\n\n20\n20\nArthrobacter sp\n55\n3.363704\n5.897580\nFalse\n24.0\n0.364155\n\n\n21\n21\nArthrobacter sp\n55\n3.411676\n5.712691\nFalse\n20.0\n0.302684\n\n\n22\n22\nArthrobacter sp\n55\n3.460915\n5.431820\nFalse\n16.0\n0.228565\n\n\n23\n23\nArthrobacter sp\n55\n3.500546\n5.106008\nFalse\n13.0\n0.165010\n\n\n24\n24\nArthrobacter sp\n55\n3.549740\n4.524336\nFalse\n9.0\n0.092235\n\n\n25\n25\nArthrobacter sp\n55\n3.605948\n3.769969\nFalse\n4.0\n0.043379\n\n\n26\n26\nArthrobacter sp\n55\n3.657034\n2.996326\nFalse\n0.0\n0.020012\n\n\n27\n27\nArthrobacter sp\n60\n3.258599\n4.923223\nFalse\n34.0\n0.137445\n\n\n28\n28\nArthrobacter sp\n60\n3.277923\n5.761909\nFalse\n32.0\n0.317955\n\n\n29\n29\nArthrobacter sp\n60\n3.322746\n5.916174\nFalse\n28.0\n0.370989\n\n\n30\n30\nArthrobacter sp\n60\n3.363047\n5.788771\nFalse\n24.0\n0.326611\n\n\n31\n31\nArthrobacter sp\n60\n3.410359\n5.463071\nFalse\n20.0\n0.235821\n\n\n32\n32\nArthrobacter sp\n60\n3.460883\n5.220620\nFalse\n16.0\n0.185049\n\n\n33\n33\nArthrobacter sp\n60\n3.499878\n4.913998\nFalse\n13.0\n0.136183\n\n\n34\n34\nArthrobacter sp\n60\n3.549093\n4.479527\nFalse\n9.0\n0.088193\n\n\n35\n35\nArthrobacter sp\n60\n3.606580\n3.712378\nFalse\n4.0\n0.040951\n\n\n36\n36\nPseudomonas\n76\n3.252846\n4.967938\nFalse\n34.0\n0.143730\n\n\n37\n37\nPseudomonas\n76\n3.276656\n5.845091\nFalse\n32.0\n0.345534\n\n\n38\n38\nPseudomonas\n76\n3.302292\n6.088670\nFalse\n30.0\n0.440835\n\n\n39\n39\nPseudomonas\n76\n3.321507\n6.191355\nFalse\n28.0\n0.488508\n\n\n40\n40\nPseudomonas\n76\n3.351601\n6.287801\nFalse\n25.0\n0.537969\n\n\n41\n41\nPseudomonas\n76\n3.411068\n5.930282\nFalse\n20.0\n0.376261\n\n\n42\n42\nPseudomonas\n76\n3.531257\n5.042463\nFalse\n10.0\n0.154851\n\n\n43\n43\nPseudomonas\n76\n3.590712\n4.601743\nFalse\n5.0\n0.099658\n\n\n44\n44\nPseudomonas\n76\n3.627744\n4.013492\nFalse\n3.0\n0.055340"
  },
  {
    "objectID": "model_comparison/Gounot.html#fitting-temperature-response-curves-to-model",
    "href": "model_comparison/Gounot.html#fitting-temperature-response-curves-to-model",
    "title": "Gounot dataset of bacteria from glaciers",
    "section": "Fitting temperature response curves to model",
    "text": "Fitting temperature response curves to model\n\ndef nloglik(θ, Tdata, rdata, model=flexTPC):\n    params, σ2 = θ[:-1], θ[-1]\n    Tmin, Tmax = θ[:2]\n    outside = (Tdata &lt; Tmin) | (Tdata &gt; Tmax)\n    inside = ~outside\n    n = len(rdata[inside])\n    if np.any(rdata[outside] &gt; 0.0):\n        return np.inf\n    else:\n        return 0.5 * (n * np.log(2*np.pi*σ2) + np.sum((rdata[inside] - model(Tdata[inside], *params))**2 / σ2))\n    \ndef AIC(nll, p):\n    return 2 * nll + 2 * p \n\ndef BIC(nll, n, p):\n    return 2 * nll + p * np.log(n)\n\n\nstrains = [55, 60, 76, 137, 158]\n\ninitial_b1 = {55:[-5.0, 35.0, 0.0002, 0.01],\n              60:[-5.0, 35.0, 0.0002, 0.01],\n              76:[-5.0, 35.0, 0.0003, 0.01],\n             137:[-5.0, 22.0, 0.00008, 0.01],\n             158:[-5.0, 22.0, 0.00008, 0.01]}\n\ninitial_b2 = {55:[-5.0, 35.0, 0.0002, 2.0, 0.01],\n              60:[-5.0, 35.0, 0.0002, 2.0, 0.01],\n              76:[-5.0, 35.0, 0.0003, 2.0, 0.01],\n             137:[-5.0, 22.0, 0.00008, 2.0, 0.01],\n             158:[-5.0, 22.0, 0.00008, 2.0, 0.01]}\n\n\ninitial_flex = {55:[-5.0, 35.0, 0.35, 0.8, 0.2, 0.01],\n              60:[-5.0, 35.0, 0.35, 0.8, 0.2, 0.01],\n              76:[-5.0, 35.0, 0.35, 0.8, 0.2, 0.01],\n             137:[-5.0, 22.0, 0.03, 0.8, 0.2, 0.01],\n             158:[-5.0, 22.0, 0.05, 0.8, 0.2, 0.01]}\n\ninitial_epc = {55:[-5.0, 35.0, 0.4, 0.05, 0.4, 0.01],\n              60:[-5.0, 35.0, 0.4, 0.05, 0.4, 0.01],\n              76:[-5.0, 35.0, 0.6, 0.05, 0.4, 0.01],\n             137:[-5.0, 22.0, 0.05, 0.05, 0.4, 0.01],\n             158:[-5.0, 22.0, 0.05, 0.05, 0.4, 0.01]}\n\ninitial_kum = {55:[-5.0, 35.0, 1.0, 3, 2, 0.01],\n              60:[-5.0, 35.0, 1.0, 3, 2, 0.01],\n              76:[-5.0, 35.0, 1.0, 3, 2, 0.01],\n             137:[-5.0, 22.0, 0.05, 3, 2, 0.01],\n             158:[-5.0, 22.0, 0.05, 3, 2, 0.01]}\n\n\n# Plot initial value curves.\nT = np.arange(-10, 40, 0.001)\n\nplt.figure(figsize=(4.33, 3), dpi=300)\n\nfor i, s in enumerate(strains):\n    subset = glacierbac[glacierbac[\"strain\"] == s]\n    species = list(subset[\"species\"])[0]\n    \n    plt.subplot(2, 3, i+1)\n    # Plot datapoints.\n    Tdata = subset[\"T\"]\n    rdata = subset[\"r\"]\n\n    plt.plot(Tdata, rdata, '^', markersize=5)\n    plt.xticks(fontsize=6)\n    plt.yticks(fontsize=6)\n    plt.xlim(-10, 37)\n    plt.ylim(0, max(rdata)*1.1)\n    \n    # Plot fitted curves by Briere.\n    #params = params_b1[s][\"x\"]\n    #plt.plot(T, briere1(T, *params[:-1]), color=\"firebrick\")\n    plt.plot(T, briere1(T, *initial_b1[s][:-1]), color=\"firebrick\")\n             \n    #params = params_eb[s][\"x\"]\n    #plt.plot(T, extbriere(T, *params[:-1]), color=\"steelblue\")\n    plt.plot(T, flexTPC(T, *initial_flex[s][:-1]), color=\"darkgreen\")\n\n    plt.ylim(-0.025, 0.525)\n    plt.plot(T, expprodcurve(T, *initial_epc[s][:-1]), color=\"purple\")\n\n    \n    plt.plot(T, kumaraswarmy(T, *initial_kum[s][:-1]), color=\"blue\")\n\n    \n    plt.xlabel(\"Temperature [°C]\", fontsize=6)\n    plt.ylabel(\"Growth rate\", fontsize=6)\n    \n\n    \n    plt.title(species + \" \" + str(s), fontsize=6)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n# LOOCV.\nloo = LeaveOneOut()\n\nparams = {\"flexTPC\":{}, \"b1\":{}, \"b2\":{}, \"epc\":{}, \"kum\":{}}\nmodels = [\"b1\", \"b2\", \"flexTPC\", \"epc\", \"kum\"]\nmethod = \"Nelder-Mead\"\nloocv_nll = {model:{strain:[] for strain in strains } for model in models}\n\nfor i, s in enumerate(strains):\n    subset = glacierbac[glacierbac[\"strain\"] == s]\n    Tdata = np.array(subset[\"T\"])\n    rdata = np.array(subset[\"r\"])\n    loo.get_n_splits(glacierbac[\"T\"][glacierbac[\"strain\"] == s])\n        \n    for train, test in loo.split(Tdata):\n        T_train, r_train = Tdata[train], rdata[train]\n        T_test, r_test = Tdata[test], rdata[test]\n        \n        params[\"b1\"][s] = minimize(nloglik, initial_b1[s],\n                                   bounds=[(-20.0, 20.0), \n                                           (10.0, 50.0), \n                                           (0.0, 1.0),  \n                                           (0, 1.0)],\n                          args=(T_train, r_train, briere1), options={\"maxiter\":10000}, method=method)\n        θ = params[\"b1\"][s][\"x\"]\n        loocv_nll[\"b1\"][s].append(nloglik(θ, T_test, r_test, model=briere1))\n        \n        params[\"b2\"][s] = minimize(nloglik, initial_b2[s],\n                                   bounds=[(-20.0, 20.0), \n                                           (10.0, 50.0), \n                                           (0.0, 1.0),\n                                           (0.0, 20.0),\n                                           (0, 1.0)],\n                          args=(Tdata, rdata, briere2), options={\"maxiter\":10000}, method=method)\n        θ = params[\"b2\"][s][\"x\"]\n        loocv_nll[\"b2\"][s].append(nloglik(θ, T_test, r_test, model=briere2))     \n        \n        params[\"flexTPC\"][s] = minimize(nloglik, initial_flex[s],\n                                        bounds=[(-20.0, 20.0), \n                                               (10.0, 50.0), \n                                               (0.0, 1.0),\n                                               (0.0, 1.0),\n                                               (0.0, np.inf),\n                                               (0, 1.0)],\n                          args=(Tdata, rdata, flexTPC), options={\"maxiter\":10000}, method=method)\n        θ = params[\"flexTPC\"][s][\"x\"]\n        loocv_nll[\"flexTPC\"][s].append(nloglik(θ, T_test, r_test, model=flexTPC))\n\n        params[\"epc\"][s] = minimize(nloglik, initial_epc[s],\n                                bounds=[(-20.0, 20.0), \n                                        (10.0, 50.0), \n                                        (0.0, 1.0), \n                                        (0.0, np.inf), \n                                        (0.0, np.inf), \n                                        (0, 1.0)],\n                          args=(Tdata, rdata, expprodcurve), options={\"maxiter\":100000}, method=method)\n        θ = params[\"epc\"][s][\"x\"]\n        loocv_nll[\"epc\"][s].append(nloglik(θ, T_test, r_test, model=expprodcurve))\n\n        params[\"kum\"][s] = minimize(nloglik, initial_kum[s],\n                                bounds=[(-20.0, 20.0), \n                                        (10.0, 50.0), \n                                        (0.0, np.inf), \n                                        (1.0, np.inf), \n                                        (1.0, np.inf), \n                                        (0, 1.0)],\n                          args=(Tdata, rdata, kumaraswarmy), options={\"maxiter\":100000}, method=method)\n        θ = params[\"kum\"][s][\"x\"]\n        loocv_nll[\"kum\"][s].append(nloglik(θ, T_test, r_test, model=kumaraswarmy))\n\n/var/folders/g8/wjqhnz1n01d5l4pdv8cnykw40000gp/T/ipykernel_90177/2868019315.py:10: RuntimeWarning: divide by zero encountered in log\n  return 0.5 * (n * np.log(2*np.pi*σ2) + np.sum((rdata[inside] - model(Tdata[inside], *params))**2 / σ2))\n/var/folders/g8/wjqhnz1n01d5l4pdv8cnykw40000gp/T/ipykernel_90177/2868019315.py:10: RuntimeWarning: divide by zero encountered in divide\n  return 0.5 * (n * np.log(2*np.pi*σ2) + np.sum((rdata[inside] - model(Tdata[inside], *params))**2 / σ2))\n/var/folders/g8/wjqhnz1n01d5l4pdv8cnykw40000gp/T/ipykernel_90177/2868019315.py:10: RuntimeWarning: invalid value encountered in scalar add\n  return 0.5 * (n * np.log(2*np.pi*σ2) + np.sum((rdata[inside] - model(Tdata[inside], *params))**2 / σ2))\n\n\n\n# Fit to whole datasets.\n\nparams = {\"flexTPC\":{}, \"b1\":{}, \"b2\":{}, \"epc\":{}, \"kum\":{}}\nmodels = [\"b1\", \"b2\", \"flexTPC\", \"epc\", \"kum\"]\nmethod = \"Nelder-Mead\"\nfor i, s in enumerate(strains):\n    subset = glacierbac[glacierbac[\"strain\"] == s]\n    Tdata = np.array(subset[\"T\"])\n    rdata = np.array(subset[\"r\"])    \n    \n\n    params[\"b1\"][s] = minimize(nloglik, initial_b1[s],\n                               bounds=[(-20.0, 20.0), \n                                           (10.0, 50.0), \n                                           (0.0, 1.0),  \n                                           (0, 1.0)],\n                      args=(Tdata, rdata, briere1), options={\"maxiter\":100000}, method=method)\n    params[\"b2\"][s] = minimize(nloglik, initial_b2[s],\n                               bounds=[(-20.0, 20.0), \n                                           (10.0, 50.0), \n                                           (0.0, 1.0),\n                                           (0.0, 20.0),\n                                           (0, 1.0)],\n                      args=(Tdata, rdata, briere2), options={\"maxiter\":100000}, method=method)\n    params[\"flexTPC\"][s] = minimize(nloglik, initial_flex[s],\n                                    bounds=[(-20.0, 20.0), \n                                               (10.0, 50.0), \n                                               (0.0, 1.0),\n                                               (0.0, 1.0),\n                                               (0.0, np.inf),\n                                               (0, 1.0)],\n                      args=(Tdata, rdata), options={\"maxiter\":100000}, method=method)\n    params[\"epc\"][s] = minimize(nloglik, initial_epc[s],\n                                bounds=[(-20.0, 20.0), \n                                        (10.0, 50.0), \n                                        (0.0, 1.0), \n                                        (0.0, np.inf), \n                                        (0.0, np.inf), \n                                        (0, 1.0)],\n                          args=(Tdata, rdata, expprodcurve), options={\"maxiter\":100000}, method=method)\n    params[\"kum\"][s] = minimize(nloglik, initial_kum[s],\n                                bounds=[(-20.0, 20.0), \n                                        (10.0, 50.0), \n                                        (0.0, np.inf), \n                                        (1.0, np.inf), \n                                        (1.0, np.inf), \n                                        (0, 1.0)],\n                          args=(Tdata, rdata, kumaraswarmy), options={\"maxiter\":100000}, method=method)\n\n/var/folders/g8/wjqhnz1n01d5l4pdv8cnykw40000gp/T/ipykernel_90177/2868019315.py:10: RuntimeWarning: divide by zero encountered in log\n  return 0.5 * (n * np.log(2*np.pi*σ2) + np.sum((rdata[inside] - model(Tdata[inside], *params))**2 / σ2))\n/var/folders/g8/wjqhnz1n01d5l4pdv8cnykw40000gp/T/ipykernel_90177/2868019315.py:10: RuntimeWarning: divide by zero encountered in divide\n  return 0.5 * (n * np.log(2*np.pi*σ2) + np.sum((rdata[inside] - model(Tdata[inside], *params))**2 / σ2))\n/var/folders/g8/wjqhnz1n01d5l4pdv8cnykw40000gp/T/ipykernel_90177/2868019315.py:10: RuntimeWarning: invalid value encountered in scalar add\n  return 0.5 * (n * np.log(2*np.pi*σ2) + np.sum((rdata[inside] - model(Tdata[inside], *params))**2 / σ2))\n\n\n\nparams[\"flexTPC\"]\n\n{55:        message: Optimization terminated successfully.\n        success: True\n         status: 0\n            fun: -40.3163673202209\n              x: [-6.241e+00  3.437e+01  3.947e-01  8.408e-01  2.134e-01\n                   3.838e-05]\n            nit: 284\n           nfev: 446\n  final_simplex: (array([[-6.241e+00,  3.437e+01, ...,  2.134e-01,\n                          3.838e-05],\n                        [-6.241e+00,  3.437e+01, ...,  2.134e-01,\n                          3.837e-05],\n                        ...,\n                        [-6.241e+00,  3.437e+01, ...,  2.134e-01,\n                          3.837e-05],\n                        [-6.241e+00,  3.437e+01, ...,  2.134e-01,\n                          3.837e-05]]), array([-4.032e+01, -4.032e+01, -4.032e+01, -4.032e+01,\n                        -4.032e+01, -4.032e+01, -4.032e+01])),\n 60:        message: Optimization terminated successfully.\n        success: True\n         status: 0\n            fun: -33.56423361085082\n              x: [-2.000e+01  3.426e+01  3.698e-01  9.001e-01  1.386e-01\n                   3.369e-05]\n            nit: 743\n           nfev: 1123\n  final_simplex: (array([[-2.000e+01,  3.426e+01, ...,  1.386e-01,\n                          3.369e-05],\n                        [-2.000e+01,  3.426e+01, ...,  1.386e-01,\n                          3.374e-05],\n                        ...,\n                        [-2.000e+01,  3.426e+01, ...,  1.386e-01,\n                          3.369e-05],\n                        [-2.000e+01,  3.426e+01, ...,  1.386e-01,\n                          3.369e-05]]), array([-3.356e+01, -3.356e+01, -3.356e+01, -3.356e+01,\n                        -3.356e+01, -3.356e+01, -3.356e+01])),\n 76:        message: Optimization terminated successfully.\n        success: True\n         status: 0\n            fun: -23.344115913719172\n              x: [-1.999e+01  3.482e+01  5.079e-01  8.481e-01  1.510e-01\n                   3.262e-04]\n            nit: 620\n           nfev: 952\n  final_simplex: (array([[-1.999e+01,  3.482e+01, ...,  1.510e-01,\n                          3.262e-04],\n                        [-1.999e+01,  3.482e+01, ...,  1.510e-01,\n                          3.262e-04],\n                        ...,\n                        [-1.999e+01,  3.482e+01, ...,  1.510e-01,\n                          3.261e-04],\n                        [-1.999e+01,  3.482e+01, ...,  1.510e-01,\n                          3.262e-04]]), array([-2.334e+01, -2.334e+01, -2.334e+01, -2.334e+01,\n                        -2.334e+01, -2.334e+01, -2.334e+01])),\n 137:        message: Optimization terminated successfully.\n        success: True\n         status: 0\n            fun: -42.14038458971266\n              x: [-2.000e+01  1.650e+01  5.441e-02  9.238e-01  1.453e-01\n                   3.458e-07]\n            nit: 1677\n           nfev: 2565\n  final_simplex: (array([[-2.000e+01,  1.650e+01, ...,  1.453e-01,\n                          3.458e-07],\n                        [-2.000e+01,  1.650e+01, ...,  1.453e-01,\n                          3.458e-07],\n                        ...,\n                        [-2.000e+01,  1.650e+01, ...,  1.453e-01,\n                          3.457e-07],\n                        [-2.000e+01,  1.650e+01, ...,  1.453e-01,\n                          3.459e-07]]), array([-4.214e+01, -4.214e+01, -4.214e+01, -4.214e+01,\n                        -4.214e+01, -4.214e+01, -4.214e+01])),\n 158:        message: Optimization terminated successfully.\n        success: True\n         status: 0\n            fun: -51.12053843952405\n              x: [-5.551e+00  2.333e+01  5.495e-02  6.703e-01  3.158e-01\n                   6.822e-07]\n            nit: 263\n           nfev: 439\n  final_simplex: (array([[-5.551e+00,  2.333e+01, ...,  3.158e-01,\n                          6.822e-07],\n                        [-5.551e+00,  2.333e+01, ...,  3.158e-01,\n                          6.819e-07],\n                        ...,\n                        [-5.551e+00,  2.333e+01, ...,  3.158e-01,\n                          6.824e-07],\n                        [-5.551e+00,  2.333e+01, ...,  3.158e-01,\n                          6.819e-07]]), array([-5.112e+01, -5.112e+01, -5.112e+01, -5.112e+01,\n                        -5.112e+01, -5.112e+01, -5.112e+01]))}\n\n\n\nT = np.arange(-10, 40, 0.001)\n\nfig, axarr = plt.subplots(2, 3, figsize=(6.5, 3), dpi=300, sharex=True, sharey='row')\n\nfor i, s in enumerate(strains):\n    ax = axarr[i // 3, i % 3]\n\n    subset = glacierbac[glacierbac[\"strain\"] == s]\n    species = list(subset[\"species\"])[0]\n    \n    # Plot datapoints.\n    Tdata = subset[\"T\"]\n    rdata = subset[\"r\"]\n\n    ax.plot(Tdata, rdata, '^', markersize=5)\n    ax.set_xticks([-10, 0, 10, 20, 30])\n    \n    ax.set_xlim(-10, 37)\n    \n    if s in (55, 60, 76):\n        #ax.set_ylim(0, round(max(rdata)*1.2, 1) )\n        #ax.set_yticks(fontsize=5)\n        ax.set_ylim(0, 0.6)\n    else:\n        ax.set_ylim(0, 0.06)\n        #ax.set_ylim(0, round(max(rdata)*1.1, 2) )\n    \n    # Plot fitted curves by Briere.\n    par = params[\"b1\"][s][\"x\"]\n    ax.plot(T, briere1(T, *par[:-1]), color=\"firebrick\", alpha=0.8)\n    \n    par = params[\"b2\"][s][\"x\"]\n    ax.plot(T, briere2(T, *par[:-1]), color=\"orange\", alpha=0.8)\n             \n    par = params[\"flexTPC\"][s][\"x\"]\n    ax.plot(T, flexTPC(T, *par[:-1]), color=\"darkgreen\", alpha=0.8)\n    \n    #par = params[\"epc\"][s][\"x\"]\n    #ax.plot(T, expprodcurve(T, *par[:-1]), color=\"purple\", alpha=0.8)\n\n    #par = params[\"kum\"][s][\"x\"]\n    #ax.plot(T, kumaraswarmy(T, *par[:-1]), color=\"blue\", alpha=0.8)\n    \n    ax.set_title(species + \" (\" + str(s) + \")\", fontsize=9)\n\n    if s in (137, 158):\n        ax.set_xlabel(\"Temperature [°C]\", fontsize=8)\n    if s in (55, 137):\n        ax.set_ylabel(r\"Growth rate [1 / h]\", fontsize=8)\n\naxarr[-1, -1].axis(\"off\")\n\nplt.tight_layout()\nfig.align_labels()\nplt.savefig(\"glacierbac.pdf\")\nplt.savefig(\"glacierbac.png\")\nplt.savefig(\"glacierbac.svg\")\n\n\n\n\n\n\n\n\n\nT = np.arange(-10, 40, 0.001)\n\nfig, axarr = plt.subplots(2, 3, figsize=(6.5, 3), dpi=300, sharex=True, sharey='row')\n\nfor i, s in enumerate(strains):\n    ax = axarr[i // 3, i % 3]\n\n    subset = glacierbac[glacierbac[\"strain\"] == s]\n    species = list(subset[\"species\"])[0]\n    \n    # Plot datapoints.\n    Tdata = subset[\"T\"]\n    rdata = subset[\"r\"]\n\n    ax.plot(Tdata, rdata, '^', markersize=5)\n    ax.set_xticks([-10, 0, 10, 20, 30])\n    \n    ax.set_xlim(-10, 37)\n    \n    if s in (55, 60, 76):\n        #ax.set_ylim(0, round(max(rdata)*1.2, 1) )\n        #ax.set_yticks(fontsize=5)\n        ax.set_ylim(0, 0.6)\n    else:\n        ax.set_ylim(0, 0.06)\n        #ax.set_ylim(0, round(max(rdata)*1.1, 2) )\n    \n    # Plot fitted curves by Briere.\n    #par = params[\"b1\"][s][\"x\"]\n    #ax.plot(T, briere1(T, *par[:-1]), color=\"firebrick\", alpha=0.8)\n    \n    #par = params[\"b2\"][s][\"x\"]\n    #ax.plot(T, briere2(T, *par[:-1]), color=\"orange\", alpha=0.8)\n    \n    #par = params[\"rmb3\"][s][\"x\"]\n    #plt.plot(T, rmodbriere_3(T, *par[:-1]), color=\"blue\", alpha=0.7)\n    \n    #par = params[\"rmb4\"][s][\"x\"]\n    #plt.plot(T, rmodbriere_4(T, *par[:-1]), color=\"gray\", alpha=0.7)\n\n        \n    par = params[\"epc\"][s][\"x\"]\n    ax.plot(T, expprodcurve(T, *par[:-1]), color=\"purple\", alpha=0.8)\n\n    par = params[\"kum\"][s][\"x\"]\n    ax.plot(T, kumaraswarmy(T, *par[:-1]), color=\"blue\", alpha=0.8)\n             \n    par = params[\"flexTPC\"][s][\"x\"]\n    ax.plot(T, flexTPC(T, *par[:-1]), color=\"darkgreen\", alpha=0.8)\n\n    \n    ax.set_title(species + \" (\" + str(s) + \")\", fontsize=9)\n\n    if s in (137, 158):\n        ax.set_xlabel(\"Temperature [°C]\", fontsize=8)\n    if s in (55, 137):\n        ax.set_ylabel(r\"Growth rate [1 / h]\", fontsize=8)\n\naxarr[-1, -1].axis(\"off\")\n\nplt.tight_layout()\nfig.align_labels()\nplt.savefig(\"glacierbac_pc_kum.pdf\")\nplt.savefig(\"glacierbac_pc_kum.png\")\nplt.savefig(\"glacierbac_pc_kum.svg\")\n\n\n\n\n\n\n\n\n\nspecies_list = ['Arthrobacter sp', 'Arthrobacter sp', 'Pseudomonas', 'Arthrobacter glacialis', 'Arthrobacter glacialis']\nn_params = {'b1':4, 'b2':5, 'flexTPC':6, 'epc':6, 'kum':6} # Includes standard deviation.\nparam_matrix = {model:np.zeros((5, n_params[model])) for model in models}\n\nfor model in models:\n    for i, strain in enumerate(strains):\n        param_matrix[model][i, ] = params[model][strain][\"x\"]\n\ncolnames = {'b1':['Tmin', 'Tmax', 'c', 'sigma2'],\n            'b2':['Tmin', 'Tmax', 'c', 'm', 'sigma2'],\n            'flexTPC':['Tmin', 'Tmax', 'rmax', 'alpha', 'beta', 'sigma2'],\n            'epc':['Tmin', 'Tmax', 'c', 'kI', 'kU', 'sigma2'],\n           'kum':['Tmin', 'Tmax', 'c', 'a', 'b', 'sigma2']}\n\nparam_df = {model:pd.DataFrame(param_matrix[model], columns=colnames[model]) for model in models}\n\nfor model in models:\n    param_df[model]['strain'] = strains\n    param_df[model]['species'] = species_list\n    param_df[model] = param_df[model][['species', 'strain'] + colnames[model]]\n    param_df[model].to_csv(f'glacierbac_params_{model}.csv', index=False)\n\n\nparam_df['flexTPC']\n\n\n\n\n\n\n\n\nspecies\nstrain\nTmin\nTmax\nrmax\nalpha\nbeta\nsigma2\n\n\n\n\n0\nArthrobacter sp\n55\n-6.240615\n34.372555\n0.394717\n0.840840\n0.213438\n3.837754e-05\n\n\n1\nArthrobacter sp\n60\n-20.000000\n34.262541\n0.369808\n0.900137\n0.138620\n3.369385e-05\n\n\n2\nPseudomonas\n76\n-19.990777\n34.821877\n0.507879\n0.848150\n0.151045\n3.261672e-04\n\n\n3\nArthrobacter glacialis\n137\n-19.999992\n16.496880\n0.054407\n0.923773\n0.145301\n3.457756e-07\n\n\n4\nArthrobacter glacialis\n158\n-5.551258\n23.326786\n0.054946\n0.670261\n0.315771\n6.822184e-07\n\n\n\n\n\n\n\n\nspecies_list = list(set(glacierbac[\"species\"]))\n\n\nnll = np.zeros((len(strains), len(models)))\nloo_nll = np.zeros((len(strains), len(models)))\naic = np.zeros((len(strains), len(models)))\nbic = np.zeros((len(strains), len(models)))\np = [4, 5, 6, 6, 6] # Number of parameters\n\nfor i, s in enumerate(strains):\n    n = len(glacierbac[glacierbac[\"strain\"] == s]) # Number of datapoints.\n\n    for j, m in enumerate(models): \n        nll[i, j] = params[m][s][\"fun\"]\n        loo_nll[i, j] = np.mean(loocv_nll[m][s])\n        aic[i, j] = AIC(nll[i, j], p[j])\n        bic[i, j] = BIC(nll[i, j], n, p[j])\n\n\nlen(glacierbac)\n\n45\n\n\n\ndfAIC = pd.DataFrame(np.round(aic, 2), columns=models, index=strains)\ndfBIC = pd.DataFrame(np.round(bic, 2), columns=models, index=strains)\ndfLOOCV = pd.DataFrame(np.round(loo_nll, 2), columns=models, index=strains)\n\n\ndfAIC\n\n\n\n\n\n\n\n\nb1\nb2\nflexTPC\nepc\nkum\n\n\n\n\n55\n-60.66\n-63.68\n-68.63\n-36.67\n-66.59\n\n\n60\n-37.43\n-46.32\n-55.13\n-34.74\n-56.17\n\n\n76\n-31.86\n-30.17\n-34.69\n-24.30\n-35.72\n\n\n137\n-41.18\n-38.82\n-72.28\n-51.64\n-71.47\n\n\n158\n-52.02\n-64.14\n-90.24\n-85.37\n-92.14\n\n\n\n\n\n\n\n\ndfBIC\n\n\n\n\n\n\n\n\nb1\nb2\nflexTPC\nepc\nkum\n\n\n\n\n55\n-59.07\n-61.69\n-66.25\n-34.29\n-64.20\n\n\n60\n-36.64\n-45.33\n-53.95\n-33.56\n-54.99\n\n\n76\n-31.07\n-29.19\n-33.50\n-23.12\n-34.54\n\n\n137\n-41.40\n-39.09\n-72.61\n-51.96\n-71.79\n\n\n158\n-51.24\n-63.15\n-89.06\n-84.19\n-90.96\n\n\n\n\n\n\n\n\ndfLOOCV\n\n\n\n\n\n\n\n\nb1\nb2\nflexTPC\nepc\nkum\n\n\n\n\n55\n0.27\n-3.35\n-3.67\n-2.21\n-3.57\n\n\n60\n2.53\n-3.13\n-3.73\n-2.60\n-3.79\n\n\n76\ninf\n-2.23\n-2.59\n-2.02\n-2.65\n\n\n137\n-1.32\n-3.49\n-6.02\n-4.55\n-5.96\n\n\n158\ninf\n-4.12\n-5.68\n-5.41\n-5.79\n\n\n\n\n\n\n\n\ndef get_delta(df):\n    '''\n    Turns results into difference relative to best model.\n    '''\n    best = np.min(df, axis=1) \n    return(df.subtract(np.min(df, axis=1), axis=0))\n\n\nget_delta(dfAIC)\n\n\n\n\n\n\n\n\nb1\nb2\nflexTPC\nepc\nkum\n\n\n\n\n55\n7.97\n4.95\n0.00\n31.96\n2.04\n\n\n60\n18.74\n9.85\n1.04\n21.43\n0.00\n\n\n76\n3.86\n5.55\n1.03\n11.42\n0.00\n\n\n137\n31.10\n33.46\n0.00\n20.64\n0.81\n\n\n158\n40.12\n28.00\n1.90\n6.77\n0.00\n\n\n\n\n\n\n\n\nget_delta(dfBIC)\n\n\n\n\n\n\n\n\nb1\nb2\nflexTPC\nepc\nkum\n\n\n\n\n55\n7.18\n4.56\n0.00\n31.96\n2.05\n\n\n60\n18.35\n9.66\n1.04\n21.43\n0.00\n\n\n76\n3.47\n5.35\n1.04\n11.42\n0.00\n\n\n137\n31.21\n33.52\n0.00\n20.65\n0.82\n\n\n158\n39.72\n27.81\n1.90\n6.77\n0.00\n\n\n\n\n\n\n\n\nget_delta(dfLOOCV)\n\n\n\n\n\n\n\n\nb1\nb2\nflexTPC\nepc\nkum\n\n\n\n\n55\n3.94\n0.32\n0.00\n1.46\n0.10\n\n\n60\n6.32\n0.66\n0.06\n1.19\n0.00\n\n\n76\ninf\n0.42\n0.06\n0.63\n0.00\n\n\n137\n4.70\n2.53\n0.00\n1.47\n0.06\n\n\n158\ninf\n1.67\n0.11\n0.38\n0.00\n\n\n\n\n\n\n\n\nget_delta(dfLOOCV[['b1', 'b2', 'flexTPC']])\n\n\n\n\n\n\n\n\nb1\nb2\nflexTPC\n\n\n\n\n55\n43.31\n3.48\n0.0\n\n\n60\n56.34\n5.40\n0.0\n\n\n76\ninf\n3.25\n0.0\n\n\n137\n32.92\n17.73\n0.0\n\n\n158\ninf\n14.05\n0.0\n\n\n\n\n\n\n\n\nnp.mean(get_delta(dfLOOCV), axis=0)\n\nb1           inf\nb2         9.182\nflexTPC    0.400\nepc        9.222\nkum        0.288\ndtype: float64\n\n\n\npd.DataFrame({'LOOCV':np.mean(get_delta(dfLOOCV), axis=0)})\n\n\n\n\n\n\n\n\nLOOCV\n\n\n\n\nb1\ninf\n\n\nb2\n9.182\n\n\nflexTPC\n0.400\n\n\nepc\n9.222\n\n\nkum\n0.288"
  },
  {
    "objectID": "model_comparison/Additional Figures.html",
    "href": "model_comparison/Additional Figures.html",
    "title": "Figure 1",
    "section": "",
    "text": "from matplotlib import pyplot as plt\n\nimport numpy as np\nimport pandas as pd\ndef briere1(T, Tmin=10.0, Tmax=40.0, c=0.001):\n    cond = (T &gt; Tmin) & (T &lt; Tmax)\n    r = np.zeros(len(T))\n    r[cond] = c * T[cond] * (T[cond] - Tmin) * np.sqrt(Tmax - T[cond])\n    return r\n\ndef briere2(T, Tmin=10.0, Tmax=50.0, c=1.0, m=2.0):\n    result = np.zeros(len(T))\n    cond = (Tmin &lt; T) & (T &lt; Tmax)\n    result[cond] = c * T[cond] * np.exp(np.log(T[cond] - Tmin) +  np.log(Tmax - T[cond]) / m)\n    return result\n\n# Fully biologically interpretable parametrization of flexTPC model\ndef flexTPC(T, Tmin=10.0, Tmax=40.0, rmax=1.0, α=0.8, β=0.2):\n    result = np.zeros(len(T))\n    cond = (Tmin &lt; T) & (T &lt; Tmax)\n    s = α * (1.0 - α) / β**2\n    result[cond] = rmax * np.exp(s * (α * (np.log(T[cond] - Tmin) - np.log(α)) +\n                                      (1 - α) * (np.log(Tmax - T[cond]) - np.log(1 - α) )\n                                       - np.log(Tmax - Tmin)))\n    return result\n\n# Exponential product curve.\ndef expprodcurve(T, Tmin=10.0, Tmax=50.0, c=1.0, kI=0.1, kU=0.5):\n    result = np.zeros(len(T))\n    cond = (Tmin &lt; T) & (T &lt; Tmax)\n    result[cond] = c * (1.0 - np.exp(-kI*(T[cond] - Tmin))) * (1.0 - np.exp(-kU*(Tmax - T[cond])))\n    return result\n\ndef flexTPCnd(τ, α=0.8, β=0.2):\n    s = α * (1.0 - α) / β**2\n    cond = (τ &gt; 0) & (τ &lt; 1)\n    r = np.zeros(len(τ))\n    r[cond] = np.exp(s * (α * (np.log(τ[cond]) - np.log(α)) + (1 - α) *  (np.log(1 - τ[cond]) - np.log(1-α))))\n    return r\ndef f1(τ, α, s, wref=np.exp(-1/8)):\n    return np.exp(np.log(α) + (1/ (α * s)) * np.log(wref) +((1-α)/(α)) * np.log((1-α)/(1-τ)))\n\ndef f2(τ, α, s, wref=np.exp(-1/8)):\n    return 1 - np.exp(np.log(1 - α) + (1/ ((1 - α) * s)) * np.log(wref) + (α / (1-α)) * np.log(α / τ))\n\n# Invert nondimensional flex numerically\ndef flexTPCroots(α, β, wref=np.exp(-1/8), err_tol=1e-3):\n    s = α * (1.0 - α) / β**2\n    τ1_prev = α * wref\n    τ2_prev = α + (1-α) * (1 - wref)\n\n    τ1 = f1(τ1_prev, α, s, wref=wref)\n    τ2 = f2(τ2_prev, α, s, wref=wref)\n    n_iter = 1\n    err = np.maximum(np.abs((τ1 - τ1_prev) / τ1_prev), np.abs((τ2 - τ2_prev) / τ2_prev))\n    while err &gt; err_tol:\n        τ1_prev = τ1\n        τ2_prev = τ2\n        τ1 = f1(τ1, α, s, wref=wref)\n        τ2 = f2(τ2, α, s, wref=wref)\n        err = np.maximum(np.abs((τ1 - τ1_prev) / τ1_prev), np.abs((τ2 - τ2_prev) / τ2_prev))\n        n_iter += 1\n    return τ1, τ2, n_iter, err"
  },
  {
    "objectID": "model_comparison/Additional Figures.html#panel-a",
    "href": "model_comparison/Additional Figures.html#panel-a",
    "title": "Figure 1",
    "section": "Panel A",
    "text": "Panel A\n\nτ = np.arange(0, 1.01, 0.01)\n\nfig, axarr = plt.subplots(4, 5)\n\nfor i, α in enumerate([0.01, 0.25, 0.5, 0.75, 0.99]):\n    for j, β in enumerate([0.03, 0.1, 0.4, 1.0]):\n        ax = axarr[j, i]\n        ax.plot(τ, flexTPCnd(τ, α=α, β=β))\n        \n        if (i == 2) and (j == 0):\n            ax.text(0.5, 1.5, r\"$\\alpha$\", fontsize=14)\n            ax.annotate('', xy=(-2.4, 1.25), xycoords='axes fraction',\n                       xytext=(3.4, 1.25), arrowprops=dict(arrowstyle='&lt;-', color='black'))\n            #ax.arrow(0, 2, 1, 0, clip_on=False)\n        \n        if (i == 0) and (j == 2):\n            ax.text(-0.65, 1.25, r\"$\\beta$\", fontsize=14)\n            ax.annotate('', xy=(-0.3, -1.25), xycoords='axes fraction',\n                       xytext=(-0.3, 3.35), arrowprops=dict(arrowstyle='-&gt;', color='black'))\n        # Hide X and Y axes label marks\n        ax.xaxis.set_tick_params(labelbottom=False)\n        ax.yaxis.set_tick_params(labelleft=False)\n\n        # Hide X and Y axes tick marks\n        ax.set_xticks([])\n        ax.set_yticks([])\n\nplt.tight_layout()\nplt.savefig(\"flexTPC_shapes.svg\")\nplt.show()\n\n/var/folders/g8/wjqhnz1n01d5l4pdv8cnykw40000gp/T/ipykernel_13589/1451454079.py:28: UserWarning: Tight layout not applied. tight_layout cannot make axes width small enough to accommodate all axes decorations\n  plt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nT = np.arange(0, 50, 0.01)\n#Previously s=1\nα = 0.75\nβ = 0.4\nwref = np.exp(-1/8)\n\nplt.plot(T, flexTPC(T, α=α, β=β))\nplt.xlabel(r\"$T$\", fontsize=20)\nplt.ylabel(r\"$r(T)$\", fontsize=20)\n#plt.title(\"The flexTPC model\", fontsize=18)\n\n\n#plt.plot(10, 0, '.k')\nplt.text(10 + 0.5, 0.02, r\"$T_{\\min}$\", fontsize=18)\nplt.axvline(x=10, linestyle='--', color='black', linewidth=2)\n\n\n#plt.plot(40, 0, '.k')\nplt.text(40 - 4.0, 0.02, r\"$T_{\\max}$\", fontsize=18)\nplt.axvline(x=40, linestyle='--', color='black', linewidth=2)\n\n\n#plt.text(0.2* 10 + 0.8*40, 0.1, r\"$T_{\\mathrm{opt}}$\")\nplt.plot(0.25* 10 + 0.75*40, 1.0, '.k', markersize=10)\n\nplt.axvline(x=0.25* 10 + 0.75*40, linestyle='--', color='darkgreen', alpha=0.75, linewidth=2)\nplt.text(0.25* 10 + 0.75*40 - 14 - 2.5, 0.2, r\"$T_{\\mathrm{opt}} = \\alpha T_{\\max} + (1 - \\alpha) T_{\\min}$\",\n         fontsize=18, color='darkgreen')\n\nτ1, τ2, n_iter, err = flexTPCroots(α, β, wref=wref, err_tol=1e-10)\nT1 = τ1 * (40 - 10) + 10\nT2 = τ2 * (40 - 10) + 10\n\nplt.plot(T1, wref, '.k', markersize=10)\nplt.plot(T2, wref, '.k', markersize=10)\nplt.hlines(y=wref, xmin=T1, xmax=T2, \n            linestyle='--', color='orange', alpha=0.75, linewidth=2)\n\n\n\nplt.text(0.25* 10 + 0.75*40 - 9.5, 0.74, r\"$\\mathrm{TB} \\approx \\beta (T_{\\max} - T_{\\min})$\",\n         fontsize=18, color='orange')\n\nplt.text(15, 0.95, r\"$r_{\\max}$\", fontsize=18)\nplt.axhline(y=1.0, linestyle='--', color='black', linewidth=2)\n\n\nplt.xlim(10 - 1, 40 + 1)\nplt.ylim(-0.001, 1.02)\nax = plt.gca()\n\n# Hide X and Y axes label marks\nax.xaxis.set_tick_params(labelbottom=False)\nax.yaxis.set_tick_params(labelleft=False)\n\n# Hide X and Y axes tick marks\nax.set_xticks([])\nax.set_yticks([])\n\nplt.savefig('scaling_params.svg')\nplt.show()"
  },
  {
    "objectID": "model_comparison/Additional Figures.html#figure-2",
    "href": "model_comparison/Additional Figures.html#figure-2",
    "title": "Figure 1",
    "section": "Figure 2",
    "text": "Figure 2\n\nmodels = ['b1', 'b2', 'flexTPC']\nlabel_name = {'b1': 'Briere1', 'b2': 'Briere2', 'flexTPC':'flexTPC'}\nmodel_r = {'b1':briere1, 'b2':briere2, 'flexTPC':flexTPC}\ncolor = {'b1':'firebrick',\n         'b2':'orange',\n         'flexTPC':'darkgreen'}\ndataset = {'botrana':pd.read_csv('briere_data_L_botrana.csv'),\n           'glacierbac':pd.read_csv('glacierbac.csv'),\n           'abcoli':pd.read_csv('ab_data.csv')}\ndataset['abcoli'] = dataset['abcoli'][dataset['abcoli'][\"drug2name\"] == \"WT\"] # Filter only single drugs\ndataset['abcoli'] = dataset['abcoli'].groupby([\"drug1name\", \"T\"], as_index=False).agg({'OD':'mean'}) # Calculate mean OD.\n\nfit_params = {'botrana':{model:pd.read_csv(f'botrana_params_{model}.csv') for model in models},\n              'glacierbac':{model:pd.read_csv(f'glacierbac_params_{model}.csv') for model in models},\n              'abcoli':{model:pd.read_csv(f'abcoli_params_{model}.csv') for model in models}}\ncurves_to_plot = {'botrana':[['eggs', 'instar 3', 'pupae'], \n                             ['eggs', 'i3', 'pupae']],\n                  'glacierbac':[['Pseudomonas (76)', 'Arthrobacter glacialis (137)', 'Arthrobacter glacialis (158)'], \n                                [76, 137, 158]],\n                  'abcoli':[['ERY', 'GEN', 'no drug'], \n                            ['ERY', 'GEN', 'WT']]}\n\nax_label_fontsize = 11\ntitle_fontsize = 12\n\nfig, axarr = plt.subplots(3, 3, figsize=(6.5*1.2, 5*1.2), sharex='col')\n\nT = np.arange(5, 36.5, 0.001)\ndata = dataset['botrana']\ni = 0\nfor plot_title, stage in zip(curves_to_plot['botrana'][0], curves_to_plot['botrana'][1]):\n    ax = axarr[i, 0]\n    ax.plot(data['T'], 1 / data[stage], '^', markersize=7)\n\n    for model in models:\n        params = fit_params['botrana'][model][fit_params['botrana'][model]['stage'] == stage]\n        ax.plot(T, model_r[model](T, *params.iloc[0, 1:-1]), color=color[model],\n               label=label_name[model])\n    if i == 2:\n        ax.set_xlabel(\"Temperature [°C]\", fontsize=ax_label_fontsize)\n    ax.set_xlim(5, 38)\n    ax.set_xticks([10, 20, 30])\n    ax.set_ylabel(\"Dev. rate [1 / day]\", fontsize=ax_label_fontsize)\n    ax.set_ylim(-0.05*np.max(1 / data[stage]), 1.25*np.max(1 / data[stage]))\n    ax.set_title(plot_title, fontsize=title_fontsize)\n    i += 1\n\n\nT = np.arange(-10, 40, 0.001)\ndata = dataset['glacierbac']\ni = 0\nfor plot_title, strain in zip(curves_to_plot['glacierbac'][0], curves_to_plot['glacierbac'][1]):\n    ax = axarr[i, 1]\n    subset = data[data['strain'] == strain]\n    ax.plot(subset['T'], subset[\"r\"], '^', markersize=7)\n    for model in models:\n        params = fit_params['glacierbac'][model][fit_params['glacierbac'][model]['strain'] == strain]\n        ax.plot(T, model_r[model](T, *params.iloc[0, 2:-1]), color=color[model])\n    if i == 2:\n        ax.set_xlabel(\"Temperature [°C]\", fontsize=ax_label_fontsize) \n    ax.set_xlim(-5, 38)\n    ax.set_xticks([0, 10, 20, 30])\n    \n    ax.set_ylabel(\"Growth rate [1 / h]\", fontsize=ax_label_fontsize )\n    ax.set_ylim(0, 1.25*np.max(subset[\"r\"]) )\n    \n    ax.set_title(plot_title, fontsize=title_fontsize)\n    i += 1\n\n\nT = np.arange(10, 50, 0.001)\ndata = dataset['abcoli']\ni = 0\nfor plot_title, drug in zip(curves_to_plot['abcoli'][0], curves_to_plot['abcoli'][1]):\n    ax = axarr[i, 2]\n    subset = data[(data['drug1name'] == drug)]\n    ax.plot(subset['T'], subset[\"OD\"], '^', markersize=7)\n    for model in models:\n        params = fit_params['abcoli'][model][fit_params['abcoli'][model]['drug'] == drug]\n        ax.plot(T, model_r[model](T, *params.iloc[0, 1:-1]), color=color[model], label=label_name[model])\n    if i == 0:\n        ax.legend(fontsize=8, loc='upper left', fancybox=True, framealpha=0.5)\n    if i == 2:\n        ax.set_xlabel(\"Temperature [°C]\", fontsize=ax_label_fontsize)\n    ax.set_ylabel(\"OD\", fontsize=ax_label_fontsize )\n    ax.set_xticks([10, 20, 30, 40, 50])\n    #ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8])\n    ax.set_ylim(-0.05*np.max(subset[\"OD\"]), 1.3*np.max(subset[\"OD\"]) )\n    ax.set_title(plot_title, fontsize=title_fontsize)\n    i += 1\n\n\nfig.align_ylabels()\n\nplt.tight_layout(w_pad=4.5)\n\nplt.savefig(\"TPC_examples.svg\")\nplt.savefig(\"TPC_examples.png\")\nplt.savefig(\"TPC_examples.pdf\")"
  },
  {
    "objectID": "fit_examples.html#plotting-flextpc-curves",
    "href": "fit_examples.html#plotting-flextpc-curves",
    "title": "Tutorial",
    "section": "Plotting flexTPC curves",
    "text": "Plotting flexTPC curves\nFirst, we provide code that implements the flexTPC model. There are two different parametrizations of flexTPC that can be useful in different circumstances (see paper for details).\n\n# FlexTPC model for thermal performance curves.\n# (parametrized with alpha/beta)\nflexTPC &lt;- function(temp, Tmin, Tmax, rmax, alpha, beta) {\n  s &lt;- alpha * (1 - alpha) / beta^2\n  result &lt;- rep(0, length(temp))\n  Tidx = (temp &gt; Tmin) & (temp &lt; Tmax)\n  result[Tidx] &lt;- rmax * exp(s * (alpha * log( (temp[Tidx] - Tmin) / alpha) \n                  + (1 - alpha) * log( (Tmax - temp[Tidx]) / (1 - alpha)) \n                  - log(Tmax - Tmin)) ) \n  return(result)\n}\n  \n# FlexTPC model for thermal performance curves.\n# (parametrized with Topt/B)\nflexTPC2 &lt;- function(temp, Tmin, Tmax, rmax, Topt, B) {\n  alpha &lt;- (Topt - Tmin) / (Tmax - Tmin)\n  beta &lt;- B / (Tmax - Tmin)\n  if((alpha &gt;= 0.0) & (alpha &lt;= 1.0)) {\n    return(flexTPC(temp, Tmin, Tmax, rmax, alpha, beta))\n  } else {\n    return(rep(0, length(temp)))\n  }\n}\n\nLet’s plot some flexTPC curves to ensure the model is working correctly.\n\n# Creates a vector of temperatures from 5 to 38°C with a 0.1°C step.\ntemp &lt;- seq(5, 38, 0.1)\n\npar(mfrow=c(1,1))\n# Plots a flexTPC curve using the alpha/beta parametrization.\nplot(temp, flexTPC(temp, Tmin=10, Tmax=35, rmax=1.0, alpha=0.8, beta=0.2), type='l',\n     xlab='Temperature [°C]', ylab='Trait performance', lwd=2)\n\n# Plots a flexTPC curve using the Topt/B parametrization.\n# We are intentionally picking a Topt that is a little lower than the previous\n# curve so they are not plotted on top of each other.\nlines(temp, flexTPC2(temp, Tmin=10, Tmax=35, rmax=1.0, Topt=29.0, B=5.0),\n      col='red', lwd=2)\n\n\n\n\n\n\n\n\nAt this point, you may want to try modifying the parameter values to observe how this changes the curve. This can be a useful way to get intuition on what the parameters are doing to the curve and can be a good way to estimate reasonable starting values for the model parameters."
  },
  {
    "objectID": "fit_examples.html#an-example-dataset",
    "href": "fit_examples.html#an-example-dataset",
    "title": "Tutorial",
    "section": "An example dataset",
    "text": "An example dataset\nIn this tutorial, we will fit flexTPC to data on the lifespan of the northern house mosquito Culex pipiens and the southern house mosquito Culex quinquefasciatus. Let’s start by plotting the data.\n\nlf.data &lt;- read.csv('./mosquito_traits/TraitData_lf.csv')\nCpip.lf.data &lt;- subset(lf.data, lf.data$host.code == 'Cpip')\nCqui.lf.data &lt;- subset(lf.data, lf.data$host.code == 'Cqui')\n\npar(mfrow=c(1,2))\nplot(Cpip.lf.data$T, Cpip.lf.data$trait, xlim=c(10, 40), ylim=c(0, 150),\n     xlab='Temperature [°C]', ylab='Lifespan [days]', main='Cx pipiens',\n     pch=20, col='steelblue')\nplot(Cqui.lf.data$T, Cqui.lf.data$trait, xlim=c(10, 40), ylim=c(0, 150),\n     xlab='Temperature [°C]', ylab='Lifespan [days]', main='Cx quinquefasciatus',\n     pch=20, col='firebrick')"
  },
  {
    "objectID": "fit_examples.html#fitting-flextpc",
    "href": "fit_examples.html#fitting-flextpc",
    "title": "Tutorial",
    "section": "Fitting flexTPC",
    "text": "Fitting flexTPC\nWhen of fitting a thermal performance curve model (e.g. flexTPC) to data, our goal is to find the model parameters that are most consistent with the observations. In statistics, curve fitting problems are referred to as regression problems. More specifically, fitting a thermal performance curve is a parametric nonlinear regression, where the functional form that is used is the TPC model.\nThere are different methods to do this of varying complexity.\n\nLeast squares estimation is a simple method for estimating"
  },
  {
    "objectID": "fit_examples.html#fitting-thermal-performance-curves-is-a-regression-problem",
    "href": "fit_examples.html#fitting-thermal-performance-curves-is-a-regression-problem",
    "title": "Tutorial",
    "section": "Fitting thermal performance curves is a regression problem",
    "text": "Fitting thermal performance curves is a regression problem\nWhen of fitting a thermal performance curve model (e.g. flexTPC) to data, our goal is to find the model parameters that are most consistent with the observations. In statistics, curve fitting problems are referred to as regression problems. More specifically, fitting a thermal performance curve is a parametric nonlinear regression, where the functional form that is used is the TPC model.\nThere are different methods to do this of varying complexity.\n\nLeast squares estimation is a simple method for estimating"
  },
  {
    "objectID": "fit_examples.html#fitting-thermal-performance-curves-is-a-nonlinear-regression-problem",
    "href": "fit_examples.html#fitting-thermal-performance-curves-is-a-nonlinear-regression-problem",
    "title": "Tutorial",
    "section": "Fitting thermal performance curves is a nonlinear regression problem",
    "text": "Fitting thermal performance curves is a nonlinear regression problem\nWhen fitting a thermal performance curve model (e.g. flexTPC) to data, our goal is to find the model parameters that are most consistent with the observations. In statistics, curve fitting problems similar to this are referred to as regression problems. More specifically, fitting a thermal performance curve is a special case of a parametric nonlinear regression, where the functional form that is used is the TPC model.\nThere are different methods to do this of varying complexity.\n\nLeast squares estimation is a simple method that is popular in practice.\nMaximum likelihood estimation allows modeling the error distribution to better account for non-constant variance and for aspects of the data (e.g. that it is non-negative), but requires more statistical expertise.\nBayesian approaches use probability distributions to describe parameter uncertainty and allow incorporating information from other experiments or the habitat of the organism when estimating TPCs."
  },
  {
    "objectID": "least_squares.html",
    "href": "least_squares.html",
    "title": "Least squares estimation",
    "section": "",
    "text": "A simple approach to estimate thermal performance curves (TPCs) is to perform nonlinear least squares estimation. This finds the curve that minimizes the squared difference between the TPC and the data points.\nIn this example, we will fit the flexTPC model to data of the radial growth rates of the fungus Metarhizium anisopliae. This data is originally from Ouedraogo et al 1997 and digitized by Kontopoulos et al 2024.\nLet’s start by reading in the data and plotting it.\ndata &lt;- read.csv('TPC_data_examples.csv') # From Kontopoulos et al.\ndata &lt;- subset(data, data$id == 39697) # Subset Ouedraogo et al dataset.\n\nplot(data$temperature, data$trait_value, xlim=c(0, 40),\n     xlab='Temperature [°C]', ylab='radial growth rate [m / s]',\n     pch=20, col='steelblue')\nIn order to fit flexTPC to this data, we will need to code the model equation and an error function that calculates the squared difference between a curve and the data points.\n# FlexTPC model for thermal performance curves.\n# (parametrized with alpha/beta)\nflexTPC &lt;- function(temp, Tmin, Tmax, rmax, alpha, beta) {\n  s &lt;- alpha * (1 - alpha) / beta^2\n  result &lt;- rep(0, length(temp))\n  Tidx = (temp &gt; Tmin) & (temp &lt; Tmax)\n  result[Tidx] &lt;- rmax * exp(s * (alpha * log( (temp[Tidx] - Tmin) / alpha) \n                  + (1 - alpha) * log( (Tmax - temp[Tidx]) / (1 - alpha)) \n                  - log(Tmax - Tmin)) ) \n  return(result)\n}\n\n# Returns a function that calculates the squared error between data and a flexTPC curve.\nget_sq_err_fn &lt;- function(temp, y, lower.bounds, upper.bounds) {\n  # temp: Vector of measured temperature values.\n  # y: Vector of measured performance values.\n  # lower_bounds, upper_bounds: Lower and upper bounds for the parameters.\n  \n  f &lt;- function(par) {\n    # Return infinity (worst possible error) if parameters are outside of chosen\n    # upper and lower bounds.\n    if((sum(par &lt; lower.bounds) + sum(par &gt; upper.bounds)) &gt; 0) {\n      return(Inf)    \n    } else {\n      # Otherwise return squared difference between data and flexTPC curve.\n      return(sum((y - flexTPC(temp, par[1], par[2], par[3], par[4], par[5]))^2))\n  }\n  }\n  return(f) \n}\nWe also need an initial guess for the parameters for the flexTPC model. We will input these initial parameters and the error function to minimize into the function in R. The intuitive parameters of the flexTPC model make it simple to choose reasonable starting parameter values.\nLet’s pick some initial values and plot the data and a flexTPC curve with initial guesses for the parameters (picked visually).\ntemp &lt;- seq(0, 45, 0.1)\n\nplot(data$temperature, data$trait_value, xlim=c(0, 40), \n     xlab='Temperature [°C]', ylab='trait',\n     pch=20, col='steelblue')\nlines(temp, flexTPC(temp, Tmin=12, Tmax=36, rmax=3e-8, alpha=0.6,\n                    beta=0.3),\n     col='gray', lwd=2)\nWe will now find the parameters that minimize the squared error to our data. Before doing so, it is convenient to change the units of the data to help the optimization algorithm (as it sometimes has trouble if the parameters to optimize are of very different orders of magnitude). Let’s do this and fit the curves.\ntemp &lt;- data$temperature\ny &lt;- data$trait_value * 10^8 # Convert units of the trait value data so they are close to unit scale.\n\ninit.par &lt;- c(12, 36, 3, 0.6, 0.3)\nnames(init.par) &lt;- c(\"Tmin\", \"Tmax\", \"rmax\", \"alpha\", \"beta\")\n\n\n# Lower and upper bounds for the parameters.\nlower.bounds &lt;- c(0, 30, 0, 0.1, 0.1)\nupper.bounds &lt;- c(15, 45, 10, 0.9, 0.7)\n\n\nf &lt;- get_sq_err_fn(temp, y, lower.bounds, upper.bounds) # Get error function with the chosen bounds of the parameters.\nfit &lt;- optim(init.par, f, control=list(maxit=10000))\nfit$par\n\n      Tmin       Tmax       rmax      alpha       beta \n11.0733505 35.1772202  2.8645324  0.6543052  0.3870970\nLet’s plot the data along with the initial parameters (in gray) and the fitted curves (in blue).\ntemp &lt;- seq(0, 45, 0.1)\n\nplot(data$temperature, y, xlim=c(0, 40), #ylim=c(0, 150),\n     xlab='Temperature [°C]', ylab='radial growth rate [m / s] * 10^8',\n     pch=20, col='steelblue')\nlines(temp, flexTPC(temp, Tmin=12, Tmax=36, rmax=3, alpha=0.6,\n                    beta=0.3),\n     col='gray', lwd=2)\nlines(temp, flexTPC(temp, Tmin=fit$par[1], Tmax=fit$par[2], rmax=fit$par[3], alpha=fit$par[4], beta=fit$par[5]),\n     col='steelblue', lwd=2)"
  },
  {
    "objectID": "least_squares.html#maximum-likelihood-estimation",
    "href": "least_squares.html#maximum-likelihood-estimation",
    "title": "Least squares estimation",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nWhen fitting a thermal performance curve model to data it is helpful to think about the model being comprised of two parts:\n\nA functional form \\(r(T)\\) for the thermal performance curve (e.g. flexTPC, the Briere model, etc.) that describes the temperature dependence of a biological trait.\nA model of the errors, which describes a plausible mechanism of generating the observed datapoints from the model around the mean given by \\(r(T)\\).\n\nLeast squares estimation is equivalent to assuming the model residuals are normally distributed with a constant variance."
  },
  {
    "objectID": "mle_estimation.html",
    "href": "mle_estimation.html",
    "title": "Maximum likelihood estimation",
    "section": "",
    "text": "As in other regression problems, when fitting a thermal performance curve model to data it is helpful to think about the model as consisting of two parts:\nPreviously, we showed how to fit flexTPC with least squares estimation. In least squares estimation, we do not model the second part explicitly. Rather, we assume that the “best curve” is the one that minimizes the square difference of the data curves and the points.\nHowever, it turns out that least squares estimation is equivalent to assuming the errors are normally distributed with a constant variance. These assumptions may need to be modified for thermal performance data because of various reasons:\nWe can see that both of these things are true our data!"
  },
  {
    "objectID": "fit_examples.html#fitting-thermal-performance-curves",
    "href": "fit_examples.html#fitting-thermal-performance-curves",
    "title": "Tutorial",
    "section": "Fitting thermal performance curves",
    "text": "Fitting thermal performance curves\nWhen fitting a thermal performance curve model (e.g. flexTPC) to data, our goal is to find the model parameters that are most consistent with the observations. In statistics, curve fitting problems similar to this are referred to as regression problems. More specifically, this is a parametric nonlinear regression, where the regression function that is used is the TPC model.\nThere are different methods to estimate parameters in a regression problem.\n\nLeast squares estimation is a simple method that is popular in practice.\nMaximum likelihood estimation allows modeling the error distribution to better account for non-constant variance and for aspects of the data (e.g. that it is non-negative), but requires more statistical expertise.\nBayesian approaches use probability distributions to describe parameter uncertainty and allow incorporating information from other experiments or the habitat of the organism when estimating TPCs."
  },
  {
    "objectID": "mle_estimation.html#extending-a-tpc-model-to-a-statistical-model",
    "href": "mle_estimation.html#extending-a-tpc-model-to-a-statistical-model",
    "title": "Maximum likelihood estimation",
    "section": "Extending a TPC model to a statistical model",
    "text": "Extending a TPC model to a statistical model\nThis is a simple method ."
  },
  {
    "objectID": "mle_estimation.html#extending-a-deterministic-tpc-model-to-a-statistical-model",
    "href": "mle_estimation.html#extending-a-deterministic-tpc-model-to-a-statistical-model",
    "title": "Maximum likelihood estimation",
    "section": "Extending a deterministic TPC model to a statistical model",
    "text": "Extending a deterministic TPC model to a statistical model\nWe can deal with these issues by explicitly modeling how the data points are generated by our model.\n\\[y_{i}|T_{i}   \\sim\\begin{cases}\n\\mathrm{Gamma}(\\mu=r(T_{i};\\mathcal{P}),\\sigma) & T_{\\min}&lt;T_{i}&lt;T_{\\max}\\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nWith the following model for the standard deviation.\n\\[\\log\\sigma    =\\eta_0 + \\eta_1 r(T_{i};\\mathcal{P}) \\] We can then estimate the parameters of our model through maximum likelihood estimation.\nThe likelihood of a statistical model is a function that describes how compatible the model parameters are with the data. It is defined as the probability of observing the data given some specific fixed values of the model parameters (in our case, these correspond to \\({T_{\\min}, T_{\\max}, r_{\\max}, \\alpha, \\beta, \\eta_{0}, \\eta_{1}}\\)).\nThe maximum likelihood estimate (MLE) is the parameter values that maximize the likelihood. Rather than maximizing the likelihood directly, this is usually coded as minimizing the negative log likelihood, which is equivalent but tends to be more numerically stable.\n\n# FlexTPC model for thermal performance curves.\n# (parametrized with alpha/beta)\nflexTPC &lt;- function(temp, Tmin, Tmax, rmax, alpha, beta) {\n  s &lt;- alpha * (1 - alpha) / beta^2\n  result &lt;- rep(0, length(temp))\n  Tidx = (temp &gt; Tmin) & (temp &lt; Tmax)\n  result[Tidx] &lt;- rmax * exp(s * (alpha * log( (temp[Tidx] - Tmin) / alpha) \n                  + (1 - alpha) * log( (Tmax - temp[Tidx]) / (1 - alpha)) \n                  - log(Tmax - Tmin)) ) \n  return(result)\n}\n\n# Returns a function that calculates the negative log likelihood.\nget_negloglk_fn &lt;- function(temp, y, lower.bounds, upper.bounds) {\n  # temp: Vector of measured temperature values.\n  # y: Vector of measured performance values.\n  # lower_bounds, upper_bounds: Lower and upper bounds for the parameters.\n  \n  f &lt;- function(par) {\n    # Return infinity (worst possible error) if parameters are outside of upper and lower bounds.\n    if((sum(par &lt; lower.bounds) + sum(par &gt; upper.bounds)) &gt; 0) {\n      return(Inf)    \n    } else {\n      \n      Tmin &lt;- par[1]\n      Tmax &lt;- par[2]\n      rmax &lt;- par[3]\n      alpha &lt;- par[4]\n      beta &lt;- par[5]\n      eta0 &lt;- par[6]\n      eta1 &lt;- par[7]\n      \n      # Return negative log-likelihood.\n      mu = pmax(flexTPC(temp, Tmin, Tmax, rmax, alpha, beta), 1e-20)\n      sigma = exp(eta0 + eta1 * mu / 50)\n      \n      # Convert mean and standard deviation to shape and rate parameters for\n      # Gamma distribution.\n      a = mu^2 / sigma^2\n      b = mu / sigma^2\n      \n      return(sum(-dgamma(y, a, b, log=TRUE)))\n  }\n  }\n  return(f) \n}\n\n# Returns a function that calculates the negative log likelihood.\nget_negloglk_fn &lt;- function(temp, y, lower.bounds, upper.bounds) {\n  # temp: Vector of measured temperature values.\n  # y: Vector of measured performance values.\n  # lower_bounds, upper_bounds: Lower and upper bounds for the parameters.\n  \n  f &lt;- function(par) {\n    # Return infinity (worst possible error) if parameters are outside of upper and lower bounds.\n    if((sum(par &lt; lower.bounds) + sum(par &gt; upper.bounds)) &gt; 0) {\n      return(Inf)    \n    } else {\n      \n      # Rename parameters for readability of code.\n      Tmin &lt;- par[1]\n      Tmax &lt;- par[2]\n      rmax &lt;- par[3]\n      alpha &lt;- par[4]\n      beta &lt;- par[5]\n      eta0 &lt;- par[6]\n      eta1 &lt;- par[7]\n      \n      T.idx &lt;- (temp &gt; Tmin) & (temp &lt; Tmax)\n      \n      # Return infinity if any measurements outside the (Tmin, Tmax)\n      # temperature range are nonzero.\n      if(sum(temp[!T.idx] != 0) &gt; 0) {\n        return(Inf)\n      }\n      \n      # Return negative log-likelihood.\n      mu = pmax(flexTPC(temp[T.idx], Tmin, Tmax, rmax, alpha, beta), 1e-20)\n      sigma = exp(eta0 + eta1 * mu / 50)\n      \n      # Convert mean and standard deviation to shape and rate parameters for\n      # Gamma distribution.\n      a = mu^2 / sigma^2\n      b = mu / sigma^2\n      \n      return(sum(-dgamma(y, a, b, log=TRUE)))\n  }\n  }\n  return(f) \n}\n\nLet’s now fit the curves.\n\n# Cx. pipiens data.\ninit.par &lt;- c(5, 37, 80, 0.3, 0.3, 2, 1)\nlower.bounds &lt;- c(0, 30, 0, 0.1, 0.1, -10, -10)\nupper.bounds &lt;- c(10, 45, 200, 0.9, 0.5, 10, 10)\ntemp &lt;- Cpip.lf.data$T\ny &lt;- Cpip.lf.data$trait\n\nf &lt;- get_negloglk_fn(temp, y, lower.bounds, upper.bounds)\nCpip.fit &lt;- optim(init.par, f, control=list(maxit=10000))\n\n\n# Cx. quinquefasciatus data.\ninit.par &lt;-     c(5,  37,  60, 0.3, 0.3,     2,   1)\nlower.bounds &lt;- c(0,  30,   0, 0.1, 0.1, -10, -10)\nupper.bounds &lt;- c(10, 45, 200, 0.9, 0.5,    10,  10)\ntemp &lt;- Cqui.lf.data$T\ny &lt;- Cqui.lf.data$trait\n\nf &lt;- get_negloglk_fn(temp, y, lower.bounds, upper.bounds)\nCqui.fit &lt;- optim(init.par, f, control=list(maxit=10000))\n\n\nCpip.fit$par\n\n[1]  0.2593252 44.9437612 85.8153498  0.2661140  0.2125402  1.9079440  0.9729116\n\nCqui.fit$par\n\n[1] 9.003182e-07 3.420246e+01 7.074355e+01 3.615743e-01 3.682643e-01\n[6] 1.224621e+00 1.319640e+00\n\n\n\ntemp &lt;- seq(0, 45, 0.1)\n\n\npar(mfrow=c(1,2))\nplot(Cpip.lf.data$T, Cpip.lf.data$trait, xlim=c(0, 40), ylim=c(0, 150),\n     xlab='Temperature [°C]', ylab='Lifespan [days]', main='Cx pipiens',\n     pch=20, col='steelblue')\nlines(temp, flexTPC(temp, Tmin=5, Tmax=37, rmax=80, alpha=0.3, beta=0.3),\n     col='gray', lwd=2)\nlines(temp, flexTPC(temp, Tmin=Cpip.fit$par[1], Tmax=Cpip.fit$par[2], rmax=Cpip.fit$par[3], alpha=Cpip.fit$par[4], beta=Cpip.fit$par[5]),\n     col='steelblue', lwd=2)\n\nplot(Cqui.lf.data$T, Cqui.lf.data$trait, xlim=c(0, 40), ylim=c(0, 150),\n     xlab='Temperature [°C]', ylab='Lifespan [days]', main='Cx quinquefasciatus',\n     pch=20, col='firebrick')\nlines(temp, flexTPC(temp, Tmin=5, Tmax=37, rmax=60, alpha=0.3, beta=0.3),\n    col='gray', lwd=2)\nlines(temp, flexTPC(temp, Tmin=Cqui.fit$par[1], Tmax=Cqui.fit$par[2], rmax=Cqui.fit$par[3], alpha=Cqui.fit$par[4], beta=Cqui.fit$par[5]),\n     col='firebrick', lwd=2)"
  },
  {
    "objectID": "bayesian.html",
    "href": "bayesian.html",
    "title": "Bayesian inference",
    "section": "",
    "text": "In Bayesian statistics, we represent parameter uncertainty with probability distributions. We start by choosing a prior distribution for every model parameter that represents the initial uncertainty in these parameters (before seeing any data).\nIn the analysis these prior distributions are then updated to a posterior distribution that represents the remaining uncertainty in the analysis."
  },
  {
    "objectID": "bayesian.html#extending-a-deterministic-tpc-model-to-a-statistical-model",
    "href": "bayesian.html#extending-a-deterministic-tpc-model-to-a-statistical-model",
    "title": "Bayesian inference",
    "section": "Extending a deterministic TPC model to a statistical model",
    "text": "Extending a deterministic TPC model to a statistical model\nWe can deal with these issues by explicitly modeling how the data points are generated by our model.\n\\[y_{i}|T_{i}   \\sim\\begin{cases}\n\\mathrm{Gamma}(\\mu=r(T_{i};\\mathcal{P}),\\sigma) & T_{\\min}&lt;T_{i}&lt;T_{\\max}\\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nWith the following model for the standard deviation.\n\\[\\log\\sigma    =\\eta_0 + \\eta_1 r(T_{i};\\mathcal{P}) \\] We can then estimate the parameters of our model through maximum likelihood estimation.\nThe likelihood of a statistical model is a function that describes how compatible the model parameters are with the data. It is defined as the probability of observing the data given some specific fixed values of the model parameters (in our case, these correspond to \\({T_{\\min}, T_{\\max}, r_{\\max}, \\alpha, \\beta, \\eta_{0}, \\eta_{1}}\\)).\nThe maximum likelihood estimate (MLE) is the parameter values that maximize the likelihood. Rather than maximizing the likelihood directly, this is usually coded as minimizing the negative log likelihood, which is equivalent but tends to be more numerically stable.\n\n# FlexTPC model for thermal performance curves.\n# (parametrized with alpha/beta)\nflexTPC &lt;- function(temp, Tmin, Tmax, rmax, alpha, beta) {\n  s &lt;- alpha * (1 - alpha) / beta^2\n  result &lt;- rep(0, length(temp))\n  Tidx = (temp &gt; Tmin) & (temp &lt; Tmax)\n  result[Tidx] &lt;- rmax * exp(s * (alpha * log( (temp[Tidx] - Tmin) / alpha) \n                  + (1 - alpha) * log( (Tmax - temp[Tidx]) / (1 - alpha)) \n                  - log(Tmax - Tmin)) ) \n  return(result)\n}\n\n# Returns a function that calculates the negative log likelihood.\nget_negloglk_fn &lt;- function(temp, y, lower.bounds, upper.bounds) {\n  # temp: Vector of measured temperature values.\n  # y: Vector of measured performance values.\n  # lower_bounds, upper_bounds: Lower and upper bounds for the parameters.\n  \n  f &lt;- function(par) {\n    # Return infinity (worst possible error) if parameters are outside of upper and lower bounds.\n    if((sum(par &lt; lower.bounds) + sum(par &gt; upper.bounds)) &gt; 0) {\n      return(Inf)    \n    } else {\n      \n      Tmin &lt;- par[1]\n      Tmax &lt;- par[2]\n      rmax &lt;- par[3]\n      alpha &lt;- par[4]\n      beta &lt;- par[5]\n      eta0 &lt;- par[6]\n      eta1 &lt;- par[7]\n      \n      # Return negative log-likelihood.\n      mu = pmax(flexTPC(temp, Tmin, Tmax, rmax, alpha, beta), 1e-20)\n      sigma = exp(eta0 + eta1 * mu / 50)\n      \n      # Convert mean and standard deviation to shape and rate parameters for\n      # Gamma distribution.\n      a = mu^2 / sigma^2\n      b = mu / sigma^2\n      \n      return(sum(-dgamma(y, a, b, log=TRUE)))\n  }\n  }\n  return(f) \n}\n\n# Returns a function that calculates the negative log likelihood.\nget_negloglk_fn &lt;- function(temp, y, lower.bounds, upper.bounds) {\n  # temp: Vector of measured temperature values.\n  # y: Vector of measured performance values.\n  # lower_bounds, upper_bounds: Lower and upper bounds for the parameters.\n  \n  f &lt;- function(par) {\n    # Return infinity (worst possible error) if parameters are outside of upper and lower bounds.\n    if((sum(par &lt; lower.bounds) + sum(par &gt; upper.bounds)) &gt; 0) {\n      return(Inf)    \n    } else {\n      \n      # Rename parameters for readability of code.\n      Tmin &lt;- par[1]\n      Tmax &lt;- par[2]\n      rmax &lt;- par[3]\n      alpha &lt;- par[4]\n      beta &lt;- par[5]\n      eta0 &lt;- par[6]\n      eta1 &lt;- par[7]\n      \n      T.idx &lt;- (temp &gt; Tmin) & (temp &lt; Tmax)\n      \n      # Return infinity if any measurements outside the (Tmin, Tmax)\n      # temperature range are nonzero.\n      if(sum(temp[!T.idx] != 0) &gt; 0) {\n        return(Inf)\n      }\n      \n      # Return negative log-likelihood.\n      mu = pmax(flexTPC(temp[T.idx], Tmin, Tmax, rmax, alpha, beta), 1e-20)\n      sigma = exp(eta0 + eta1 * mu / 50)\n      \n      # Convert mean and standard deviation to shape and rate parameters for\n      # Gamma distribution.\n      a = mu^2 / sigma^2\n      b = mu / sigma^2\n      \n      return(sum(-dgamma(y, a, b, log=TRUE)))\n  }\n  }\n  return(f) \n}\n\nLet’s now fit the curves.\n\n# Cx. pipiens data.\ninit.par &lt;- c(5, 37, 80, 0.3, 0.3, 2, 1)\nlower.bounds &lt;- c(0, 30, 0, 0.1, 0.1, -10, -10)\nupper.bounds &lt;- c(10, 45, 200, 0.9, 0.5, 10, 10)\ntemp &lt;- Cpip.lf.data$T\ny &lt;- Cpip.lf.data$trait\n\nf &lt;- get_negloglk_fn(temp, y, lower.bounds, upper.bounds)\nCpip.fit &lt;- optim(init.par, f, control=list(maxit=10000))\n\n\n# Cx. quinquefasciatus data.\ninit.par &lt;-     c(5,  37,  60, 0.3, 0.3,     2,   1)\nlower.bounds &lt;- c(0,  30,   0, 0.1, 0.1, -10, -10)\nupper.bounds &lt;- c(10, 45, 200, 0.9, 0.5,    10,  10)\ntemp &lt;- Cqui.lf.data$T\ny &lt;- Cqui.lf.data$trait\n\nf &lt;- get_negloglk_fn(temp, y, lower.bounds, upper.bounds)\nCqui.fit &lt;- optim(init.par, f, control=list(maxit=10000))\n\n\nCpip.fit$par\n\n[1]  0.2593252 44.9437612 85.8153498  0.2661140  0.2125402  1.9079440  0.9729116\n\nCqui.fit$par\n\n[1] 9.003182e-07 3.420246e+01 7.074355e+01 3.615743e-01 3.682643e-01\n[6] 1.224621e+00 1.319640e+00\n\n\n\ntemp &lt;- seq(0, 45, 0.1)\n\n\npar(mfrow=c(1,2))\nplot(Cpip.lf.data$T, Cpip.lf.data$trait, xlim=c(0, 40), ylim=c(0, 150),\n     xlab='Temperature [°C]', ylab='Lifespan [days]', main='Cx pipiens',\n     pch=20, col='steelblue')\nlines(temp, flexTPC(temp, Tmin=5, Tmax=37, rmax=80, alpha=0.3, beta=0.3),\n     col='gray', lwd=2)\nlines(temp, flexTPC(temp, Tmin=Cpip.fit$par[1], Tmax=Cpip.fit$par[2], rmax=Cpip.fit$par[3], alpha=Cpip.fit$par[4], beta=Cpip.fit$par[5]),\n     col='steelblue', lwd=2)\n\nplot(Cqui.lf.data$T, Cqui.lf.data$trait, xlim=c(0, 40), ylim=c(0, 150),\n     xlab='Temperature [°C]', ylab='Lifespan [days]', main='Cx quinquefasciatus',\n     pch=20, col='firebrick')\nlines(temp, flexTPC(temp, Tmin=5, Tmax=37, rmax=60, alpha=0.3, beta=0.3),\n    col='gray', lwd=2)\nlines(temp, flexTPC(temp, Tmin=Cqui.fit$par[1], Tmax=Cqui.fit$par[2], rmax=Cqui.fit$par[3], alpha=Cqui.fit$par[4], beta=Cqui.fit$par[5]),\n     col='firebrick', lwd=2)"
  },
  {
    "objectID": "least_squares_mdr.html",
    "href": "least_squares_mdr.html",
    "title": "Least squares estimation",
    "section": "",
    "text": "A simple approach to estimate TPCs is to perform nonlinear least squares estimation. This finds the curve that minimizes the squared difference between the TPC and the data points.\nLet’s start by reading the data.\n\nlf.data &lt;- read.csv('./mosquito_traits/TraitData_lf.csv')\nCpip.lf.data &lt;- subset(lf.data, lf.data$host.code == 'Cpip')\nCqui.lf.data &lt;- subset(lf.data, lf.data$host.code == 'Cqui')\n\nMDR.data &lt;- read.csv('./mosquito_traits/TraitData_MDR.csv')\nCpip.MDR.data &lt;- subset(MDR.data, MDR.data$host.code == 'Cpip')\nCqui.MDR.data &lt;- subset(MDR.data, MDR.data$host.code == 'Cqui')\n\n\n# FlexTPC model for thermal performance curves.\n# (parametrized with alpha/beta)\nflexTPC &lt;- function(temp, Tmin, Tmax, rmax, alpha, beta) {\n  s &lt;- alpha * (1 - alpha) / beta^2\n  result &lt;- rep(0, length(temp))\n  Tidx = (temp &gt; Tmin) & (temp &lt; Tmax)\n  result[Tidx] &lt;- rmax * exp(s * (alpha * log( (temp[Tidx] - Tmin) / alpha) \n                  + (1 - alpha) * log( (Tmax - temp[Tidx]) / (1 - alpha)) \n                  - log(Tmax - Tmin)) ) \n  return(result)\n}\n\n# Returns a function that calculates the squared error between data and a flexTPC curve.\nget_sq_err_fn &lt;- function(temp, y, lower.bounds, upper.bounds) {\n  # temp: Vector of measured temperature values.\n  # y: Vector of measured performance values.\n  # lower_bounds, upper_bounds: Lower and upper bounds for the parameters.\n  \n  f &lt;- function(par) {\n    # Return infinity (worst possible error) if parameters are outside of upper and lower bounds.\n    if((sum(par &lt; lower.bounds) + sum(par &gt; upper.bounds)) &gt; 0) {\n      return(Inf)    \n    } else {\n      # Otherwise return squared difference between data and flexTPC curve.\n      return(sum((y - flexTPC(temp, par[1], par[2], par[3], par[4], par[5]))^2))\n  }\n  }\n  return(f) \n}\n\nWe need to choose the starting parameters for the flexTPC model. The intuitive parameters of the model make it simple to choose reasonable starting values from the plot. Let’s pick some initial values and plot the data and the curves. We will plot the curves in gray to emphasize that these are not the fitted curves, but the initial values.\n\ntemp &lt;- seq(5, 45, 0.1)\n\npar(mfrow=c(1,2))\nplot(Cpip.MDR.data$T, 1 / Cpip.MDR.data$trait, xlim=c(5, 40), ylim=c(0, 0.2),\n     xlab='Temperature [°C]', ylab='Dev. rate [1 / day]', main='Cx pipiens',\n     pch=20, col='steelblue')\nlines(temp, flexTPC(temp, Tmin=0, Tmax=40, rmax=0.15, alpha=0.8, beta=0.2),\n     xlab='Temperature [°C]', ylab='Trait performance', col='gray', lwd=2)\n\nplot(Cqui.MDR.data$T, 1 / Cqui.MDR.data$trait, xlim=c(5, 40), ylim=c(0, 0.2),\n     xlab='Temperature [°C]', ylab='Dev. rate [1 / day]', main='Cx quinquefasciatus',\n     pch=20, col='firebrick')\nlines(temp, flexTPC(temp, Tmin=0, Tmax=40, rmax=0.15, alpha=0.8, beta=0.2),\n     xlab='Temperature [°C]', ylab='Trait performance', col='gray', lwd=2)\n\n\n\n\n\n\n\n\nLet’s now fit the curves.\n\nset.seed(42)\n\nfit.df.Cpip &lt;- data.frame(temp=Cpip.MDR.data$T, y=1 / Cpip.MDR.data$trait)\nfit.df.Cqui &lt;- data.frame(temp=Cqui.MDR.data$T, y=1 / Cqui.MDR.data$trait)\n\n# Cx. pipiens data.\ninit.par &lt;- c(0, 40, 0.15, 0.8, 0.2)\nlower.bounds &lt;- c(0, 35, 0.001, 0.001, 0.001)\nupper.bounds &lt;- c(10, 50, 1.0, 0.999, 1.0)\ntemp &lt;- Cpip.MDR.data$T\ny &lt;- 1 / Cpip.MDR.data$trait\n\nf &lt;- get_sq_err_fn(temp, y, lower.bounds, upper.bounds)\nCpip.fit &lt;- optim(init.par, f, control=list(maxit=100000), hessian=TRUE)\n\n# Cx. quinquefasciatus data.\ninit.par &lt;- c(0, 40, 0.15, 0.8, 0.2)\nlower.bounds &lt;- c(0, 35, 0.001, 0.001, 0.001)\nupper.bounds &lt;- c(10, 50, 1.0, 0.999, 1.0)\ntemp &lt;- Cqui.MDR.data$T\ny &lt;- 1 / Cqui.MDR.data$trait\n\nf &lt;- get_sq_err_fn(temp, y, lower.bounds, upper.bounds)\nCqui.fit &lt;- optim(init.par, f, control=list(maxit=100000), hessian=TRUE)\n\n\nCpip.fit$par\n\n[1]  3.2217787 40.0168361  0.0956810  0.7217708  0.2782219\n\nCqui.fit$par\n\n[1]  8.8721689 38.3061771  0.1052981  0.7401064  0.3852558\n\n\nLet’s plot the data with the fitted curves.\n\ntemp &lt;- seq(0, 45, 0.1)\n\npar(mfrow=c(1,2))\nplot(Cpip.MDR.data$T, 1 / Cpip.MDR.data$trait, xlim=c(0, 40), ylim=c(0, 0.2),\n     xlab='Temperature [°C]', ylab='Dev. rate [1 / day]', main='Cx pipiens',\n     pch=20, col='steelblue')\nlines(temp, flexTPC(temp, Tmin=0, Tmax=40, rmax=0.15, alpha=0.8, beta=0.2),\n     xlab='Temperature [°C]', ylab='Trait performance', col='gray', lwd=2)\nlines(temp, flexTPC(temp, Tmin=Cpip.fit$par[1], Tmax=Cpip.fit$par[2], rmax=Cpip.fit$par[3], alpha=Cpip.fit$par[4], beta=Cpip.fit$par[5]),\n     xlab='Temperature [°C]', ylab='Trait performance', col='steelblue', lwd=2)\n\nplot(Cqui.MDR.data$T, 1 / Cqui.MDR.data$trait, xlim=c(0, 40), ylim=c(0, 0.2),\n     xlab='Temperature [°C]', ylab='Dev. rate [1 / day]', main='Cx quinquefasciatus',\n     pch=20, col='firebrick')\nlines(temp, flexTPC(temp, Tmin=0, Tmax=40, rmax=0.15, alpha=0.8, beta=0.2),\n     xlab='Temperature [°C]', ylab='Trait performance', col='gray', lwd=2)\nlines(temp, flexTPC(temp, Tmin=Cqui.fit$par[1], Tmax=Cqui.fit$par[2], rmax=Cqui.fit$par[3], alpha=Cqui.fit$par[4], beta=Cqui.fit$par[5]),\n     xlab='Temperature [°C]', ylab='Trait performance', col='firebrick', lwd=2)\n\n\n\n\n\n\n\n\nWe can also construct confidence intervals for the parameters by bootstrapping.\n\n## Performs a nonparametric boostrap to get confidence intervals.\nset.seed(42)\n# Number of bootstrap samples\nN &lt;- 1000\n\n# Matrix to store bootstrap results\nCpip.bts_samples &lt;- matrix(nrow=N, ncol=7)\n\ntemp &lt;- Cpip.MDR.data$T \ny &lt;- 1 / Cpip.MDR.data$trait\nn.obs &lt;- length(temp)\n\ninit.par &lt;- c(0, 40, 0.15, 0.8, 0.2)\nlower.bounds &lt;- c(0, 35, 0.001, 0.001, 0.001)\nupper.bounds &lt;- c(10, 50, 1.0, 0.999, 1.0)\n\nfor(i in 1:N) {\n    # Sample observations with replacement\n    sample.idx &lt;- sample(1:n.obs, n.obs, replace=TRUE)\n    \n    f &lt;- get_sq_err_fn(temp[sample.idx], y[sample.idx], lower.bounds, upper.bounds)\n    # Store current sampled observations in dataframe for fitting.\n    sample.fit &lt;- optim(init.par, f, control=list(maxit=100000))\n\n    # Store bootstrap samples.\n    Cpip.bts_samples[i, 1:5] &lt;- sample.fit$par\n}\n\n\ncolnames(Cpip.bts_samples) &lt;-  c('Tmin', 'Tmax', 'rmax', 'alpha', 'beta',\n'Topt', 'B')\n\nCpip.bts_samples[, 'Topt'] &lt;- (Cpip.bts_samples[, 'alpha'] * Cpip.bts_samples[, 'Tmax'] \n                       + (1 - Cpip.bts_samples[, 'alpha']) * Cpip.bts_samples[, 'Tmin'])\nCpip.bts_samples[, 'B'] &lt;- Cpip.bts_samples[, 'beta'] * (Cpip.bts_samples[, 'Tmax'] - Cpip.bts_samples[, 'Tmin'])\n\n\n# Calculate Topt and B.\n \nCpip.ci &lt;- apply(Cpip.bts_samples, 2, quantile, c(0.025, 0.975))\nCpip.ci\n\n              Tmin     Tmax       rmax     alpha      beta     Topt        B\n2.5%  2.408653e-07 37.01149 0.08654817 0.5702977 0.2035559 27.57860  8.21782\n97.5% 7.803201e+00 49.26113 0.10800769 0.9026549 0.3605150 33.56464 11.69802\n\n\n\n## Performs a nonparametric boostrap to get confidence intervals.\nset.seed(42)\n\n# Number of bootstrap samples\nN &lt;- 1000\n\n# Matrix to store bootstrap results\nCqui.bts_samples &lt;- matrix(nrow=N, ncol=7)\n\ntemp &lt;- Cqui.MDR.data$T \ny &lt;- 1 / Cqui.MDR.data$trait\nn.obs &lt;- length(temp)\n\ninit.par &lt;- c(0, 40, 0.15, 0.8, 0.2)\nlower.bounds &lt;- c(0, 35, 0.001, 0.001, 0.001)\nupper.bounds &lt;- c(10, 50, 1.0, 0.999, 1.0)\n\nfor(i in 1:N) {\n    # Sample observations with replacement\n    sample.idx &lt;- sample(1:n.obs, n.obs, replace=TRUE)\n    \n    f &lt;- get_sq_err_fn(temp[sample.idx], y[sample.idx], lower.bounds, upper.bounds)\n    # Store current sampled observations in dataframe for fitting.\n    sample.fit &lt;- optim(init.par, f, control=list(maxit=100000))\n\n    # Store bootstrap samples.\n    Cqui.bts_samples[i, 1:5] &lt;- sample.fit$par\n}\n\n\ncolnames(Cqui.bts_samples) &lt;-  c('Tmin', 'Tmax', 'rmax', 'alpha', 'beta',\n'Topt', 'B')\n\nCqui.bts_samples[, 'Topt'] &lt;- (Cqui.bts_samples[, 'alpha'] * Cqui.bts_samples[, 'Tmax'] \n                       + (1 - Cqui.bts_samples[, 'alpha']) * Cqui.bts_samples[, 'Tmin'])\nCqui.bts_samples[, 'B'] &lt;- Cqui.bts_samples[, 'beta'] * (Cqui.bts_samples[, 'Tmax'] - Cqui.bts_samples[, 'Tmin'])\n\n\n# Calculate Topt and B.\n \nCqui.ci &lt;- apply(Cqui.bts_samples, 2, quantile, c(0.025, 0.975))\nCqui.ci\n\n              Tmin Tmax       rmax     alpha      beta     Topt        B\n2.5%  7.236327e-08   35 0.09263496 0.5152757 0.1140054 26.69007  3.89936\n97.5% 9.991245e+00   50 0.13769996 0.9847668 0.4644770 37.40110 14.78753"
  },
  {
    "objectID": "bayesian_estimation.html",
    "href": "bayesian_estimation.html",
    "title": "Bayesian inference",
    "section": "",
    "text": "# FlexTPC model for thermal performance curves.\n# (parametrized with alpha/beta)\nflexTPC &lt;- function(temp, Tmin, Tmax, rmax, alpha, beta) {\n  s &lt;- alpha * (1 - alpha) / beta^2\n  result &lt;- rep(0, length(temp))\n  Tidx = (temp &gt; Tmin) & (temp &lt; Tmax)\n  result[Tidx] &lt;- rmax * exp(s * (alpha * log( (temp[Tidx] - Tmin) / alpha) \n                  + (1 - alpha) * log( (Tmax - temp[Tidx]) / (1 - alpha)) \n                  - log(Tmax - Tmin)) ) \n  return(result)\n}\nIn this example, we will fit data on the lifespan of Culex tarsalis mosquitoes with Bayesian methods. Bayesian approaches make it possible to use external information in the form of prior distributions to inform parameter estimates.\nAs before, let’s start by plotting the data.\nlf.data &lt;- read.csv('./mosquito_traits/TraitData_lf.csv')\n\n# Let's filter only the data for female mosquitoes.\nCtar.lf.data &lt;- subset(lf.data, (lf.data$host.code == 'Ctar') &\n                       (lf.data$Trait == 'Female'))\n\nplot(Ctar.lf.data$T, Ctar.lf.data$trait, xlim=c(0, 40), ylim=c(0, 80), xlab='Temperature [°C]',\n     ylab='Lifespan [days]', pch=20, col='steelblue')\nWe can see a few things from this plot:"
  },
  {
    "objectID": "bayesian_estimation.html#extending-a-deterministic-tpc-model-to-a-statistical-model",
    "href": "bayesian_estimation.html#extending-a-deterministic-tpc-model-to-a-statistical-model",
    "title": "Bayesian inference",
    "section": "Extending a deterministic TPC model to a statistical model",
    "text": "Extending a deterministic TPC model to a statistical model\nWe can deal with these issues by explicitly modeling how the data points are generated by our model.\n\\[y_{i}|T_{i}   \\sim\\begin{cases}\n\\mathrm{Gamma}(\\mu=r(T_{i};\\mathcal{P}),\\sigma) & T_{\\min}&lt;T_{i}&lt;T_{\\max}\\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nWith the following model for the standard deviation.\n\\[\\log\\sigma    =\\eta_0 + \\eta_1 r(T_{i};\\mathcal{P}) \\] We can then estimate the parameters of our model through maximum likelihood estimation.\nThe likelihood of a statistical model is a function that describes how compatible the model parameters are with the data. It is defined as the probability of observing the data given some specific fixed values of the model parameters (in our case, these correspond to \\({T_{\\min}, T_{\\max}, r_{\\max}, \\alpha, \\beta, \\eta_{0}, \\eta_{1}}\\)).\nThe maximum likelihood estimate (MLE) is the parameter values that maximize the likelihood. Rather than maximizing the likelihood directly, this is usually coded as minimizing the negative log likelihood, which is equivalent but tends to be more numerically stable.\n\n# FlexTPC model for thermal performance curves.\n# (parametrized with alpha/beta)\nflexTPC &lt;- function(temp, Tmin, Tmax, rmax, alpha, beta) {\n  s &lt;- alpha * (1 - alpha) / beta^2\n  result &lt;- rep(0, length(temp))\n  Tidx = (temp &gt; Tmin) & (temp &lt; Tmax)\n  result[Tidx] &lt;- rmax * exp(s * (alpha * log( (temp[Tidx] - Tmin) / alpha) \n                  + (1 - alpha) * log( (Tmax - temp[Tidx]) / (1 - alpha)) \n                  - log(Tmax - Tmin)) ) \n  return(result)\n}\n\n# Returns a function that calculates the negative log likelihood.\nget_negloglk_fn &lt;- function(temp, y, lower.bounds, upper.bounds) {\n  # temp: Vector of measured temperature values.\n  # y: Vector of measured performance values.\n  # lower_bounds, upper_bounds: Lower and upper bounds for the parameters.\n  \n  f &lt;- function(par) {\n    # Return infinity (worst possible error) if parameters are outside of upper and lower bounds.\n    if((sum(par &lt; lower.bounds) + sum(par &gt; upper.bounds)) &gt; 0) {\n      return(Inf)    \n    } else {\n      \n      Tmin &lt;- par[1]\n      Tmax &lt;- par[2]\n      rmax &lt;- par[3]\n      alpha &lt;- par[4]\n      beta &lt;- par[5]\n      eta0 &lt;- par[6]\n      eta1 &lt;- par[7]\n      \n      # Return negative log-likelihood.\n      mu = pmax(flexTPC(temp, Tmin, Tmax, rmax, alpha, beta), 1e-20)\n      sigma = exp(eta0 + eta1 * mu / 50)\n      \n      # Convert mean and standard deviation to shape and rate parameters for\n      # Gamma distribution.\n      a = mu^2 / sigma^2\n      b = mu / sigma^2\n      \n      return(sum(-dgamma(y, a, b, log=TRUE)))\n  }\n  }\n  return(f) \n}\n\n# Returns a function that calculates the negative log likelihood.\nget_negloglk_fn &lt;- function(temp, y, lower.bounds, upper.bounds) {\n  # temp: Vector of measured temperature values.\n  # y: Vector of measured performance values.\n  # lower_bounds, upper_bounds: Lower and upper bounds for the parameters.\n  \n  f &lt;- function(par) {\n    # Return infinity (worst possible error) if parameters are outside of upper and lower bounds.\n    if((sum(par &lt; lower.bounds) + sum(par &gt; upper.bounds)) &gt; 0) {\n      return(Inf)    \n    } else {\n      \n      # Rename parameters for readability of code.\n      Tmin &lt;- par[1]\n      Tmax &lt;- par[2]\n      rmax &lt;- par[3]\n      alpha &lt;- par[4]\n      beta &lt;- par[5]\n      eta0 &lt;- par[6]\n      eta1 &lt;- par[7]\n      \n      T.idx &lt;- (temp &gt; Tmin) & (temp &lt; Tmax)\n      \n      # Return infinity if any measurements outside the (Tmin, Tmax)\n      # temperature range are nonzero.\n      if(sum(temp[!T.idx] != 0) &gt; 0) {\n        return(Inf)\n      }\n      \n      # Return negative log-likelihood.\n      mu = pmax(flexTPC(temp[T.idx], Tmin, Tmax, rmax, alpha, beta), 1e-20)\n      sigma = exp(eta0 + eta1 * mu / 50)\n      \n      # Convert mean and standard deviation to shape and rate parameters for\n      # Gamma distribution.\n      a = mu^2 / sigma^2\n      b = mu / sigma^2\n      \n      return(sum(-dgamma(y, a, b, log=TRUE)))\n  }\n  }\n  return(f) \n}\n\nLet’s now fit the curves.\n\n# Cx. pipiens data.\ninit.par &lt;- c(5, 37, 80, 0.3, 0.3, 2, 1)\nlower.bounds &lt;- c(0, 30, 0, 0.1, 0.1, -10, -10)\nupper.bounds &lt;- c(10, 45, 200, 0.9, 0.5, 10, 10)\ntemp &lt;- Cpip.lf.data$T\ny &lt;- Cpip.lf.data$trait\n\nf &lt;- get_negloglk_fn(temp, y, lower.bounds, upper.bounds)\nCpip.fit &lt;- optim(init.par, f, control=list(maxit=10000))\n\n\n# Cx. quinquefasciatus data.\ninit.par &lt;-     c(5,  37,  60, 0.3, 0.3,     2,   1)\nlower.bounds &lt;- c(0,  30,   0, 0.1, 0.1, -10, -10)\nupper.bounds &lt;- c(10, 45, 200, 0.9, 0.5,    10,  10)\ntemp &lt;- Cqui.lf.data$T\ny &lt;- Cqui.lf.data$trait\n\nf &lt;- get_negloglk_fn(temp, y, lower.bounds, upper.bounds)\nCqui.fit &lt;- optim(init.par, f, control=list(maxit=10000))\n\n\nCpip.fit$par\n\n[1]  0.2593252 44.9437612 85.8153498  0.2661140  0.2125402  1.9079440  0.9729116\n\nCqui.fit$par\n\n[1] 9.003182e-07 3.420246e+01 7.074355e+01 3.615743e-01 3.682643e-01\n[6] 1.224621e+00 1.319640e+00\n\n\n\ntemp &lt;- seq(0, 45, 0.1)\n\n\npar(mfrow=c(1,2))\nplot(Cpip.lf.data$T, Cpip.lf.data$trait, xlim=c(0, 40), ylim=c(0, 150),\n     xlab='Temperature [°C]', ylab='Lifespan [days]', main='Cx pipiens',\n     pch=20, col='steelblue')\nlines(temp, flexTPC(temp, Tmin=5, Tmax=37, rmax=80, alpha=0.3, beta=0.3),\n     col='gray', lwd=2)\nlines(temp, flexTPC(temp, Tmin=Cpip.fit$par[1], Tmax=Cpip.fit$par[2], rmax=Cpip.fit$par[3], alpha=Cpip.fit$par[4], beta=Cpip.fit$par[5]),\n     col='steelblue', lwd=2)\n\nplot(Cqui.lf.data$T, Cqui.lf.data$trait, xlim=c(0, 40), ylim=c(0, 150),\n     xlab='Temperature [°C]', ylab='Lifespan [days]', main='Cx quinquefasciatus',\n     pch=20, col='firebrick')\nlines(temp, flexTPC(temp, Tmin=5, Tmax=37, rmax=60, alpha=0.3, beta=0.3),\n    col='gray', lwd=2)\nlines(temp, flexTPC(temp, Tmin=Cqui.fit$par[1], Tmax=Cqui.fit$par[2], rmax=Cqui.fit$par[3], alpha=Cqui.fit$par[4], beta=Cqui.fit$par[5]),\n     col='firebrick', lwd=2)"
  },
  {
    "objectID": "bayesian_estimation.html#general-guidance-for-setting-priors",
    "href": "bayesian_estimation.html#general-guidance-for-setting-priors",
    "title": "Bayesian inference",
    "section": "General guidance for setting priors",
    "text": "General guidance for setting priors\nPrior distributions can vary in their informativeness, depending on what the desired effect of the prior choices in the analysis.\n\n\\(T_{\\min}\\) and \\(T_{\\max}\\)\nFor the minimum and maximum temperatures, we suggest the use of normal priors."
  },
  {
    "objectID": "bayesian_estimation.html#t_min-and-t_max",
    "href": "bayesian_estimation.html#t_min-and-t_max",
    "title": "Bayesian inference",
    "section": "\\(T_{\\min}\\) and \\(T_{\\max}\\)",
    "text": "\\(T_{\\min}\\) and \\(T_{\\max}\\)\nFor the minimum and maximum temperatures, we suggest the use of normal priors."
  },
  {
    "objectID": "bayesian_estimation.html#general-guidance-for-setting-prior-distributions",
    "href": "bayesian_estimation.html#general-guidance-for-setting-prior-distributions",
    "title": "Bayesian inference",
    "section": "General guidance for setting prior distributions",
    "text": "General guidance for setting prior distributions\nPrior distributions can vary in their informativeness, depending on what the desired effect of the prior choices in the analysis.\n\nOn one extreme, priors can be weakly informative, only constraining the parameters slightly (e.g. trait performance needs to be non-negative, or within a certain order of magnitude that is biologically relevant) in order to let the data do the heavy lifting.\nOn the other extreme, priors can be strongly informative, having a strong influence in the inferred parameters. This can be desirable when there is a good source of information about the parameters independent of the data (e.g. previous experiments in related species or information about the species habitat), especially when data is limited.\n\n\n\\(T_{\\min}\\) and \\(T_{\\max}\\)\nFor the minimum and maximum temperatures, we suggest the use of normal priors."
  },
  {
    "objectID": "bayesian_estimation.html#general-guidance-for-choosing-prior-distributions",
    "href": "bayesian_estimation.html#general-guidance-for-choosing-prior-distributions",
    "title": "Bayesian inference",
    "section": "General guidance for choosing prior distributions",
    "text": "General guidance for choosing prior distributions\nA prior distribution represents our initial state of uncertainty about the value of the model parameters. Prior distributions can vary in their informativeness, depending the desired effect of the prior choices in the analysis.\n\nOn one extreme, priors can be very weakly informative, only constraining the parameters slightly (e.g. trait performance needs to be non-negative, or within a certain order of magnitude that is biologically relevant). This kind of prior is used in order to minimize its effect on parameter inference and let the data do the heavy lifting.\nOn the other extreme, priors can be strongly informative, having a large influence in the inferred parameters. This can be desirable when there is a good source of information about the parameters independent of the data (e.g. previous experiments in related species or information about the species habitat), especially when data is limited.\nSometimes, strong priors can also be used as a form of regularization, preferring simple models over complex models unless the complexity is necessary. As the quadratic model is a special case of flexTPC, priors for \\(\\alpha\\) and \\(\\beta\\) can be chosen that pull the parameter estimates closer to those of the quadratic model.\n\nIn general, we prefer the use of informative priors when good sources for this information are available from previous experiments in related species, the species habitats or biological/physical constraints. However, it is considered good practice to make priors less informative than the researcher’s true beliefs about the parameter to account for the possibility of being wrong, as well as for any different experimental conditions being different from the previous sources of information.\nOne important consideration is that prior distributions should be chosen from information independent from the dataset that will be used for fitting. Not doing this violates the assumptions of Bayesian statistics by using the data twice, which will lead to overconfident predictions.\n\nThe upper and lower thermal limits \\(T_{\\min}\\) and \\(T_{\\max}\\)\nWe suggest the use of normal priors for the minimum and maximum temperatures, which do not introduce a hard boundary to the allowed temperatures. The location can be set with the mean of the distribution and the standard deviation determines how informative the prior is (with larger standard deviations corresponding to less informative priors).\nFor example, a reasonable prior for the minimum temperature for lifespan could be:\n\\[T_{\\min} \\sim \\mathrm{Normal}(\\mu=\\text{5°C}, \\sigma=\\text{2.5°C})\\]\nOne way to conceptualize the assumption that is made by a prior distribution is to consider a 95% credible interval assumed by the prior. This is an interval that our prior assumes the true value of the parameter is with 95% probability. With a normal distribution, an approximate prior 95% CI is given by the interval \\([\\mu - 2 \\sigma, \\mu + 2 \\sigma]\\) (which would be \\([\\text{0°C},\\text{10°C}]\\) in our example). This prior is thus assuming that \\(T_\\min\\) is 95% likely to be in this interval, although it still gives 5% probability that it is outside this interval.\nWe can follow a similar approach to construct a prior for the maximum temperature. If we want a 95% prior credible interval for the maximum temperature to be \\([\\text{25°C}, \\text{45°C}]\\), we can set the following prior:\n\\[T_{\\max} \\sim \\mathrm{Normal}(\\mu=\\text{35°C}, \\sigma=\\text{5°C})\\] which ensures the prior 95% CI is what we intend.\n\n\nThe peak trait value \\(r_{\\max}\\)\nWe typically deal with traits that are strictly non-negative. Because of this, we need to choose a prior distribution for the peak trait value \\(r_{\\max}\\) that has support over the non-negative numbers. Sometimes traits have a true “hard” upper limit (for example, the proportion of larvae surviving to adulthood have to be between zero and one) and sometimes they are unbounded. The prior choice should reflect these properties: some recommendations are given below.\nIn general, thermal performance data is available near the temperatures of optimum performance. This usually means that there is good information for \\(r_\\max\\) in the data and it is not usually necessary to have a strong prior for this parameter.\nOne simple choice is a uniform prior on \\(r_{\\max}\\) where the upper limit is chosen to be higher than any reasonable value for the trait (but not orders of magnitude higher). For example, we could choose\n\\[r_\\max \\sim \\mathrm{Uniform}(0, 200)\\] This prior assumes that any value of the maximum lifespan between 0 and 200 is a priori equally likely, with any value outside this range being impossible.\nAlternatively, we could have a weakly informative prior that does not have a “hard” upper bound. One choice can be an exponential prior, with the mean chosen to one guess for a reasonable parameter value. For example, we could choose\n\\[r_\\max \\sim \\mathrm{Exponential}(\\mu=100)\\] with a 95% CI of\n\nqexp(c(0.025, 0.975), rate=1/100)\n\n[1]   2.531781 368.887945\n\n\n\n\nThe relative thermal optimum \\(\\alpha\\)\nFlexTPC has two parameters that determine the shape of the curve. Parameter \\(\\alpha \\in (0,1)\\) determines the relative position for the optimum relative to the minimum and maximum. A noninformative prior for \\(\\alpha\\) can be\n\\[\\alpha \\sim \\mathrm{Uniform}(0, 1)\\] We may often want to avoid fitting extremely skewed curves (which are rare), especially in the absence of data to accurately determine the skew. To do this, we may want to use a prior that shrinks \\(\\alpha\\) towards 0.5 (but that allows more skewed curves if required to describe the data). To do this, the prior\n\\[ \\alpha \\sim \\mathrm{Beta}(2,2)\\] can be employed, which has a prior mean of 0.5, and a 95% prior CI of\n\nround(qbeta(c(0.025, 0.975), 2, 2), 3)\n\n[1] 0.094 0.906\n\n\nIn applications with prior knowledge from other TPCs of the same trait in similar organisms, an informative prior can be used that is centered over a different value of \\(\\alpha\\) (to prefer left or right skewed curves).\n\n\nThe thermal breadth / tolerance ratio \\(\\beta\\)\nParameter \\(\\beta\\) can be interpreted as the approximate ration between the thermal breadth (range of temperatures for which \\(r(T)&gt;e^{-1/8}r_\\max \\approx 0.88r_\\max\\)) and thermal tolerance range (\\(T_\\max\\) - \\(T_\\min\\)). The quadratic model is a special case of flexTPC when \\(\\alpha=0.5\\) and \\(\\beta=1/\\sqrt8\\approx0.35\\) with most TPCs being well-described by \\(\\beta \\in [0.2, 0.5]\\). Because of this, we recommend a prior that is centered around this value for \\(\\beta\\). A very weak prior can be given by\n\\[\\beta \\sim \\mathrm{Gamma}(\\mu=0.35, \\sigma=0.4)\\] which has a prior 95% credible interval of\n\nround(qgamma(c(0.025, 0.975), shape=0.35^2/0.4^2, rate=0.35/0.4^2), 3)\n\n[1] 0.003 1.451\n\n\nand a more informative prior by\n\\[\\beta \\sim \\mathrm{Gamma}(\\mu=0.35, \\sigma=0.1)\\]\n\nround(qgamma(c(0.025, 0.975), shape=0.35^2/0.1^2, rate=0.35/0.1^2), 3)\n\n[1] 0.182 0.572\n\n\nWe will fit flexTPC models with two sets of priors for \\(r_\\max\\), \\(\\alpha\\) and \\(\\beta\\): a weakly informative set of priors and a moderately informative set of priors.\n\npar(mfrow=c(2,5))\n\n# Tmin\ntemps &lt;- seq(-5, 15, 0.1)\nplot(temps, dnorm(temps, mean=5, sd=2.5), type='l', main='Tmin',\n     xlab='temperature [°C]', ylab='prob. density', lwd=2)\n\n# Tmax\ntemps &lt;- seq(20, 50, 0.1)\nplot(temps, dnorm(temps, mean=35, sd=5), type='l', main='Tmax',\n     xlab='temperature [°C]', ylab='prob. density', lwd=2)\n\n# rmax\nrmax &lt;- seq(0, 150, 0.1)\nplot(rmax, dunif(rmax, 0, 200), type='l', main='rmax',\n     xlab='lifespan [days]', ylab='prob. density', lwd=2)\n\n# alpha\nalpha &lt;-seq(0, 1, 0.01)\nplot(alpha, dunif(alpha, 0, 1), type='l', main='alpha',\n     xlab='alpha [unitless]', ylab='prob. density', lwd=2)\n\n# beta\nbeta &lt;-seq(0, 1, 0.01)\nplot(beta, dgamma(beta, shape=(1/sqrt(8))^2/0.4^2, rate=0.35/0.4^2), type='l', main='beta',\n     xlab='beta [unitless]', ylab='prob. density', lwd=2)\n\n\n# Tmin\ntemps &lt;- seq(-5, 15, 0.1)\nplot(temps, dnorm(temps, mean=5, sd=2.5), type='l', main='Tmin',\n     xlab='temperature [°C]', ylab='prob. density', lwd=2)\n\n# Tmax\ntemps &lt;- seq(20, 50, 0.1)\nplot(temps, dnorm(temps, mean=35, sd=5), type='l', main='Tmax',\n     xlab='temperature [°C]', ylab='prob. density', lwd=2)\n\n# rmax\nrmax &lt;- seq(0, 150, 0.1)\nplot(rmax, dexp(rmax, 1/100), type='l', main='rmax',\n     xlab='lifespan [days]', ylab='prob. density', lwd=2)\n\n# alpha\nalpha &lt;-seq(0, 1, 0.01)\nplot(alpha, dbeta(alpha, 2, 2), type='l', main='alpha',\n     xlab='alpha [unitless]', ylab='prob. density', lwd=2)\n\n# beta\nbeta &lt;-seq(0, 1, 0.01)\nplot(beta, dgamma(beta, shape=0.35^2/0.1^2, rate=(1/sqrt(8))/0.1^2), type='l', main='beta',\n     xlab='beta [unitless]', ylab='prob. density', lwd=2)"
  },
  {
    "objectID": "least_squares_v2.html",
    "href": "least_squares_v2.html",
    "title": "Least squares estimation",
    "section": "",
    "text": "A simple approach to estimate TPCs is to perform nonlinear least squares estimation. This finds the curve that minimizes the squared difference between the TPC and the data points.\nIn this example, we will fit a thermal performance curve to data of the radial growth rates of the fungus Metarhizium anisopliae. This data is originally from Ouedraogo et al 1997 and digitized by Kontopoulos et al 2024.\nLet’s start by reading in the data and plotting it.\ndata &lt;- read.csv('TPC_data_examples.csv') # From Kontopoulos et al.\ndata &lt;- subset(data, data$id == 39697) # Subset Ouedraogo et al dataset.\n\nplot(data$temperature, data$trait_value, xlim=c(0, 40),\n     xlab='Temperature [°C]', ylab='radial growth rate [m / s]',\n     pch=20, col='steelblue')\nIn order to fit flexTPC to this data, we will need to code the model equation and an error function that calculates the difference between a curve and the data points.\n# FlexTPC model for thermal performance curves.\n# (parametrized with alpha/beta)\nflexTPC &lt;- function(temp, Tmin, Tmax, rmax, alpha, beta) {\n  s &lt;- alpha * (1 - alpha) / beta^2\n  result &lt;- rep(0, length(temp))\n  Tidx = (temp &gt; Tmin) & (temp &lt; Tmax)\n  result[Tidx] &lt;- rmax * exp(s * (alpha * log( (temp[Tidx] - Tmin) / alpha) \n                  + (1 - alpha) * log( (Tmax - temp[Tidx]) / (1 - alpha)) \n                  - log(Tmax - Tmin)) ) \n  return(result)\n}\n\n# Returns a function that calculates the squared error between data and a flexTPC curve.\nget_sq_err_fn &lt;- function(temp, y, lower.bounds, upper.bounds) {\n  # temp: Vector of measured temperature values.\n  # y: Vector of measured performance values.\n  # lower_bounds, upper_bounds: Lower and upper bounds for the parameters.\n  \n  f &lt;- function(par) {\n    # Return infinity (worst possible error) if parameters are outside of chosen\n    # upper and lower bounds.\n    if((sum(par &lt; lower.bounds) + sum(par &gt; upper.bounds)) &gt; 0) {\n      return(Inf)    \n    } else {\n      # Otherwise return squared difference between data and flexTPC curve.\n      return(sum((y - flexTPC(temp, par[1], par[2], par[3], par[4], par[5]))^2))\n  }\n  }\n  return(f) \n}\nWe also need an initial guess for the parameters for the flexTPC model. We will input these initial parameters and the error function to minimize into the function in R. The intuitive parameters of the flexTPC model make it simple to choose reasonable starting parameter values from the plot of the data.\nLet’s pick some initial values and plot the data and the flexTPC curve with the initial parameters (picked visually).\ntemp &lt;- seq(0, 45, 0.1)\n\nplot(data$temperature, data$trait_value, xlim=c(0, 40), \n     xlab='Temperature [°C]', ylab='trait',\n     pch=20, col='steelblue')\nlines(temp, flexTPC(temp, Tmin=12, Tmax=36, rmax=3e-8, alpha=0.6,\n                    beta=0.3),\n     col='gray', lwd=2)\nWe will now find the parameters that minimize the squared error to our data. Before doing so, it is convenient to change the units of the data to help the optimization algorithm (as it sometimes has trouble if the parameters to optimize are of very different orders of magnitude). Let’s do this and fit the curves.\ntemp &lt;- data$temperature\ny &lt;- data$trait_value * 10^8 # Convert units of the trait value data so they are close to unity.\n\ninit.par &lt;- c(12, 36, 3, 0.6, 0.3)\nnames(init.par) &lt;- c(\"Tmin\", \"Tmax\", \"rmax\", \"alpha\", \"beta\")\n\n\n# Lower and upper bounds for the parameters.\nlower.bounds &lt;- c(0, 30, 0, 0.1, 0.1)\nupper.bounds &lt;- c(15, 45, 10, 0.9, 0.7)\n\n\nf &lt;- get_sq_err_fn(temp, y, lower.bounds, upper.bounds) # Get error function with the chosen bounds of the parameters.\nfit &lt;- optim(init.par, f, control=list(maxit=10000))\nfit$par\n\n      Tmin       Tmax       rmax      alpha       beta \n11.0733505 35.1772202  2.8645324  0.6543052  0.3870970\nLet’s plot the data along with the initial parameters (in gray) and the fitted curves (in blue).\ntemp &lt;- seq(0, 45, 0.1)\n\nplot(data$temperature, y, xlim=c(0, 40), #ylim=c(0, 150),\n     xlab='Temperature [°C]', ylab='radial growth rate [m / s] * 10^8',\n     pch=20, col='steelblue')\nlines(temp, flexTPC(temp, Tmin=12, Tmax=36, rmax=3, alpha=0.6,\n                    beta=0.3),\n     col='gray', lwd=2)\nlines(temp, flexTPC(temp, Tmin=fit$par[1], Tmax=fit$par[2], rmax=fit$par[3], alpha=fit$par[4], beta=fit$par[5]),\n     col='steelblue', lwd=2)"
  },
  {
    "objectID": "least_squares_v2.html#finding-the-thermal-breadth-at-other-reference-values",
    "href": "least_squares_v2.html#finding-the-thermal-breadth-at-other-reference-values",
    "title": "Least squares estimation",
    "section": "Finding the thermal breadth at other reference values",
    "text": "Finding the thermal breadth at other reference values\nIn many applications, researchers may be interested on the thermal breadth at a different reference performance value than 88%. We provide an algorithm to find the temperatures corresponding to any performance value from flexTPC numerically. This can be used to find the least square estimate of the thermal breadth at any desired performance value.\n\nf1 &lt;- function(tau, wref, alpha, s) {\n  logf1 &lt;- log(alpha) + (1 / (alpha * s) ) * log(wref) + ((1 - alpha) / alpha) * (log(1 - alpha) - log(1 - tau))\n  return(exp(logf1))\n}\n\nf2 &lt;- function(tau, wref, alpha, s) {\n  B &lt;- log(1 - alpha) + (1 / ((1 - alpha) * s) ) * log(wref) + (alpha / (1 - alpha)) * (log(alpha) - log(tau))\n  return(1 - exp(B))\n}\n\nflexTPC_nd_roots &lt;- function(wref, alpha, beta, tol=1e-6) {\n  s &lt;- alpha * (1 - alpha) / beta^2\n  tau1 &lt;- alpha / 2\n  tau2 &lt;- alpha + (1 - alpha) / 2\n  err &lt;- Inf\n  while(err &gt; tol) {\n    new_tau1 &lt;- f1(tau1, wref, alpha, s)\n    new_tau2 &lt;- f2(tau2, wref, alpha, s)\n    err &lt;- pmax(abs(tau1 - new_tau1), abs(tau2 - new_tau2))\n    tau1 &lt;- new_tau1\n    tau2 &lt;- new_tau2\n  }\n  return(c(tau1, tau2))\n}\n\n# wref: reference relative performance (e.g. 0.5 for half-max temperatures)\nflexTPC_roots &lt;- function(wref, Tmin, Tmax, alpha, beta, tol=1e-6) {\n  nd_roots &lt;- flexTPC_nd_roots(wref, alpha, beta, tol=tol)\n  return(Tmin + nd_roots*(Tmax - Tmin))\n}\n\nLet’s find the half-maximum temperatures and thermal breadth.\n\nhalfmax_temps &lt;- flexTPC_roots(0.5, # wref\n                               fit$par[1], # Tmin\n                               fit$par[2], # Tmax\n                               fit$par[4], # alpha\n                               fit$par[5]) # beta\nnames(halfmax_temps) &lt;- c('Thm1','Thm2')\n\nhalfmax_breadth &lt;- halfmax_temps[2] - halfmax_temps[1]\nnames(halfmax_breadth) &lt;- 'hm_breadth'\n\nhalfmax_temps\n\n    Thm1     Thm2 \n16.12346 34.09863 \n\nhalfmax_breadth\n\nhm_breadth \n  17.97517"
  },
  {
    "objectID": "least_squares_v2.html#confidence-intervals",
    "href": "least_squares_v2.html#confidence-intervals",
    "title": "Least squares estimation",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nNext, we construct confidence intervals for the parameters by bootstrapping. Here we follow a nonparametric boostrap approach that resamples the data with replacement.\n\n## Performs a nonparametric boostrap to get confidence intervals of model\n## parameters.\n\nset.seed(42) # Set seed for reproducibility\n\n# Number of bootstrap samples\n# Note: You can make this small for testing so the code runs faster, but should be\n# at least 10000 for real applications.\nN &lt;- 10000\n\n# Matrix to store bootstrap results\nbts_samples &lt;- matrix(nrow=N, ncol=9)\n\ntemp &lt;- data$temperature\n\n# Change scale of trait values to be close to unit scale to help optimization.\ny &lt;- data$trait_value * 10^8 \nn.obs &lt;- length(temp)\n\ninit.par &lt;- c(12, 36, 3, 0.6, 0.3)\nlower.bounds &lt;- c(0, 30, 0, 0.1, 0.1)\nupper.bounds &lt;- c(15, 45, 10, 0.9, 0.7)\n\nfor(i in 1:N) {\n    # Sample observations with replacement\n    sample.idx &lt;- sample(1:n.obs, n.obs, replace=TRUE)\n    \n    f &lt;- get_sq_err_fn(temp[sample.idx], y[sample.idx], lower.bounds, upper.bounds)\n    # Store current sampled observations in dataframe for fitting.\n    sample.fit &lt;- optim(init.par, f, control=list(maxit=100000))\n\n    # Store bootstrap samples.\n    bts_samples[i, 1:5] &lt;- sample.fit$par\n}\n\n\ncolnames(bts_samples) &lt;-  c('Tmin', 'Tmax', 'rmax', 'alpha', 'beta',\n'Topt', 'B', 'Thm1', 'Thm2')\n\n# Calculate Topt and B from equations.\nbts_samples[, 'Topt'] &lt;- (bts_samples[, 'alpha'] * bts_samples[, 'Tmax'] \n                       + (1 - bts_samples[, 'alpha']) * bts_samples[, 'Tmin'])\nbts_samples[, 'B'] &lt;- bts_samples[, 'beta'] * (bts_samples[, 'Tmax'] - bts_samples[, 'Tmin'])\n\n# Calculate half-max temperatures numerically for all bootstrap samples.\nhm_temps_samples &lt;- apply(bts_samples[, 1:5], 1, function(x) flexTPC_roots(0.5, x[['Tmin']], x[['Tmax']], x[['alpha']], x[['beta']], tol=1e-6))\n\n#apply(hm_temps_samples, 1, quantile, c(0.025, 0.975))\n\nbts_samples[, c('Thm1', 'Thm2')] &lt;- t(hm_temps_samples)\n\nci &lt;- apply(bts_samples, 2, quantile, c(0.025, 0.975))\nt(ci)\n\n            2.5%      97.5%\nTmin   7.7111399 14.9999999\nTmax  33.7659444 38.8288356\nrmax   2.7723317  3.1335018\nalpha  0.4545014  0.7479570\nbeta   0.2745437  0.6999994\nTopt  24.0493660 28.1469150\nB      7.0124164 14.0858612\nThm1  13.6793069 18.6691363\nThm2  32.8693103 35.5887098"
  },
  {
    "objectID": "mle.html",
    "href": "mle.html",
    "title": "Maximum likelihood estimation",
    "section": "",
    "text": "In this example, we’ll fit flexTPC to data on the population growth of the marine cyanobacteria Trichodesmium erythraeum with a maximum likelihood approach. This example shows how to deal with data where the variance varies with trait performance, and how to assume a different probability distribution than the normal distribution.\nLet’s plot the data!\ndata &lt;- read.csv('TPC_data_examples.csv')\ndata &lt;- subset(data, data$id == 43041)\n\nplot(data$temperature, data$trait_value, xlim=c(10, 40),\n     xlab='Temperature [°C]', ylab='growth rate',\n     pch=20, col='steelblue')"
  },
  {
    "objectID": "mle.html#extending-a-deterministic-tpc-model-to-a-statistical-model",
    "href": "mle.html#extending-a-deterministic-tpc-model-to-a-statistical-model",
    "title": "Maximum likelihood estimation",
    "section": "Extending a deterministic TPC model to a statistical model",
    "text": "Extending a deterministic TPC model to a statistical model\nWe can deal with these issues by explicitly modeling how the data points are generated by our model. As an example, we will assume that, when \\(T_{\\min}&lt;T&lt;T_{\\max}\\) a data point \\(y_i\\) will be Gamma distributed, with a mean given by the flexTPC model with parameters \\(\\mathcal{P}\\). We will further assume that performance will be strictly zero outside this interval. This leads to the following statistical model:\n\\[y_{i}|T_{i},\\mathcal{P},\\sigma   \\sim\\begin{cases}\n\\mathrm{Gamma}(\\mu=r(T_{i};\\mathcal{P}),\\sigma) & T_{\\min}&lt;T_{i}&lt;T_{\\max}\\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nHere, we use the notation \\(r(T_{i};\\mathcal{P}),\\sigma)\\) to represent the flexTPC model equation at temperature \\(T_i\\) with parameters \\(\\mathcal{P}=\\{T_{\\min}, T_{\\max},r_{\\max},\\alpha,\\beta\\}\\).\nWe could stop here and assume that the standard deviation \\(\\sigma\\) is constant. However, we can also account for variation in the standard deviation with trait performance. Many approaches are possible, but here we will do this with the following model:\n\\[\\ln\\sigma  = \\eta_0 + \\eta_1 r(T_{i};\\mathcal{P}) \\] This is a linear model for the natural logarithm of the standard deviation. The log-transform ensures that we always have a positive standard deviation, since\n\\[\\sigma = e^{\\eta_0 +\\eta_{1}r(T_i;\\mathcal{P})}\\].\nWe can now estimate the parameters of our model through maximum likelihood estimation. The likelihood of a statistical model is a function that describes how compatible the model parameters are with the data. It is defined as the probability of observing the data given some specific fixed values of the model parameters (in our case, these correspond to \\({T_{\\min}, T_{\\max}, r_{\\max}, \\alpha, \\beta, \\eta_{0}, \\eta_{1}}\\)). The maximum likelihood estimate (MLE) of the parameters consists of the parameter values that maximize the likelihood (or, equivalently, minimize the negative log likelihood, which tends to be more numerically stable).\nIn R, the Gamma distribution is parametrized in terms of a shape parameter \\(a\\) and a rate parameter \\(b\\) rather than mean and standard deviation. But it turns out to be simple to convert between these parametrizations since \\(a=\\mu^2/\\sigma^2\\) and \\(b=\\mu/\\sigma^2\\).\nLet’s code a function that calculates the negative log-likelihood according to our model.\n\n# Returns a function that calculates the negative log likelihood.\nget_negloglk_fn &lt;- function(temp, y, lower.bounds, upper.bounds) {\n  # temp: Vector of measured temperature values.\n  # y: Vector of measured performance values.\n  \n  # lower_bounds, upper_bounds: Lower and upper bounds for the parameters.\n  \n  f &lt;- function(par) {\n    # Return infinity (worst possible error) if parameters are outside of upper and lower bounds.\n    if((sum(par &lt; lower.bounds) + sum(par &gt; upper.bounds)) &gt; 0) {\n      return(Inf)    \n    } else {\n      \n      # Rename parameters for readability of code.\n      Tmin &lt;- par[1]\n      Tmax &lt;- par[2]\n      rmax &lt;- par[3]\n      alpha &lt;- par[4]\n      beta &lt;- par[5]\n      eta0 &lt;- par[6]\n      eta1 &lt;- par[7]\n      \n      # Find observations that are inside the interval (Tmin, Tmax)\n      T.idx &lt;- (temp &gt; Tmin) & (temp &lt; Tmax)\n\n      # Return infinity if any measurements outside the (Tmin, Tmax)\n      # temperature range are nonzero. Otherwise these measurements have\n      # a probability of one (so they don't contribute to the log-likelihood as\n      # log(1) = 0.\n      if(sum(y[!T.idx] != 0) &gt; 0) {\n        return(Inf)\n      }\n      \n      # Return negative log-likelihood of Gamma model if inside (Tmin, Tmax) interval.\n      mu = flexTPC(temp[T.idx], Tmin, Tmax, rmax, alpha, beta)\n      sigma = exp(eta0 + eta1 * mu)\n      \n      # Convert mean and standard deviation to shape and rate parameters for\n      # Gamma distribution.\n      a = mu^2 / sigma^2\n      b = mu / sigma^2\n      \n      return(sum(-dgamma(y[T.idx], a, b, log=TRUE))) # log=TRUE calculates the log-likelihood.\n  }\n  }\n  return(f) \n}\n\nLet’s now fit the model to the data.\n\ninit.par &lt;- c(16.5, 34, 3.2, 0.7, 0.3, log(0.5), 0)\nnames(init.par) &lt;- c(\"Tmin\", \"Tmax\", \"rmax\", \"alpha\", \"beta\", \"eta0\", \"eta1\")\n\nlower.bounds &lt;- c(0, 30, 0, 0.1, 0.1, -10, -10)\nupper.bounds &lt;- c(25, 45, 10, 0.9, 0.7, 10, 10)\ntemp &lt;- data$temperature\ny &lt;- data$trait_value * 10^6\n\nf &lt;- get_negloglk_fn(temp, y, lower.bounds, upper.bounds)\n\n#f(init.par)\nfit &lt;- optim(init.par, f, control=list(maxit=10000))\n\n\nfit$par\n\n      Tmin       Tmax       rmax      alpha       beta       eta0       eta1 \n16.1215158 33.0563842  3.2488154  0.6852062  0.3544492 -2.6753234  0.5518849 \n\n\nLet’s plot the data with the fitted flexTPC model.\n\ntemp &lt;- seq(0, 45, 0.1)\n\nplot(data$temperature, y, xlim=c(10, 40),\n     xlab='Temperature [°C]', ylab='growth rate * 10^6',\n     pch=20, col='steelblue')\nlines(temp, flexTPC(temp, Tmin=16, Tmax=34, rmax=3.2, alpha=0.7, beta=0.3),\n      lwd=2, col='gray')\nlines(temp, flexTPC(temp, Tmin=fit$par[1], Tmax=fit$par[2], rmax=fit$par[3], alpha=fit$par[4], beta=fit$par[5]),\n     col='steelblue', lwd=2)\n\n\n\n\n\n\n\n\nAs before, we can calculate the optimal temperature, approximate thermal breadth at 88% and half-maximum temperatures.\n\nTopt &lt;- fit$par['alpha'] * fit$par['Tmax'] + (1.0 - fit$par['alpha']) * fit$par['Tmin']\nB &lt;- fit$par['beta'] * (fit$par['Tmax'] - fit$par['Tmin'])\n\nhalfmax_temps &lt;- flexTPC_roots(0.5, # wref \n                               fit$par['Tmin'],\n                               fit$par['Tmax'], \n                               fit$par['alpha'], \n                               fit$par['beta']) \n\nhalfmax_breadth &lt;- halfmax_temps[2] - halfmax_temps[1]\n\nfit$par &lt;- c(fit$par, Topt, B, halfmax_temps, halfmax_breadth)\nnames(fit$par) &lt;- c(\"Tmin\", \"Tmax\", \"rmax\", \"alpha\", \"beta\", \"eta0\", \"eta1\", \"Topt\", \"B\", \"Thm1\", \"Thm2\", \"halfmax_breadth\")\nfit$par\n\n           Tmin            Tmax            rmax           alpha            beta \n     16.1215158      33.0563842       3.2488154       0.6852062       0.3544492 \n           eta0            eta1            Topt               B            Thm1 \n     -2.6753234       0.5518849      27.7253934       6.0025511      20.4578027 \n           Thm2 halfmax_breadth \n     32.3433299      11.8855272"
  },
  {
    "objectID": "least_squares_v2.html#finding-the-optimal-temperature-and-approximate-thermal-breadth",
    "href": "least_squares_v2.html#finding-the-optimal-temperature-and-approximate-thermal-breadth",
    "title": "Least squares estimation",
    "section": "Finding the optimal temperature and approximate thermal breadth",
    "text": "Finding the optimal temperature and approximate thermal breadth\nWe know that in the flexTPC model the optimum temperature \\(T_\\mathrm{opt}\\) is defined as\n\\[T_{\\mathrm{opt}} = \\alpha T_\\max + (1- \\alpha) T_\\min\\] and the approximate thermal breadth \\(B\\) at 88% of the maximum performance is defined as \\[ B = \\beta (T_\\max - T_\\min)\\] We can use these equations to find the least square estimates for \\(T_\\mathrm{opt}\\) and \\(B\\).\n\nTopt &lt;- fit$par['alpha'] * fit$par['Tmax'] + (1.0 - fit$par['alpha']) * fit$par['Tmin']\nnames(Topt) &lt;- 'Topt'\nTopt\n\n    Topt \n26.84464 \n\n\n\nB &lt;- fit$par['beta'] * (fit$par['Tmax'] - fit$par['Tmin'])\nnames(B) &lt;- 'B'\nB\n\n       B \n9.330535"
  },
  {
    "objectID": "least_squares.html#finding-the-optimal-temperature-and-approximate-thermal-breadth",
    "href": "least_squares.html#finding-the-optimal-temperature-and-approximate-thermal-breadth",
    "title": "Least squares estimation",
    "section": "Finding the optimal temperature and approximate thermal breadth",
    "text": "Finding the optimal temperature and approximate thermal breadth\nIn the flexTPC model the optimum temperature \\(T_\\mathrm{opt}\\) can be calculated with the following equation:\n\\[T_{\\mathrm{opt}} = \\alpha T_\\max + (1- \\alpha) T_\\min\\] and the approximate thermal breadth \\(B\\) at 88% of the maximum performance with the equation \\[ B = \\beta (T_\\max - T_\\min)\\] We can use these equations to find the least square estimates for \\(T_\\mathrm{opt}\\) and \\(B\\).\n\nTopt &lt;- fit$par['alpha'] * fit$par['Tmax'] + (1.0 - fit$par['alpha']) * fit$par['Tmin']\nnames(Topt) &lt;- 'Topt'\nTopt\n\n    Topt \n26.84464 \n\n\n\nB &lt;- fit$par['beta'] * (fit$par['Tmax'] - fit$par['Tmin'])\nnames(B) &lt;- 'B'\nB\n\n       B \n9.330535"
  },
  {
    "objectID": "least_squares.html#finding-the-thermal-breadth-at-other-reference-values",
    "href": "least_squares.html#finding-the-thermal-breadth-at-other-reference-values",
    "title": "Least squares estimation",
    "section": "Finding the thermal breadth at other reference values",
    "text": "Finding the thermal breadth at other reference values\nIn many applications, researchers may be interested on the thermal breadth at a different reference performance value than 88%. We provide an algorithm to find the temperatures corresponding to any performance numerically from the flexTPC model. This can be used to find the least square estimate of the thermal breadth at any desired performance value (or more accurately at 88%). As an input, our algorithm takes the parameters of a flexTPC curve and a reference relative performance level \\(w_\\mathrm{ref}\\), expressed as a percentage of the maximum trait performance. For example, finding the temperatures with \\(w_\\mathrm{ref}=0.5\\) corresponds to the half-maximum temperatures, or \\(w_\\mathrm{ref}=0.8\\) to the temperatures where performance is 80% of the maximum.\n\n## f1 and f2 are auxiliary functions used by our algorithm.\nf1 &lt;- function(tau, wref, alpha, s) {\n  logf1 &lt;- log(alpha) + (1 / (alpha * s) ) * log(wref) + ((1 - alpha) / alpha) * (log(1 - alpha) - log(1 - tau))\n  return(exp(logf1))\n}\n\nf2 &lt;- function(tau, wref, alpha, s) {\n  B &lt;- log(1 - alpha) + (1 / ((1 - alpha) * s) ) * log(wref) + (alpha / (1 - alpha)) * (log(alpha) - log(tau))\n  return(1 - exp(B))\n}\n\nflexTPC_nd_roots &lt;- function(wref, alpha, beta, tol=1e-6) {\n  ## Implements fixed point algorithm to find nondimensional temperatures tau1, \n  ## tau2 corresponding to a specific wref in a flexTPC model.\n  ## tol: Error tolerance for tau1, tau2 (nondimensional temperatures where\n  ## performance is wref).\n  ## alpha, beta: Parameters for flexTPC curve.\n\n  s &lt;- alpha * (1 - alpha) / beta^2\n  tau1 &lt;- alpha / 2\n  tau2 &lt;- alpha + (1 - alpha) / 2\n  err &lt;- Inf\n  while(err &gt; tol) {\n    new_tau1 &lt;- f1(tau1, wref, alpha, s)\n    new_tau2 &lt;- f2(tau2, wref, alpha, s)\n    err &lt;- pmax(abs(tau1 - new_tau1), abs(tau2 - new_tau2))\n    tau1 &lt;- new_tau1\n    tau2 &lt;- new_tau2\n  }\n  return(c(tau1, tau2))\n}\n\nflexTPC_roots &lt;- function(wref, Tmin, Tmax, alpha, beta, tol=1e-6) {\n  ## Finds (dimensional) temperatures T1, T2 corresponding to a specific wref\n  ## in a flexTPC model.\n  ## Does this by finding nondimensional temperatures and converting them to\n  ## dimensional temperatures.\n  ## tol: Error tolerance for tau1, tau2 (nondimensional temperatures where performance is wref).\n  ## alpha, beta: Parameters for flexTPC curve.\n  nd_roots &lt;- flexTPC_nd_roots(wref, alpha, beta, tol=tol)\n  return(Tmin + nd_roots*(Tmax - Tmin))\n}\n\nLet’s find the half-maximum temperatures and thermal breadth.\n\nhalfmax_temps &lt;- flexTPC_roots(0.5, # wref\n                               fit$par[1], # Tmin\n                               fit$par[2], # Tmax\n                               fit$par[4], # alpha\n                               fit$par[5]) # beta\nnames(halfmax_temps) &lt;- c('Thm1','Thm2')\n\nhalfmax_breadth &lt;- halfmax_temps[2] - halfmax_temps[1]\nnames(halfmax_breadth) &lt;- 'hm_breadth'\n\nhalfmax_temps\n\n    Thm1     Thm2 \n16.12346 34.09863 \n\nhalfmax_breadth\n\nhm_breadth \n  17.97517"
  },
  {
    "objectID": "least_squares.html#confidence-intervals",
    "href": "least_squares.html#confidence-intervals",
    "title": "Least squares estimation",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nResearchers may be interested on quantifying the uncertainty in the estimated model parameters. In frequentist statistics, this uncertainty arises because the data we have is random, and if we were to hypothetically repeat the experiments and data collection we’d get slightly different measurements (and therefore different TPC estimates when fitting the curve!).\nOne general approach to construct confidence intervals is bootstrapping, a procedure based on data resampling. The most common approach is a nonparametric bootstrap, in which construct many bootstrap samples by sampling our original \\(N\\) datapoints to get a new dataset of the same size. In these bootstrap samples some observations will be repeated and some will be missing, so each sample will be different than our original dataset. We can then fit the model to each of these bootstrap samples, and keep track of the estimated model parameters. We can then construct confidence intervals from the distribution of the parameters for the individual bootstrap sample estimates.\n\n## Performs a nonparametric boostrap to get confidence intervals of model parameters.\n\nset.seed(42) # Set seed for reproducibility\n\n# Number of bootstrap samples\n# Note: You can make this small for testing (so the code runs faster), but\n# should be at least 10000 for real applications.\nN &lt;- 10000\n\n# Matrix to store bootstrap results\nbts_samples &lt;- matrix(nrow=N, ncol=9)\n\ntemp &lt;- data$temperature\n\n# Change scale of trait values to be close to unit scale to help optimization.\ny &lt;- data$trait_value * 10^8 \nn.obs &lt;- length(temp)\n\ninit.par &lt;- c(12, 36, 3, 0.6, 0.3)\nlower.bounds &lt;- c(0, 30, 0, 0.1, 0.1)\nupper.bounds &lt;- c(15, 45, 10, 0.9, 0.7)\n\nfor(i in 1:N) {\n    # Sample observations with replacement\n    sample.idx &lt;- sample(1:n.obs, n.obs, replace=TRUE)\n    \n    # Get function to calculate error from current bootstrap sample.\n    f &lt;- get_sq_err_fn(temp[sample.idx], y[sample.idx], lower.bounds, upper.bounds)\n    \n    # Fit flexTPC curve to current bootstrap sample.\n    sample.fit &lt;- optim(init.par, f, control=list(maxit=100000))\n\n    # Store estimated flexTPC parameters for current boostrap sample.\n    bts_samples[i, 1:5] &lt;- sample.fit$par\n}\n\nWe can now find confidence intervals for the model parameters (and also other quantities derived from them, like \\(T_\\mathrm{opt}\\) and \\(B\\), or the half-maximum temperatuers). We can get a boostrap estimate for these quantites by transforming our bootstrap samples for the model parameters as appropriate.\n\ncolnames(bts_samples) &lt;-  c('Tmin', 'Tmax', 'rmax', 'alpha', 'beta',\n'Topt', 'B', 'Thm1', 'Thm2')\n\n# Calculate Topt and B from equations.\nbts_samples[, 'Topt'] &lt;- (bts_samples[, 'alpha'] * bts_samples[, 'Tmax'] \n                       + (1 - bts_samples[, 'alpha']) * bts_samples[, 'Tmin'])\nbts_samples[, 'B'] &lt;- bts_samples[, 'beta'] * (bts_samples[, 'Tmax'] - bts_samples[, 'Tmin'])\n\n# Calculate half-max temperatures numerically for all bootstrap samples.\nhm_temps_samples &lt;- apply(bts_samples[, 1:5], 1, function(x) flexTPC_roots(0.5, x[['Tmin']], x[['Tmax']], x[['alpha']], x[['beta']], tol=1e-6))\n\n#apply(hm_temps_samples, 1, quantile, c(0.025, 0.975))\n\nbts_samples[, c('Thm1', 'Thm2')] &lt;- t(hm_temps_samples)\n\n# Find 95% confidence intervals.\nci &lt;- apply(bts_samples, 2, quantile, c(0.025, 0.975))\nt(ci)\n\n            2.5%      97.5%\nTmin   7.7111399 14.9999999\nTmax  33.7659444 38.8288356\nrmax   2.7723317  3.1335018\nalpha  0.4545014  0.7479570\nbeta   0.2745437  0.6999994\nTopt  24.0493660 28.1469150\nB      7.0124164 14.0858612\nThm1  13.6793069 18.6691363\nThm2  32.8693103 35.5887098\n\n\nThis approach helps us quantify the uncertainty of our estimates. However, in a typical TPC dataset we have don’t have too measurements near the extremes, which will get removed from the boostrap samples, potentially resulting on wild estimates for the minimum and maximum temperatures. Because of this, a nonparametric boostrap can sometimes give very wide confidence intervals for the endpoint temperatures.\nAn arguably more realistic way to resample our data is to keep the same experimental design (e.g. same temperatures as measured in the experimental data), but simulate getting different values. To do this, it helps to have an explicit model of how our data is generated. In the next section, we show how this can be done with maximum likelihood estimation."
  },
  {
    "objectID": "bayesian_estimation.html#part-1.-setting-up-the-model",
    "href": "bayesian_estimation.html#part-1.-setting-up-the-model",
    "title": "Bayesian inference",
    "section": "",
    "text": "In this example, we will fit data on the lifespan of the northern house mosquito Culex pipiens with Bayesian methods. Bayesian methods make it possible to use external information in the form of informative prior distributions to inform parameter estimates.\nAs before, let’s start by plotting the data.\n\ndata &lt;- read.csv('./mosquito_traits/TraitData_lf.csv')\ndata &lt;- subset(data, data$host.code == 'Cpip')\n\nplot(data$T, data$trait, xlim=c(0, 40), ylim=c(0, 150),\n     xlab='Temperature [°C]', ylab='Lifespan [days]', main='Cx pipiens',\n     pch=20, col='steelblue')\n\n\n\n\n\n\n\n\nWe can see a few things from this plot:\n\nLifespan increases at lower temperatures throughout the measured range. However, this cannot continue indefinitely, as eventually temperatures will be too low for the mosquitoes to survive. The lowest temperature that mosquitoes can survive is unlikely to be much lower than 0°C (the freezing point of water). Thus, the thermal performance curves for lifespan are likely left-skewed.\nIt looks like the variability in the data is not constant throughout. Rather, there is greater variability in the measurements at temperatures where lifespan is higher."
  },
  {
    "objectID": "bayesian_estimation.html#prior-distributions",
    "href": "bayesian_estimation.html#prior-distributions",
    "title": "Bayesian inference",
    "section": "Prior distributions",
    "text": "Prior distributions\nIn Bayesian statistics, we represent parameter uncertainty with probability distributions. Rather than considering what happens under hypothetical repeated experiments, we are concerned with estimating the probability that the model parameters have different values.\nA Bayesian analysis starts by choosing a prior distribution for every model parameter that represents the initial uncertainty in these parameters (before seeing any data). In the analysis these prior distributions are updated to a posterior distribution that represents the remaining uncertainty in the analysis after considering which parameter values are compatible with the data (through the likelihood).\nFor the rationale of choosing the priors of this model, please check Appendix S1 of the manuscript. We also give general advice on how to choose prior distributions for the flexTPC model in a separate page.\n\\[T_\\min \\sim \\mathrm{Normal}(\\mu=5\\mathrm{°C}, \\sigma=2.5\\mathrm{°C})\\] \\[T_\\max \\sim \\mathrm{Normal}(\\mu=35\\mathrm{°C}, \\sigma=5\\mathrm{°C})\\] \\[r_\\max \\sim \\mathrm{Uniform}(0, 150)\\] \\[\\alpha \\sim \\mathrm{Uniform}(0, 1)\\] \\[ \\beta \\sim \\mathrm{Gamma}(\\mu=0.35, \\sigma=0.2) \\] We also need to choose priors for the parameters that relate to the statistical model. We will choose the following noninformative priors:\n\\[\\sigma_0 \\sim \\mathrm{Uniform}(0, 100)\\]\n\\[\\eta_1 \\sim \\mathrm{Normal}(\\mu=0, \\sigma=1)\\]"
  },
  {
    "objectID": "priors.html",
    "href": "priors.html",
    "title": "Prior recommendations",
    "section": "",
    "text": "A prior distribution represents our initial state of uncertainty about the value of the model parameters. Prior distributions can vary in their informativeness, depending the desired effect of the prior choices in the analysis.\n\nOn one extreme, priors can be noninformative (or very weakly informative), only constraining the parameters slightly (e.g. only constraining trait performance to be non-negative, or within a certain order of magnitude that is biologically relevant). This kind of prior is used in order to minimize its effect on parameter inference in order to let the data “do the heavy lifting”. The upside of these priors is that they decrease the influence of a priori assumptions made by the analyst about the model parameters on the results. The downside is that models with biologically unrealistic parameter values (e.g. unrealistic thermal minimum or maximum for the organism, extremely skewed curves, curves with an unrealistic thermal breadth, etc.) are assumed plausible and are considered in equal footing with more realistic models when determining the remaining uncertainty on the model parameters (e.g. with credible intervals). This is especially an issue when data is limited.\nOn the other extreme, priors can be strongly informative, having tighter constraints on the parameters that can have a large influence in the results of the analysis. This can be desirable when there is a good source of information about the parameters independent of the data (e.g. previous experiments in related species or information about the species habitat), especially when data is limited. However, strong priors that make inaccurate assumptions can lead to erroneous results, so it is important to be careful in their usage. Moreover, it is considered good practice to make priors less informative than the researcher’s true beliefs about the parameter to account for the possibility of being wrong, as well as for any experimental conditions being different from the previous sources of information.\nSometimes, informative priors can also be used as a form of regularization, preferring simple models over complex models unless the complexity is necessary to describe the data. As the quadratic model is a special case of flexTPC, priors for \\(\\alpha\\) and \\(\\beta\\) can be chosen that pull the parameter estimates closer to those of the quadratic model.\n\nIn general, we prefer the use of weakly to moderately informative priors when good sources of information for constraining the model parameters are available from previous experiments in related species, the species habitat or biological/physical constraints.\nOne important consideration is that prior distributions should be chosen from information independent from the dataset that will be used for fitting (i.e. not from those data points). Not doing this violates the assumptions of Bayesian statistics by using the data twice, which will lead to overconfident predictions.\nOne advantage of flexTPC relative to other approaches is its intuitive parameters, which allow setting informative prior distributions to incorporate information from previous experiments or the species habitat when data is scarce. In this section, we’ll start by providing suggestions on the type of prior distribution to use for each model parameter when using flexTPC.\n\n\nWe suggest the use of normal priors for the minimum and maximum temperatures, which do not introduce a hard boundary to the allowed temperatures. The location can be set with the mean of the distribution and the standard deviation determines how informative the prior is (with larger standard deviations corresponding to less informative priors).\nFor example, a reasonable prior for the minimum temperature for mosquito lifespan could be:\n\\[T_{\\min} \\sim \\mathrm{Normal}(\\mu=\\text{5°C}, \\sigma=\\text{2.5°C})\\]\nOne way to conceptualize the assumption that is made by a prior distribution is to consider a 95% credible interval assumed by the prior. This is an interval that our prior assumes the true value of the parameter is with 95% probability. With a normal distribution, an approximate prior 95% CI is given by the interval \\([\\mu - 2 \\sigma, \\mu + 2 \\sigma]\\) (which would be \\([\\text{0°C},\\text{10°C}]\\) in our example). This prior is thus assuming that \\(T_\\min\\) is 95% likely to be in this interval, although it still gives 5% probability that it is outside this interval.\nWe can follow a similar approach to construct a prior for the maximum temperature. If we want a 95% prior credible interval for the maximum temperature to be \\([\\text{25°C}, \\text{45°C}]\\), we can set the following normal distribution as a prior:\n\\[T_{\\max} \\sim \\mathrm{Normal}(\\mu=\\text{35°C}, \\sigma=\\text{5°C})\\] which ensures the prior 95% CI is what we intend.\n\n\n\nWe typically deal with traits that are strictly non-negative. Because of this, we need to choose a prior distribution for the peak trait value \\(r_{\\max}\\) that has support over the non-negative numbers. Sometimes traits have a true “hard” upper limit (for example, the proportion of individuals surviving to adulthood has to be between zero and one) and sometimes they are unbounded.\nIn most datasets, thermal performance data is collected near the temperatures of optimum performance. This usually means that there is information for estimating \\(r_\\max\\) in the data and it is not usually necessary to have a strong prior for this parameter.\nOne simple choice is a uniform prior on \\(r_{\\max}\\) with a high upper limit. For our mosquito lifespan example, we could choose\n\\[r_\\max \\sim \\mathrm{Uniform}(0, 150)\\] This prior assumes that any value of the maximum lifespan between 0 and 150 days is a priori equally likely, with any value outside this range being impossible.\nHowever, we may prefer a weakly informative prior that does not have a “hard” upper bound. If only a guess as to a reasonable value is available, one choice can be an exponential prior, which only requires one parameter (the rate \\(\\lambda=1/\\mu\\)). For example, from previous fits to related species (e.g. Cx pipiens and Cx quinquefasciatus) we know that a maximum lifespan of around 100 would be reasonable (but a bit high) for a Culex mosquito species. We can thus set\n\\[r_\\max \\sim \\mathrm{Exponential}(\\mu=100)\\] as our prior distribution, which has a 95% prior CI of\n\nround(qexp(c(0.025, 0.975), 1/100), 1) # Exponential distribution is parameterized with rate parameter lambda = 1 / mu.\n\n[1]   2.5 368.9\n\n\nFor a more informative prior for \\(r_{\\max}\\), we recommend a Gamma distribution.\n\n\n\nFlexTPC has two parameters that determine the shape of the curve. Parameter \\(\\alpha \\in (0,1)\\) determines the relative position for the optimum relative to the minimum and maximum. A noninformative prior for \\(\\alpha\\) can be\n\\[\\alpha \\sim \\mathrm{Uniform}(0, 1)\\] We may often want to avoid fitting extremely skewed curves (which are rare), especially in the absence of data to accurately determine the skew. To do this, we may want to use a prior that shrinks \\(\\alpha\\) towards 0.5 (but that allows more skewed curves if required to describe the data). To do this, the prior\n\\[ \\alpha \\sim \\mathrm{Beta}(2,2)\\] can be employed, which has a prior mean of 0.5, and a 95% prior CI of\n\nround(qbeta(c(0.025, 0.975), 2, 2), 3)\n\n[1] 0.094 0.906\n\n\nIn applications with prior knowledge from other TPCs of the same trait in similar organisms, an informative prior can be used that is centered over a different value of \\(\\alpha\\) (to prefer left or right skewed curves).\n\n\n\nParameter \\(\\beta\\) can be interpreted as the approximate ratio between the thermal breadth (range of temperatures for which \\(r(T)&gt;e^{-1/8}r_\\max \\approx 0.88r_\\max\\)) and thermal tolerance range (\\(T_\\max\\) - \\(T_\\min\\)). The quadratic model is a special case of flexTPC when \\(\\alpha=0.5\\) and \\(\\beta=1/\\sqrt8\\approx0.35\\) with most TPCs being well-described by \\(\\beta \\in [0.2, 0.5]\\). Because of this, we recommend a prior that is centered around this value for \\(\\beta\\). A (very) weakly informative prior can be given by\n\\[\\beta \\sim \\mathrm{Gamma}(\\mu=0.35, \\sigma=0.2)\\] which has a prior 95% credible interval of\n\nround(qgamma(c(0.025, 0.975), shape=0.35^2/0.3^2, rate=0.35/0.3^2), 3)\n\n[1] 0.020 1.134\n\n\nand a more informative prior by\n\\[\\beta \\sim \\mathrm{Gamma}(\\mu=0.35, \\sigma=0.1)\\]\n\nround(qgamma(c(0.025, 0.975), shape=0.35^2/0.1^2, rate=0.35/0.1^2), 3)\n\n[1] 0.182 0.572\n\n\nWe can plot both sets of priors (weakly informative and informative) to see how these different assumptions\n\npar(mfrow=c(2,5))\n\n# Tmin\ntemps &lt;- seq(-5, 15, 0.1)\nplot(temps, dnorm(temps, mean=5, sd=2.5), type='l', main='Tmin',\n     xlab='temperature [°C]', ylab='prob. density', lwd=2)\n\n# Tmax\ntemps &lt;- seq(20, 50, 0.1)\nplot(temps, dnorm(temps, mean=35, sd=5), type='l', main='Tmax',\n     xlab='temperature [°C]', ylab='prob. density', lwd=2)\n\n# rmax\nrmax &lt;- seq(0, 150, 0.1)\nplot(rmax, dunif(rmax, 0, 150), type='l', main='rmax',\n     xlab='lifespan [days]', ylab='prob. density', lwd=2)\n\n# alpha\nalpha &lt;-seq(0, 1, 0.01)\nplot(alpha, dunif(alpha, 0, 1), type='l', main='alpha',\n     xlab='alpha [unitless]', ylab='prob. density', lwd=2)\n\n# beta\nbeta &lt;-seq(0, 1.5, 0.01)\nplot(beta, dgamma(beta, shape=0.35^2/0.3^2, rate=0.35/0.3^2), type='l', main='beta',\n     xlab='beta [unitless]', ylab='prob. density', lwd=2)\n\n\n# Tmin\ntemps &lt;- seq(-5, 15, 0.1)\nplot(temps, dnorm(temps, mean=5, sd=2.5), type='l', main='Tmin',\n     xlab='temperature [°C]', ylab='prob. density', lwd=2)\n\n# Tmax\ntemps &lt;- seq(20, 50, 0.1)\nplot(temps, dnorm(temps, mean=35, sd=5), type='l', main='Tmax',\n     xlab='temperature [°C]', ylab='prob. density', lwd=2)\n\n# rmax\nrmax &lt;- seq(0, 150, 0.1)\nplot(rmax, dexp(rmax, 1/100), type='l', main='rmax',\n     xlab='lifespan [days]', ylab='prob. density', lwd=2)\n\n# alpha\nalpha &lt;-seq(0, 1, 0.01)\nplot(alpha, dbeta(alpha, 2, 2), type='l', main='alpha',\n     xlab='alpha [unitless]', ylab='prob. density', lwd=2)\n\n# beta\nbeta &lt;-seq(0, 1.5, 0.01)\nplot(beta, dgamma(beta, shape=0.35^2/0.1^2, rate=0.35/0.1^2), type='l', main='beta',\n     xlab='beta [unitless]', ylab='prob. density', lwd=2)"
  },
  {
    "objectID": "priors.html#general-guidance-for-choosing-prior-distributions",
    "href": "priors.html#general-guidance-for-choosing-prior-distributions",
    "title": "Prior recommendations",
    "section": "",
    "text": "A prior distribution represents our initial state of uncertainty about the value of the model parameters. Prior distributions can vary in their informativeness, depending the desired effect of the prior choices in the analysis.\n\nOn one extreme, priors can be noninformative (or very weakly informative), only constraining the parameters slightly (e.g. only constraining trait performance to be non-negative, or within a certain order of magnitude that is biologically relevant). This kind of prior is used in order to minimize its effect on parameter inference in order to let the data “do the heavy lifting”. The upside of these priors is that they decrease the influence of a priori assumptions made by the analyst about the model parameters on the results. The downside is that models with biologically unrealistic parameter values (e.g. unrealistic thermal minimum or maximum for the organism, extremely skewed curves, curves with an unrealistic thermal breadth, etc.) are assumed plausible and are considered in equal footing with more realistic models when determining the remaining uncertainty on the model parameters (e.g. with credible intervals). This is especially an issue when data is limited.\nOn the other extreme, priors can be strongly informative, having tighter constraints on the parameters that can have a large influence in the results of the analysis. This can be desirable when there is a good source of information about the parameters independent of the data (e.g. previous experiments in related species or information about the species habitat), especially when data is limited. However, strong priors that make inaccurate assumptions can lead to erroneous results, so it is important to be careful in their usage. Moreover, it is considered good practice to make priors less informative than the researcher’s true beliefs about the parameter to account for the possibility of being wrong, as well as for any experimental conditions being different from the previous sources of information.\nSometimes, informative priors can also be used as a form of regularization, preferring simple models over complex models unless the complexity is necessary to describe the data. As the quadratic model is a special case of flexTPC, priors for \\(\\alpha\\) and \\(\\beta\\) can be chosen that pull the parameter estimates closer to those of the quadratic model.\n\nIn general, we prefer the use of weakly to moderately informative priors when good sources of information for constraining the model parameters are available from previous experiments in related species, the species habitat or biological/physical constraints.\nOne important consideration is that prior distributions should be chosen from information independent from the dataset that will be used for fitting (i.e. not from those data points). Not doing this violates the assumptions of Bayesian statistics by using the data twice, which will lead to overconfident predictions.\nOne advantage of flexTPC relative to other approaches is its intuitive parameters, which allow setting informative prior distributions to incorporate information from previous experiments or the species habitat when data is scarce. In this section, we’ll start by providing suggestions on the type of prior distribution to use for each model parameter when using flexTPC.\n\n\nWe suggest the use of normal priors for the minimum and maximum temperatures, which do not introduce a hard boundary to the allowed temperatures. The location can be set with the mean of the distribution and the standard deviation determines how informative the prior is (with larger standard deviations corresponding to less informative priors).\nFor example, a reasonable prior for the minimum temperature for mosquito lifespan could be:\n\\[T_{\\min} \\sim \\mathrm{Normal}(\\mu=\\text{5°C}, \\sigma=\\text{2.5°C})\\]\nOne way to conceptualize the assumption that is made by a prior distribution is to consider a 95% credible interval assumed by the prior. This is an interval that our prior assumes the true value of the parameter is with 95% probability. With a normal distribution, an approximate prior 95% CI is given by the interval \\([\\mu - 2 \\sigma, \\mu + 2 \\sigma]\\) (which would be \\([\\text{0°C},\\text{10°C}]\\) in our example). This prior is thus assuming that \\(T_\\min\\) is 95% likely to be in this interval, although it still gives 5% probability that it is outside this interval.\nWe can follow a similar approach to construct a prior for the maximum temperature. If we want a 95% prior credible interval for the maximum temperature to be \\([\\text{25°C}, \\text{45°C}]\\), we can set the following normal distribution as a prior:\n\\[T_{\\max} \\sim \\mathrm{Normal}(\\mu=\\text{35°C}, \\sigma=\\text{5°C})\\] which ensures the prior 95% CI is what we intend.\n\n\n\nWe typically deal with traits that are strictly non-negative. Because of this, we need to choose a prior distribution for the peak trait value \\(r_{\\max}\\) that has support over the non-negative numbers. Sometimes traits have a true “hard” upper limit (for example, the proportion of individuals surviving to adulthood has to be between zero and one) and sometimes they are unbounded.\nIn most datasets, thermal performance data is collected near the temperatures of optimum performance. This usually means that there is information for estimating \\(r_\\max\\) in the data and it is not usually necessary to have a strong prior for this parameter.\nOne simple choice is a uniform prior on \\(r_{\\max}\\) with a high upper limit. For our mosquito lifespan example, we could choose\n\\[r_\\max \\sim \\mathrm{Uniform}(0, 150)\\] This prior assumes that any value of the maximum lifespan between 0 and 150 days is a priori equally likely, with any value outside this range being impossible.\nHowever, we may prefer a weakly informative prior that does not have a “hard” upper bound. If only a guess as to a reasonable value is available, one choice can be an exponential prior, which only requires one parameter (the rate \\(\\lambda=1/\\mu\\)). For example, from previous fits to related species (e.g. Cx pipiens and Cx quinquefasciatus) we know that a maximum lifespan of around 100 would be reasonable (but a bit high) for a Culex mosquito species. We can thus set\n\\[r_\\max \\sim \\mathrm{Exponential}(\\mu=100)\\] as our prior distribution, which has a 95% prior CI of\n\nround(qexp(c(0.025, 0.975), 1/100), 1) # Exponential distribution is parameterized with rate parameter lambda = 1 / mu.\n\n[1]   2.5 368.9\n\n\nFor a more informative prior for \\(r_{\\max}\\), we recommend a Gamma distribution.\n\n\n\nFlexTPC has two parameters that determine the shape of the curve. Parameter \\(\\alpha \\in (0,1)\\) determines the relative position for the optimum relative to the minimum and maximum. A noninformative prior for \\(\\alpha\\) can be\n\\[\\alpha \\sim \\mathrm{Uniform}(0, 1)\\] We may often want to avoid fitting extremely skewed curves (which are rare), especially in the absence of data to accurately determine the skew. To do this, we may want to use a prior that shrinks \\(\\alpha\\) towards 0.5 (but that allows more skewed curves if required to describe the data). To do this, the prior\n\\[ \\alpha \\sim \\mathrm{Beta}(2,2)\\] can be employed, which has a prior mean of 0.5, and a 95% prior CI of\n\nround(qbeta(c(0.025, 0.975), 2, 2), 3)\n\n[1] 0.094 0.906\n\n\nIn applications with prior knowledge from other TPCs of the same trait in similar organisms, an informative prior can be used that is centered over a different value of \\(\\alpha\\) (to prefer left or right skewed curves).\n\n\n\nParameter \\(\\beta\\) can be interpreted as the approximate ratio between the thermal breadth (range of temperatures for which \\(r(T)&gt;e^{-1/8}r_\\max \\approx 0.88r_\\max\\)) and thermal tolerance range (\\(T_\\max\\) - \\(T_\\min\\)). The quadratic model is a special case of flexTPC when \\(\\alpha=0.5\\) and \\(\\beta=1/\\sqrt8\\approx0.35\\) with most TPCs being well-described by \\(\\beta \\in [0.2, 0.5]\\). Because of this, we recommend a prior that is centered around this value for \\(\\beta\\). A (very) weakly informative prior can be given by\n\\[\\beta \\sim \\mathrm{Gamma}(\\mu=0.35, \\sigma=0.2)\\] which has a prior 95% credible interval of\n\nround(qgamma(c(0.025, 0.975), shape=0.35^2/0.3^2, rate=0.35/0.3^2), 3)\n\n[1] 0.020 1.134\n\n\nand a more informative prior by\n\\[\\beta \\sim \\mathrm{Gamma}(\\mu=0.35, \\sigma=0.1)\\]\n\nround(qgamma(c(0.025, 0.975), shape=0.35^2/0.1^2, rate=0.35/0.1^2), 3)\n\n[1] 0.182 0.572\n\n\nWe can plot both sets of priors (weakly informative and informative) to see how these different assumptions\n\npar(mfrow=c(2,5))\n\n# Tmin\ntemps &lt;- seq(-5, 15, 0.1)\nplot(temps, dnorm(temps, mean=5, sd=2.5), type='l', main='Tmin',\n     xlab='temperature [°C]', ylab='prob. density', lwd=2)\n\n# Tmax\ntemps &lt;- seq(20, 50, 0.1)\nplot(temps, dnorm(temps, mean=35, sd=5), type='l', main='Tmax',\n     xlab='temperature [°C]', ylab='prob. density', lwd=2)\n\n# rmax\nrmax &lt;- seq(0, 150, 0.1)\nplot(rmax, dunif(rmax, 0, 150), type='l', main='rmax',\n     xlab='lifespan [days]', ylab='prob. density', lwd=2)\n\n# alpha\nalpha &lt;-seq(0, 1, 0.01)\nplot(alpha, dunif(alpha, 0, 1), type='l', main='alpha',\n     xlab='alpha [unitless]', ylab='prob. density', lwd=2)\n\n# beta\nbeta &lt;-seq(0, 1.5, 0.01)\nplot(beta, dgamma(beta, shape=0.35^2/0.3^2, rate=0.35/0.3^2), type='l', main='beta',\n     xlab='beta [unitless]', ylab='prob. density', lwd=2)\n\n\n# Tmin\ntemps &lt;- seq(-5, 15, 0.1)\nplot(temps, dnorm(temps, mean=5, sd=2.5), type='l', main='Tmin',\n     xlab='temperature [°C]', ylab='prob. density', lwd=2)\n\n# Tmax\ntemps &lt;- seq(20, 50, 0.1)\nplot(temps, dnorm(temps, mean=35, sd=5), type='l', main='Tmax',\n     xlab='temperature [°C]', ylab='prob. density', lwd=2)\n\n# rmax\nrmax &lt;- seq(0, 150, 0.1)\nplot(rmax, dexp(rmax, 1/100), type='l', main='rmax',\n     xlab='lifespan [days]', ylab='prob. density', lwd=2)\n\n# alpha\nalpha &lt;-seq(0, 1, 0.01)\nplot(alpha, dbeta(alpha, 2, 2), type='l', main='alpha',\n     xlab='alpha [unitless]', ylab='prob. density', lwd=2)\n\n# beta\nbeta &lt;-seq(0, 1.5, 0.01)\nplot(beta, dgamma(beta, shape=0.35^2/0.1^2, rate=0.35/0.1^2), type='l', main='beta',\n     xlab='beta [unitless]', ylab='prob. density', lwd=2)"
  },
  {
    "objectID": "bayesian_estimation.html#the-posterior-distribution",
    "href": "bayesian_estimation.html#the-posterior-distribution",
    "title": "Bayesian inference",
    "section": "The posterior distribution",
    "text": "The posterior distribution\nThe main goal in a Bayesian analysis is to obtain the posterior distribution of the parameters. This corresponds to the remaining uncertainty in the parameters after seeing the data. The prior distribution and the likelihood are combined to obtain the posterior distribution with Bayes’s Theorem:\n\\[p(\\theta | y) = \\frac{p(y|\\theta) p(\\theta)}{p(y)} \\]\n\\[\\mathrm{posterior} \\propto \\mathrm{prior} \\times \\mathrm{likelihood} \\]"
  },
  {
    "objectID": "bayesian_estimation.html#bayesian-inference-with-mcmc-methods",
    "href": "bayesian_estimation.html#bayesian-inference-with-mcmc-methods",
    "title": "Bayesian inference",
    "section": "Bayesian inference with MCMC methods",
    "text": "Bayesian inference with MCMC methods\nNow that we’ve written down our model (priors and likelihood) in the JAGS language, we can proceed to running the analysis. JAGS implements a Gibbs sampler, which is a form of Markov Chain Monte Carlo (MCMC), an algorithm that allows us to get samples from the posterior distribution of a Bayesian statistical model. Explaining how MCMC methods work in detail is beyond the scope of this tutorial. However, we provide a simplified explanation here to aid understanding of the process of fitting a Bayesian model (and to better understand the code below):\n\nWe start with initial guesses for the model parameters. This is considered the first value of the MCMC chain.\nThe Gibbs sampling algorithm then decides a random new value for the model parameters based on their current value, the model likelihood and the priors. This algorithm is cleverly set up so that in the long run, the distribution of parameters of the chain will correspond to draws from the posterior distribution of our model.\nWe run the Gibbs sampling algorithm iteratively with the updated values, saving the values the parameters take at each iteration. Eventually, if we run the chain long enough, the MCMC chain will converge to its equilibrium distribution (which corresponds to the posterior distribution).\nAfter convergence, we the samples from the MCMC chain are samples from the posterior distribution of our model. We can use these samples to calculate summaries of this distribution (like means and credible intervals for the individual parameters).\n\nMCMC is only guaranteed to sample from the posterior distribution in the long run. Because of this, it is important to a) run the chains for a large number of iterations, and b) check for signs that the chains may not have yet converged, as in that case we cannot trust the results.\nIt is usually a good idea to run multiple MCMC chains with different starting values. Once the chains have converged, they should be drawing samples from the same distribution (the posterior). We can examine plots of the multiple MCMC chains to see if they look similar as an informal check if they converged. We can also check some convergence criteria that are based on statistical comparisons of the variability between and within chains.\nIn this example, we will run four independent MCMC chains. We will choose the initial values of the MCMC chains by drawing these values randomly (but within specified ranges). In this example, we will run the chains for 500000 iterations. We often want to discard some of the initial iterations (a burn-in period), since it is likely that the chains have not yet converged in early iterations We will save only one out of every eight iterations to save some memory. Optionally, we can discard some of the iterations to save some memory.\n\n# Set random seed for reproducibility.\nset.seed(42)\n\n##### Set MCMC settings\n# Number of posterior dist elements = [(ni - nb) / nt ] * nc = [ (25000 - 5000) / 8 ] * 3 = 7500\nnc &lt;- 4 # number of MCMC chains\nni &lt;- 500000 # number of iterations in each chain\nnb &lt;- 50000 # number of 'burn in' iterations to discard\nnt &lt;- 8 # thinning rate - jags saves every nt iterations in each chain\n\n#### Recipe for choosing initial values for MCMC (so each chain starts at different values).\ninits&lt;-function(){list(\n  Tmin = runif(1, min=2.5, max=7.5),\n  Tmax = runif(1, min=35, max=40),\n  rmax = runif(1, min=0, max=150),\n  alpha = runif(1, min=0.2, max=0.8),\n  beta = runif(1, min=0.2, max=0.7),\n  sigma0 = runif(1, min=0, max=50),\n  eta1 = runif(1, min=-0.1, max=0.1))}\n\n##### Parameters to save from MCMC chains.\nparameters &lt;- c(\"Tmin\", \"Tmax\", \"rmax\", \"alpha\", \"beta\", 's', \"Topt\", \"sigma0\", \"eta1\")\n\nNote that what we have in \\(\\texttt{inits}\\) is different than the prior distributions. The purpose of this code is to choose the initial values of our MCMC chains randomly. In principle, we could choose any starting values, but in practice things work better (i.e. the MCMC chains will converge faster) if the initial values are plausible parameter values for describing the data.\nNow that we have defined the relevant MCMC settings, we can organize the data and fit the model.\n\n##### Organize Data for JAGS\ny &lt;- Ctar.lf.data$trait\ntemp &lt;- Ctar.lf.data$T\nN.obs &lt;- length(y)\n\n##### Bundle Data\njag.data &lt;- list(y=y, temp = temp, N.obs=N.obs)\n\njags.out &lt;- jags(data=jag.data, inits=inits, parameters.to.save=parameters, \n                         model.file=\"lifespan.txt\", n.thin=nt, n.chains=nc, \n                         n.burnin=nb, n.iter=ni, DIC=TRUE, working.directory=getwd())\n\nmodule glm loaded\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 33\n   Unobserved stochastic nodes: 7\n   Total graph size: 286\n\nInitializing model\n\n\nThe previous code ran an MCMC algorithm to sample from the posterior distribution of our model. We can look at the values of the first few iterations of our MCMC chain.\n\nchains &lt;- MCMCchains(jags.out, params = c('Tmin', 'Tmax', 'rmax', 'alpha', 'beta', 'Topt',\n                                'sigma0', 'eta1'))\nhead(chains)\n\n          Tmin     Tmax     rmax      alpha      beta      Topt    sigma0\n[1,]  5.679186 38.93609 50.58937 0.06088373 0.2594664  7.703991  1.205484\n[2,]  5.059177 41.68388 36.53974 0.25604239 0.2992110 14.436655  5.137441\n[3,]  3.395206 41.79350 40.90831 0.18941846 0.3262908 10.668552  2.263047\n[4,] 10.002884 42.45772 30.98153 0.08614849 0.2899040 12.798819  1.448849\n[5,]  3.062419 37.59707 40.72979 0.43290347 0.3023225 18.012588 20.156065\n[6,]  3.389629 43.43354 54.49044 0.05553694 0.2025542  5.613545  6.219664\n             eta1\n[1,]  0.064093208\n[2,]  0.023128628\n[3,]  0.059832411\n[4,]  0.090174379\n[5,] -0.009103983\n[6,]  0.023000565\n\n\nEach row here corresponds to one MCMC iteration. If the MCMC chains have converged, it also corresponds to one sample from the posterior distribution of our model parameters, which describes the remaining uncertainty in these parameters after the analysis.\nAs an informal graphical check for convergence, we can look at a traceplot of our four independent MCMC chains, which shows the values of the parameters at each iteration. If the chains have converged, these four chains should be drawing samples from the same distribution and should look very similar to each other. Let’s look at the values for some model parameters.\n\nMCMCtrace(jags.out, \n          params = c('Tmin', 'Tmax', 'rmax'), \n          ISB = FALSE, \n          iter=225000, # Plot all saved traces.\n          exact = TRUE,\n          ind=TRUE,\n          pdf = FALSE)\n\n\n\n\n\n\n\n\nHere the values of our four chains are plotted with different colors. The left plots are traceplots that show the value of the parameters at each iteration of our four MCMC chains (each shown in a different color). If the chains look systematically different from each other, it indicates that they are not yet drawing from the same distribution, making it likely that the chains have not yet converged. An MCMC chain that has converged usually looks like a “fuzzy caterpillar” (like the plots above).\nThe plots on the right show a density plot (i.e. essentially a smoothed histogram) of the values of our individual MCMC chains. These plots estimate the posterior distribution of the model parameters, provided the MCMC chains have converged. If the distribution from samples from different chains look different from each other, this indicates that the chains have not yet converged. When running an MCMC algorithm, these plots should always be checked (for all parameters of interest, although we only show these three here) before trusting the results. One convenient way to do this is the function\n\nmcmcplot(jags.out)\n\n\n                                                                                \nPreparing plots for Tmax.  10% complete.\n\n\n\n                                                                                \nPreparing plots for Tmin.  20% complete.\n\n\n\n                                                                                \nPreparing plots for Topt.  30% complete.\n\n\n\n                                                                                \nPreparing plots for alpha.  40% complete.\n\n\n\n                                                                                \nPreparing plots for beta.  50% complete.\n\n\n\n                                                                                \nPreparing plots for deviance.  60% complete.\n\n\n\n                                                                                \nPreparing plots for eta1.  70% complete.\n\n\n\n                                                                                \nPreparing plots for rmax.  80% complete.\n\n\n\n                                                                                \nPreparing plots for s.  90% complete.\n\n\n\n                                                                                \nPreparing plots for sigma0.  100% complete.\n\n\nwhich can show traceplots and density plots for all parameters in a web browser (this won’t work in this tutorial, but you can run this if you are following along).\nAfter inspecting the chains and finding no obvious issues, we can now look at a summary of our analysis.\n\njags.out\n\nInference for Bugs model at \"lifespan.txt\", fit using jags,\n 4 chains, each with 5e+05 iterations (first 50000 discarded), n.thin = 8\n n.sims = 225000 iterations saved\n         mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat  n.eff\nTmax      40.527   3.116  34.900  38.455  40.291  42.421  47.181 1.001 200000\nTmin       4.668   2.464  -0.174   3.010   4.673   6.336   9.491 1.001 220000\nTopt      10.729   3.623   3.579   8.131  10.859  13.493  17.060 1.001 110000\nalpha      0.168   0.092   0.023   0.094   0.161   0.233   0.358 1.001 180000\nbeta       0.304   0.073   0.154   0.260   0.303   0.348   0.449 1.001  61000\neta1       0.049   0.026   0.002   0.032   0.047   0.064   0.109 1.001  36000\nrmax      44.169   7.169  32.433  39.288  43.299  48.105  60.766 1.001  34000\ns          1.436   0.678   0.590   0.978   1.290   1.729   3.099 1.001 220000\nsigma0     3.260   2.957   0.602   1.520   2.368   3.834  12.025 1.001  38000\ndeviance 239.331   4.599 233.349 235.978 238.308 241.593 250.986 1.001 120000\n\nFor each parameter, n.eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n\nDIC info (using the rule, pD = var(deviance)/2)\npD = 10.6 and DIC = 249.9\nDIC is an estimate of expected predictive error (lower deviance is better).\n\n\nThis contains the posterior mean (\\(\\texttt{mu.vect}\\)) and standard deviation (\\(\\texttt{sd.vect}\\)), as well as the 2.5%, 25%, 50% (median), 75% and 97.5% quantiles of the posterior distribution for each model parameter and generated quantity. A 95% credible interval can be contructed from the 2.5% and 97.5% quantiles.\nThis summary also contains some MCMC diagnostics that can be useful to detect nonconvergence. The Gelman-Rubin statistic \\(\\hat{R}\\) is a convergence diagnostic that compares the variability within and across the different MCMC chains. If the MCMC chains have converged Rhat should be very close to one. One rule of thumb is that we should only use samples for which \\(\\hat{R}&lt;1.01\\) for all parameters of interest.\nThe samples of an MCMC chain are usually correlated. The effective sample size \\(n_\\mathrm{eff}\\) is an estimate of how many independent samples would be equivalent to the (non-independent) samples that we have from our chains. One rule of thumb is that ideally we want \\(n_\\mathrm{eff} \\approx 10000\\) or higher if we want to estimate a 95% credible interval for a parameter. Having low \\(n_\\mathrm{eff}\\) after many iterations often indicates an issue with the model and/or MCMC convergence issues.\nEach MCMC iteration corresponds to a draw from the posterior distribution of the model parameters. As the model parameters define a flexTPC curve, each iteration can also be interpreted as a sample from the posterior distribution of flexTPC curves that could plausibly describe the data. Let’s plot a few of these TPCs samples from the posterior:\n\ntemps &lt;- seq(-5, 45, 0.1)\nchains &lt;- MCMCchains(jags.out, params=c(\"Tmin\", \"Tmax\", \"rmax\", \"alpha\", \"beta\"))\n\n\ncurves &lt;- apply(chains, 1, function(x) flexTPC(temps, x[1], x[2], x[3], x[4], x[5]))\n\n# Sample 10 curves from posterior distribution\nsample.idx &lt;- sample(1:225000, 10)\nplot(Ctar.lf.data$T, Ctar.lf.data$trait, xlim=c(0, 40), ylim=c(0, 80), xlab='Temperature [°C]',\n     ylab='Lifespan [days]', pch=20, col='steelblue')\nfor(i in 1:10) {\n  lines(temps, curves[, sample.idx[i]], col=alpha(\"grey\", 0.3), lwd=2)\n}\n\n\n\n\n\n\n\n\nEach gray line corresponds to a flexTPC curve that could plausibly describe this data. We usually want to summarize these posterior samples. We can do this by taking the mean and a 95% credible interval at each temperature from a large number of samples of these TPCs. Let’s do this and plot it with the data.\n\nmeancurve &lt;- apply(curves, 1, mean)\nCI &lt;- apply(curves, 1, quantile, c(0.025, 0.975))\n\nplot(Ctar.lf.data$T, Ctar.lf.data$trait, xlim=c(0, 40), ylim=c(0, 80), xlab='Temperature [°C]',\n     ylab='Lifespan [days]', pch=20, col='steelblue')\nlines(temps, meancurve, col=\"steelblue\", lwd=1.5, type='l')\npolygon(c(temps, rev(temps)), c(CI[1,], rev(CI[2,])), \n        col=alpha(\"steelblue\", 0.2), lty=0)\n\n\n\n\n\n\n\n\nWe can compare the prior and posterior distribution for each parameter to see how the data changed the uncertainty.\n\npar(mfrow=c(2, 4))\n\nposterior.col = \"purple\"\n\n# Tmin\ntemps &lt;- seq(-5, 15, 0.1)\nplot(density(MCMCchains(jags.out, params=c(\"Tmin\"))), lwd=2, col=posterior.col,, type='l', main='Tmin',\n     xlab='temperature [°C]', ylab='prob. density')\nlines(temps, dnorm(temps, mean=5, sd=2.5), lwd=2)\n\n# Tmax\ntemps &lt;- seq(20, 50, 0.1)\nplot(density(MCMCchains(jags.out, params=c(\"Tmax\"))), lwd=2, col=posterior.col, type='l', main='Tmax',\n     xlab='temperature [°C]', ylab='prob. density', xlim=c(20, 50))\nlines(temps, dnorm(temps, mean=35, sd=5), lwd=2)\n\n# rmax\nrmax &lt;- seq(0, 150, 0.1)\nplot(density(MCMCchains(jags.out, params=c(\"rmax\"))), lwd=2,\n     col=posterior.col, type='l', main='rmax',\n     xlab='max lifespan [days]', ylab='prob. density', xlim=c(0, 150))\nlines(rmax, dunif(rmax, 0, 150), lwd=2)\n\n\n# alpha\nalpha &lt;-seq(0, 1, 0.01)\nplot(density(MCMCchains(jags.out, params=c(\"alpha\"))), lwd=2, col=posterior.col, type='l', main='alpha',\n     xlab='alpha [unitless]', ylab='prob. density', xlim=c(0, 1))\nlines(alpha, dunif(alpha, 0, 1), lwd=2)\n\n\n# beta\nbeta &lt;-seq(0, 1.5, 0.01)\nplot(density(MCMCchains(jags.out, params=c(\"beta\"))), lwd=2,\n     col=posterior.col, type='l', main='beta', xlim=c(0, 1.5),\n     xlab='beta [unitless]', ylab='prob. density')\nlines(beta, dgamma(beta, shape=0.35^2/0.2^2, rate=0.35/0.2^2), lwd=2)\n\n# sigma0\nsigma0 &lt;-seq(0, 50, 0.01)\nplot(density(MCMCchains(jags.out, params=c(\"sigma0\"))), lwd=2,\n     col=posterior.col, type='l', main='sigma0', xlim=c(0, 50),\n     xlab='sigma0 [days]', ylab='prob. density')\nlines(sigma0, dexp(sigma0, 1/50), lwd=2)\n\n#eta 1\neta1 &lt;-seq(-0.5, 0.5, 0.01)\nplot(density(MCMCchains(jags.out, params=c(\"eta1\"))), lwd=2,\n     col=posterior.col, type='l', main='eta1', xlim=c(-0.3, 0.3),\n     xlab='eta1 [1 / day]', ylab='prob. density')\nlines(eta1, dnorm(eta1, 0, 0.1), lwd=2)\n\n\n\n\n\n\n\n\nHere, the prior distribution is shown in black and the posterior distribution in purple. Conceptually, the prior distribution corresponds to our initial uncertainty about the model parameters (based on model assumptions), and the posterior to the updated uncertainty after considering the data.\nWe can see that the the posterior distribution for \\(T_\\mathrm{min}\\) is almost identical to the prior. This is expected, as there are no measurements at low temperatures, so the data had very little (if any) information about this parameter. Because of this our \\(T_\\mathrm{min}\\) estimates are heavily dependent on the choice of prior. This is not necessarily a bad thing, as the prior makes it possible to constrain the parameters to biologically relevant ranges even when data is limited at those temperatures. However, for parameters where the data should be informing a parameter, a prior and posterior that are very similar can suggest that the prior is too strong and overly constraining the values of the parameter.\nIn this example, we can see that for all other parameters other than \\(T_{\\min}\\), the posterior distribution is different than the prior, and is more concentrated/narrower. This shows us how the uncertainty on the model parameters changed upon seeing the data."
  }
]